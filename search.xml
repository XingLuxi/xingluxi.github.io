<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Defining Comprehension on MRC</title>
      <link href="/2021/02/20/paper-mrc-define-comprehension/"/>
      <url>/2021/02/20/paper-mrc-define-comprehension/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Content:<br><a id="more"></a></p><p>1 - <a href="https://www.aclweb.org/anthology/D19-5815" target="_blank" rel="noopener">On Making Reading Comprehension More Comprehensive</a></p><blockquote><p><em>Authors</em>: Matt Gardner, Jonathan Berant, Hannaneh Hajishirzi, Alon Talmor, Sewon Min<br><em>Org.</em>: AI2, Tel Aviv University, UW<br><em>Published</em>: EMNLP 2019 workshop on MRQA</p></blockquote><p>2 - <a href="https://www.aclweb.org/anthology/2020.acl-main.701" target="_blank" rel="noopener">To Test Machine Comprehension, Start by Defining Comprehension</a></p><blockquote><p><em>Authors</em>: Jesse Dunietz, Gregory Burnham, Akash Baharadwaj, Owen Rambow, Jennifer Chu-Carroll, David Ferrucci<br><em>Org.</em>: Elemental Cognition<br><em>Published</em>: ACL 2020</p></blockquote><h2 id="On-Making-RC-more-Comprehensive"><a href="#On-Making-RC-more-Comprehensive" class="headerlink" title="On Making RC more Comprehensive"></a>On Making RC more Comprehensive</h2><h3 id="Intro："><a href="#Intro：" class="headerlink" title="Intro："></a>Intro：</h3><p>使机器<strong>理解</strong>自然语言文本是一个重要的任务，令这个任务更有挑战性的是我们尚未搞清什么是理解文本，或，如何评判一个machine是否成功完成了这个任务。</p><p>现有研究中，采取了问答式机器阅读理解的方式来验证一个系统是否理解了文本，即，给定一个文本片段，一个自然语言问题（假定该问题需要在一定程度上对文本的理解才能回答）。</p><p>但是却没有工作系统地验证这种理解的方法（问答式机器阅读理解），或讨论其缺点。</p><p>本文认为，QA式MRC是一种很好的但是存在潜在担忧的方式以衡量机器对文本的理解。</p><p>数据集层面：<br>现有的RC数据集，大部分其发自SQuAD，SQuAD数据集为RC的研究开启了很好的铺垫，但是远不足以探究系统的理解能力。<br>这些数据集仅需要简单的对局部的谓词-论元结构或实体类型的理解，然而理解文本远不止这些，还有比如，在篇章中跟踪实体，理解所读文本暗含的意思，以及恢复作者想要传达的潜在世界模型。</p><p>任务层面：<br>Question Answering是用来探究这些复杂问题的自然形式，但是QA存在很多固有的挑战。<br>具体而言：（在构造数据集时）非常容易写一些看似需要深入理解文本才能回答的问题，但事实上，这些问题会给机器提供词汇或其他线索，让系统在回答问题时绕过预期的推理。<br>这就需要我们在数据收集的过程中，设计一些机制以对抗这样的shortcuts。</p><p>本文的工作：</p><ol><li>证实了问答式阅读理解的合理性；</li><li>并描述了各种问题，可以用来更全面地测试一个系统对一篇文章的理解，而不仅仅是探究局部的predicate-argument结构；</li><li>该方法（问答式阅读理解）的主要陷阱问题是：问题容易找到surface cue，或者其他偏置，而使模型找到推理过程中的捷径（shortcut）<ul><li>本文针对当前文献中提出的mitigate shortcuts 的方法进行了讨论，并对未来数据集收集工作提出了建议。</li></ul></li></ol><h3 id="Defining-Reading-Comprehension"><a href="#Defining-Reading-Comprehension" class="headerlink" title="Defining Reading Comprehension"></a>Defining Reading Comprehension</h3><p>如何定义<strong>理解</strong>一段文本？</p><p>根据[Kendeou and Trevors,2012]<sup id="fnref:00"><a href="#fn:00" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Panayiota Kendeou and Gregory Trevors. 2012. Quality learning from texts we read: What does it take?, pages 251–275. Cambridge University Press.">[00]</span></a></sup>，理想的基准是像人一样：人类在阅读一篇文章时用来恢复某些意义概念的过程。但是这样的过程在计算上并没有明确的定义。</p><p>长期以来，自然语言处理界一直利用<strong>语言形式</strong>（<strong>linguistic formalism</strong>）来表示这一意义，比如：语法树、词义消歧、语义角色、共指消解。然而，没有一种语言形式主义能够抓住段落的全部含义。</p><p>本文借助图灵测试的想法（通过在自然语言中进行交互，agents才能展示他们对语言的理解），提出了一个postulate：</p><blockquote><p>an entity (human or machine) understands a passage of text if it can correctly answer arbitrary questions about that text.</p></blockquote><p>该postulate的内容是理解的充分条件，而非必要条件（还会存在其他定义理解的方式）。</p><p>基于上述的postulate，本文将MRC定义为一个任务，旨在理解一个连贯段落文本，其中会给定系统一个段落和一个关于该段落的问题，并且系统必须给出答案。</p><p>然而利用Questions来评判理解存在几个问题，比如：</p><ol><li>尚不清楚问题的范围应该是什么？</li><li>收集任意的问题是非常有挑战性的，因为那些似乎在探索某种特定理解的问题可能有捷径，可以在没有真正理解文本的情况下正确地回答它们。</li></ol><p>本文后续对这两个问题进行了分析</p><h3 id="What-kinds-of-Questions"><a href="#What-kinds-of-Questions" class="headerlink" title="What kinds of Questions?"></a>What kinds of Questions?</h3><p>本文中列举了一些能够刻画文本理解的高层次问题，目前很少有数据集中显示地提出这些问题。</p><p>1、Sentence-level Linguistic Structure</p><ul><li>目前的工作：鼓励非常局部（单个句子内部）的推理</li><li>超越单个句子：需要结果passage中多个文本片段；<ul><li>E.g.: Drop</li></ul></li><li>关注针对谓词论元结构以外的其他现象：<ul><li>在语义分析中存在着许多丰富的问题，如否定范围、分配与非分配的协调、事实性、指示语、结合与空成分、介词意义、名词复合词等等；</li><li>这些问题都已经有很好的形式化定义；</li></ul></li></ul><p>2、Paragraph-level Structure</p><ul><li>目前的工作：虽然阅读理解数据集的输入是一段文本，但大多数数据集并没有明确针对需要理解整个段落的问题，或者句子如何组合成一个连贯的整体；</li><li>关注段落或语篇层面现象的问题，如实体跟踪、语篇关系或语用学；<ul><li>E.g.: Quoref</li></ul></li></ul><p>3、Grounding and Background Knowledge</p><ul><li>根据已知的知识来理解文本</li><li>人们在阅读特定文本时表现出不同的理解水平，这在很大程度上取决于他们在适当背景知识的下理解文本的能力。</li></ul><p>4、Implicative Reasoning</p><ul><li>理解文本包括理解该文本对可能看到的其他文本的含义（或蕴涵）</li><li>在某种意义上，这可以看作是将文本中的谓词grounding到一些先验知识中，这些先验知识包括该谓词的含义，也包括重建文本所描述的世界模型的更一般的概念；<ul><li>E.g.: ShARC、ROPES</li></ul></li></ul><p>5、Communicative Aspects</p><h3 id="Ways-to-Combat-Shortcuts"><a href="#Ways-to-Combat-Shortcuts" class="headerlink" title="Ways to Combat Shortcuts"></a>Ways to Combat Shortcuts</h3><p>在解决阅读理解任务时有一些捷径shortcuts，使模型能够通过lexical overlap和entity types等表面线索（superficial clues）找到答案</p><p>以下几种方式可以防止这种shortcus：</p><p>1、 Question/Passage Mismatch: 减少 lexical overlap；</p><ul><li>构造问题时，不提供原文，而是提供相同语义的文本；<ul><li>NarrativeQA、DuoRC</li></ul></li><li>先收集问题，再去pair到相应的文本上；<ul><li>QuAC、TriviaQA</li></ul></li><li>更真实的问题；<ul><li>Natural Question、BoolQ</li></ul></li></ul><p>2、“No answer” option</p><p>3、Dialog</p><ul><li>需要额外上下文</li></ul><p>4、Complex Reasoning</p><p>5、Context Construction</p><p>6、Adversarial Construction</p><p>7、Minimal question pairs</p><ul><li>借用语言分析中的“最小对”概念：ROPES；<ul><li>为了避免问题或段落中哪个实体首先出现的细微偏差，或者问题和段落词之间的简单词汇联想偏差，人群工作者被要求对他们写的问题进行最小的修改，以改变答案；</li><li>例如，问题【哪个城市会有更多的树？】可能会改成【哪个城市的树会更少？】。</li><li>这种方法并不适用于所有的阅读理解场景，但它可以成为减少捷径的有效手段：一个单独的问题可能会表现出捷径的特征，但假设最小对中的另一个问题也会有相同的捷径，导致了一个依赖于快捷方式的系统，使其中至少一个错误。</li></ul></li></ul><p>8、Free-form answers</p><ul><li>捷径出现的原因：有限的输出空间，可以搜索到导致正确答案的简单偏差。</li></ul><p>9、Multi-task Evaluation</p><ul><li>在多个数据集上评估模型</li></ul><p>10、Explainability</p><ul><li>要求对阅读理解模型提供的最终答案进行某种解释:<ul><li>E.g.: HotpotQA</li></ul></li></ul><h3 id="Recommendations-for-Future-Research"><a href="#Recommendations-for-Future-Research" class="headerlink" title="Recommendations for Future Research"></a>Recommendations for Future Research</h3><ul><li>创建没有捷径的数据集</li><li>在多数据集上评估模型<ul><li>防止模型过度拟合单个数据集中的统计偏差</li></ul></li><li>防止先验偏差</li><li>检测偏差</li><li>构建对抗性样例</li></ul><h2 id="To-test-MC-start-by-Defining-Comprehension"><a href="#To-test-MC-start-by-Defining-Comprehension" class="headerlink" title="To test MC, start by Defining Comprehension"></a>To test MC, start by Defining Comprehension</h2><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><p>本文贡献：</p><ol><li>本文认为，现有的方法不能充分定义理解，对于测试的内容太不系统。</li><li>本文对一类广泛使用的文本，即简短的叙述，给出了理解的详细定义——理解的模板。</li><li>实验表明现有的系统不能胜任本文中定义的叙事理解任务。</li></ol><h3 id="Defining-deep-story-understanding"><a href="#Defining-deep-story-understanding" class="headerlink" title="Defining deep story understanding"></a>Defining deep story understanding</h3><p>Template of Understanding (ToU):</p><ul><li>一组问题模板，可以填写任何给定段落的特定事件和实体。</li></ul><p>A ToU for Story，针对故事理解:</p><ol><li>Spatial<ul><li>Where are entities positioned over time, relative to landmarks and each other? How are they physically oriented? And where do events take place?</li></ul></li><li>Temporal<ul><li>What events and sub-events occur, and in what order? Also, for what blocks of that timeline do entities’ states hold true?</li></ul></li><li>Causal<ul><li>How do events and states lead mechanistically to the events and states described or implied by the text?</li></ul></li><li>Motivational<ul><li>How do agents’ beliefs, desires, and emotions lead to their actions?</li></ul></li></ol><p>参考：</p><ol><li>四组重叠的故事理解问题，对应于[1]<sup id="fnref:01"><a href="#fn:01" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rolf A. Zwaan, Mark C. Langston, and Arthur C. Graesser. 1995. The construction of situation models in narrative comprehension: An event-indexing model. Psychological Science, 6(5):292–297.">[01]</span></a></sup>确定的人类阅读故事时关注的四个要素。</li><li>后两个问题，来自于计算故事理解的早期工作：[2]<sup id="fnref:02"><a href="#fn:02" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Roger C. Schank and Robert P. Abelson. 1977. Scripts, Plans, Goals and Understanding. Lawrence Erlbaum Associates, Hillsdale, NJ.">[02]</span></a></sup>将causal chains、plans和goals确定为理解多句故事的关键要素。</li></ol><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:00"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">00.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Panayiota Kendeou and Gregory Trevors. 2012. Quality learning from texts we read: What does it take?, pages 251–275. Cambridge University Press.<a href="#fnref:00" rev="footnote"> ↩</a></span></li><li id="fn:01"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">01.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rolf A. Zwaan, Mark C. Langston, and Arthur C. Graesser. 1995. The construction of situation models in narrative comprehension: An event-indexing model. Psychological Science, 6(5):292–297.<a href="#fnref:01" rev="footnote"> ↩</a></span></li><li id="fn:02"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">02.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Roger C. Schank and Robert P. Abelson. 1977. Scripts, Plans, Goals and Understanding. Lawrence Erlbaum Associates, Hillsdale, NJ.<a href="#fnref:02" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2020 | X as Expert</title>
      <link href="/2020/12/26/paper-2020-x-as-expert/"/>
      <url>/2020/12/26/paper-2020-x-as-expert/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><ol><li>Entities as Experts: Sparse Memory Access with Entity Supervision. EMNLP,2020.</li><li>Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge. 2020.</li></ol><h2 id="Entities-as-Experts"><a href="#Entities-as-Experts" class="headerlink" title="Entities as Experts"></a>Entities as Experts</h2><blockquote><p><em>Title</em>: <a href="https://www.aclweb.org/anthology/2020.emnlp-main.400/" target="_blank" rel="noopener">Entities as Experts: Sparse Memory Access with Entity Supervision</a><br><em>Authors</em>: Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, Tom Kwiatkowski<br><em>Org.</em>: Google Research;<br><em>Published</em>: EMNLP,2020.</p></blockquote><h3 id="A-Motivation"><a href="#A-Motivation" class="headerlink" title="A. Motivation"></a>A. Motivation</h3><ul><li>捕获语言模型参数中的实体陈述型知识</li><li>学习、使用专用的独立的实体表示，可以在使用时，直接对目标entity的memory进行读取</li></ul><h3 id="B-Contribution"><a href="#B-Contribution" class="headerlink" title="B. Contribution"></a>B. Contribution</h3><ul><li>设计了一个新的模型结构，可以通过文本，和其他的模型参数一起学习实体表示；</li><li>在开放域QA数据集和LAMA任务上验证了模型性能</li></ul><h3 id="C-Approach"><a href="#C-Approach" class="headerlink" title="C. Approach"></a>C. Approach</h3><p><img src="/../images/paper-2020-x-as-expert/image-e.png" alt="e-expert"></p><p>监督信号：</p><ul><li>mention boundary detection loss：BIO分类</li><li>entity linking loss：用hyperlinked entity去监督entity memory assess？access？</li><li>masked language modeling loss</li></ul><h3 id="D-Experiment"><a href="#D-Experiment" class="headerlink" title="D. Experiment"></a>D. Experiment</h3><ul><li>QA</li><li>Knowledge Probing：LAMA、预测Wikipedia hyperlink</li><li>K近邻的选择</li></ul><h3 id="E-Analysis"><a href="#E-Analysis" class="headerlink" title="E. Analysis"></a>E. Analysis</h3><ul><li>预定义实体词表，存在unseen entity问题</li><li>entity memory layer的训练和主体模型训练是耦合的，训练好的entity memory无法迁移</li><li>针对Wikipedia领域，factual entity 的学习，对常识知识不敏感</li><li>从头训练模型，没有借助任何预先可获取的LM参数</li></ul><h2 id="Fact-as-Experts"><a href="#Fact-as-Experts" class="headerlink" title="Fact as Experts"></a>Fact as Experts</h2><blockquote><p><em>Title</em>: <a href="https://arxiv.org/abs/2007.00849" target="_blank" rel="noopener">Fact as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge</a><br><em>Authors</em>: Pat Verga, Haitian Sun, Livio Baldini Soares, William W. Cohen<br><em>Org.</em>: Google Research;<br><em>Published</em>: 2020.</p></blockquote><h3 id="A-Motivation-1"><a href="#A-Motivation-1" class="headerlink" title="A. Motivation"></a>A. Motivation</h3><p>针对问题：</p><ul><li>知识以隐式参数的形式存在在模型中，无法检查和解释；</li><li>训练语料中事实信息易过时；</li><li>存储在模型参数中的知识，容易受到源材料内在bias的影响；</li><li>文本中无法包含符号化知识</li></ul><h3 id="B-Contribution-1"><a href="#B-Contribution-1" class="headerlink" title="B. Contribution"></a>B. Contribution</h3><ul><li>本文提出了一个神经语言模型，构建了一个显式的接口，连接符号化可解释事实信息和非符号的神经知识；<ul><li>将上下文编码和从外部记忆中抽取的知识整合，这些外部的记忆和符号知识相绑定，可以添加和修改；</li><li>可以在Inference阶段注入新的记忆，处理在pretrain阶段没有见过的实体</li><li>可以用新的事实overwrite先验记忆，迭代更新</li></ul></li><li>本文方法的优势：<ul><li>模型可以通过操纵其符号化表示来更新，而不需要re-training<ul><li>can be inserted by updating the symbolically bound memory</li></ul></li><li>本文的方法可以实现更好的factual coverage，通过向symbolic memory中插入受信的事实内容</li></ul></li></ul><p>与 Entity As Expert工作的不同：</p><ul><li>EaE以end-to-end的方式学习实体表示，本文的方法是采用separatly-trained KB embedding</li><li>本文对EaE进行扩展，引入关于triplets的符号化记忆</li></ul><h3 id="C-Approach-1"><a href="#C-Approach-1" class="headerlink" title="C. Approach"></a>C. Approach</h3><p><img src="/../images/paper-2020-x-as-expert/image-f.png" alt="f-expert"></p><p>fact memory的输入是symbolic KB中的三元组</p><ul><li>三元组由EaE学习的entity embedding组合成</li><li>key-value 记忆的形式<ul><li>key是head set = (subject, realtion)</li><li>value是tail set = (object)</li></ul></li></ul><p>预训练输入：</p><ul><li>构建一个cloze-type QA 任务</li></ul><h3 id="D-Experiment-1"><a href="#D-Experiment-1" class="headerlink" title="D. Experiment"></a>D. Experiment</h3><p>[TBU]</p><h3 id="E-Analysis-1"><a href="#E-Analysis-1" class="headerlink" title="E. Analysis"></a>E. Analysis</h3><ul><li>基于EaE模型之上的工作</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> dataset </tag>
            
            <tag> survey </tag>
            
            <tag> reasoning </tag>
            
            <tag> qa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Use KB for MRC/QA</title>
      <link href="/2020/08/07/note-dev-nlp-with-kg/"/>
      <url>/2020/08/07/note-dev-nlp-with-kg/</url>
      
        <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <div class="hbe-input-container">  <input type="password" id="hbePass" placeholder="" />    <label for="hbePass">Please Enter password to open this blog.</label>    <div class="bottom-line"></div>  </div>  <script id="hbeData" type="hbeData" data-hmacdigest="daca4245e4ed5a1c23718d6c269795bf856053102dd50e55ff5892bea753abf2">b3c5347fd465ea3fda76b5687e46af8e4b9c7691b8e634b365ae495b498527785eaf1f73ba9aa3fa0f0990eef58cb5515d9ef52bf98cf9ce22a7fe38741ff86c65c4e961445f4748e07931cdc9c7bacc0112402db56070698f6845bec3a9501feb5ccab37612d0283c997d3b9d3e089caee690da06973934d530d13349686ebdec2497f3b3ec539ac974ca906313dd73734253cc8f31a502720503ccd4666c610104a3479d5510651205e74f50754725c7398c544dcace24ce46686d68c6f791226880c8693f00b1518f062254d88b0c6d4e7ad7bb95fec0ae2ddbe2abe7f594afa84e0a5c99586bd7bae8967d4a3212418acdf9fcd0a1b155b397049b54a4641374faaa5fd1628b1a4f15f02133c818c19ab157e554593d6542066208c572894b8ca3028bdbf6044fd84e593c406317d6725387fe726e78dfecc345f6b89a37ba29619969411479dcf33dc69c98d1dee85b6866b04d95370324a7b76cf4bbb2848b05043da442d099b45faeb21dec10200e0cc933593218e5c82a59a7e8d7580ea35214ab3d5d0f79eacfd42b0a2f14d75044d6b6888be7e8543d5c75550b98f8c822860714a5ada8ae11090368ea6f49d8e794f0200f81a6706a63884eda4110ef308b6dbda969e5516a5accbb2bed26cd3b5348fdc79fa34c4a589ccb281627f10f8ef5c30e91c8fbe2306514297682763031ca24f9c9b0fd2af4fada66cf557710afe1ad0a5fccf27ef91197e705fb93feb7d3c9a8b815f2f72df1363255938b5ea6379a2fd224aa0a669f5f1778737c4efa640df1e97ea9b5fcf2f2a8bac923deff78f134a52df116501d39e1a323c7613973f2c41d8d4b528e9979c746e115b3f1cf9051e3a58b341238eb99772db45dd7b6fca353e0f194a89bb0b64223151a3cc9f6606cc74e8b1d0de4a519cdc1d030486ed2556f08866ac75123794d6bf46f02c0c33206d2e3c09e6258dccac41b89e086f6cbbf9dbb594e89d3561e059e12d7306792fadb5d82597b448b58cf2da966fdc308d077090d9ee93424e760c03a37e4ca35c09fffd242494d51720f7ba3c37483c212210a8e11fc4d5564e42fcffb1263762a7114e0d43f6dd7b1f4e212badfd78edaeecc5744bb104cf6e43615328841af3cdd0d7089589b59082568f6e1d0b0ad411598c9b44febfa99f8b261c70620e1873290c00a23bc8eaaaed07a938ba1749d16271866e2a1d1cfd5fb5ca84605dacbf1e0b76696e872c36f1be86298ee85bbcdf3223e6ef1fc35e0857bbd6478fb238cd2ffd780e146bee4f4494a77cd6947029d7657984a60fe0e5a5d37a20dc4d48c389aaa80607a87b3cd6ebd557c7040e1b67ff55d04a4b01fe93212d8f6c321e9c857861646b4601365589b5390db6c4d3fcc9796edff548f7e6e4aa4d908e90de29722ca04c1fc8ce1d1e864357a57d8c75e54c3d9bebadc0b1d6c5eb0aa6e4ec25d09ad709b219c47919c3f5b77a1e5823e07bdfc9cec0dab2e7fe250ea45fd70a8b043888ee00e3e6a0e0cf8f63e3fb95058ac3180e0d1e9cd04759ab4977ffa8c9c1d139a72b611abeb2e88bb5a4c6a278b9dc54c902b7d7dace62e63847ee77cb91a3f66e89beb621ca8973ae6cc4ad330446dcb743b51d2702ad5e8eca96fef1b85d92c4989421e1804788889e0ba26fb95b19fa173901ae72656280adced6e002c9e51eebb04d5da3e893767c6bf5e222c7f1ddffdf3298dcc68afd91f8c5ba8fe6fb09db63a7f3e17903c223efe477da090063b03af37f63abe5d20dac3d5123ca3eb78fad200d9d63624630f4a1634b58b57163fe6220d8a86e48d5d6651fca6ba72573e4ef05bc674ce5d89269b000e3cdea4a61839cebc01a57f381af41d347a7df873a721ee2f83c12e67b67c6b0dd83fae6fc475260615be4e8bd2bb7420c236c96fa657f74ab68c8609b258c0dc3682bd27c3bca70134dae90277457f7837656095ee4b1bacc63c09e0daeaabf63c99771dc600dab0ca3ba902907d44f3baf8e7ec9dc8d26b0717cfd9fd6914e470757da10efdbf01469531051eb6123cb2554ad292773b5e3d405d965c0748b7caf6f84057522cbc6476435c406e09ad740d268d5bfe77ed01b33f41104554a089ac43b27807e457b0888c288846987425d454def936a55119dc842f313ed6e2431328119258a8d05f07c0a4e278c4d8366b39e3b8f1fcb0abdc1066bfec765d9e17a9fa0f224eb4e012b8ea3ebb52c5804ef0e123f876003999bfe3bb65c4db403ff4900b793c2a32cfd9ce4657d73d08dc32c5d99e5bc16ece32214d7dc870528dc65d1ef28ba127cc781f062699cf010a4f1f7e37e4feb203beb8666989e2084b5a461d5a6d2cb0685e4e2686873fb36acae5c9f85088c4f6bb9962a9bf575063cebfceae913d2e2779c7b0821ede5efbeb7e9c43daecb74d4a6ccf461e51e53a04e50baf1202f9897168098e344a453c03732d6a6d37e8c5d49c3a30462f589248e7f672abcb2f5573764a960b235de10c52c876f3e970f41442f575cdb059e99864b8221a95b020963d87d20578565d2d542da47e936f124a4a44fb09b7c43b60202fac094b4befe9fe146feaa5a814f7876e60e8d1376a04b1101ce7c46f67110ee06a4722b9fac581ff3ec435ec73f0af0674d4e642a5bfb7e32932afdfb95a76b1b229c5073252ff115fdacb2e4b6c06ea35808a5f5fea72505a887560692a8b679130cbc3db784e65ceea4b78d4cd4c82861fbf8ab82ff0b65bb50d8b527abfe14b05dc7879f708d05c49765e5073e68e1d8580466500a1d42e8e52d1e414b1b77f326d98f812c9582f69456b3c91df26a69c19d4b792dd8339f69a753bbce67fba426c358120721cd497221bfba2bfc3fd95cbb60f20fd714ee0d7212039e1f9a9d23996334c02d640b3be3f06f31a8df4b02b0da2bc217a041fc3135a7690e123e31bb485b34d27c1e842c325ee253926f396ef940f3d11989e5c95f941e5c880dceaf883b8cca012e687bb6b027eec8082623af40b98ab803ca96e2766886d3edcadfc07232b98516500af8780fd4e01501fed2fb772044a40881b242481e6fdd5fa71dddb2d382b1b6180b27b625f1f421acebcf35be141020f546152f992f94f932c837ea7fd98cff54b9cc1f291afe697e8ccc23da1db869ef11aa66d652864743ce443aefef6b8027123e8833ace44ef6163fe9431e744b45fc933b3cf9867b0103b7d2145c170cfb4a26b8c92bca376e02a611f3f4c8827dc6aacd65d11636e2a6ef7cc10e1c0aa40308a2178d318a2bfb7e0b71c02b8c12730a6658917f9b75fc6d1a833f7b415e861f49a7a15645d7cb77c543d11ecc64acbf01d5ccf3c3facff409eb565888d421a453658e610a6a1c64047df6deb1afe6cae26f26ce4cfd90b73d3000dd761a99647c2cb559d55e6972c5485f332b344198b6ce689820692303a80edf026d611490b6d446385a3d7fb6537856c2c7d32d5b79097d34d416dc5ac241d8586f8c5c69bcfcb2f2888d7f108ee86d01761a5bc4b8f8d003f1c9f5cf9cae709a7e90e024371e43051669e3cf630c704d01ea60a0e0428038f68317687cd3f3621bf29f542ef33f7c43b5c2a0e8cebf639de9faca3cdc4b13686bd67cbec83e67ca7a6bc8231c965f8754a32599a3c7c4445dafbd1712ad0fa25d9774c6a46de232426d97a7cdb927d562a5f16bb3baf8ec3c52ac32bb5d5d2becaef90a379deed706592b82d7a64ef6b2ea398ef0a802e99787a16931523a4e61a4e0a5325bf92bcd48dc2bcb8e706c106f2b9085a5a68fdc31bafebbaf2ceea0e2a02e5307ffe8f7ad874a71badfeeebf4ee9f206cf7a168c01e633bd8696c4faf3a2e1d6465f8a3584e9908cf1ae47a94568ea41ce420e4973e408253d73b8c15a463209885d98511820c3e514bc0003da73f74636b831904af65c762a81a71e4fdc6da6f144c5caa20c296e55948873da22c91bbfe0c1ce6b47c99cf030127015cb0d4a50c75fc26a96fa63d64ec05e52893cc2fc39e5d7eae541be9e3f3724f9851911d78acf93c7f278cb656313210518fe4798264f0fbaa35e6b440f54e14ed0444a6d9e62ea6a3f6c77caeebd4c9a11b9803c0f7099086166cc5da9ca3e86bd1eb46c793bdc916ea5a713c3dcd582d976c6508534e94c6217f91fabe38ab26965ff35e9d2dcfc1501c0ec9953a82ce0d3604e78194ba965c68bd3d2622d04b9b3026b2fc8943a6cf59a2187d7fd21309796f853578f497837925dd13ee5a51451a191012a08c5084c8d7a37ea96e721cd7d0c8ddc705d63ad3fe7366dc4d8748e944bb954c99c202d0be540204030207b2e42bc57dc1df1e780468b324c31d8e3a357358d3e90c1c2a9358bfaf43d1766fb3a98cf68e0eddfc539c1364ee0dcdd1bdb796013923d45e10f39b2bb33f010861c79a3dd72c77e915fbf02ecd7fd053b5ece13191e1a57c470437632f719c7443db6752a11daed4c51fd7f820c7f954f4e2d44b91cfbfba7db2c531affb17451594d524ddc1940e787dba78550b0a299aa785c20fda36180c33c67d96c0431a4da6797cc1016f8a937bed60d1dfffc6bb4ef8b1940cb0fdb4f2b0a3ed49306d286fc252527ec141a68cf5f336e2911ee377f0d7cb9191036876acb1b8ae7379b966b81c33c64f6fea86b808252cecbcf2861b547b8d3eb9a4d7746c0ccddaad523c83798358cb1512714468101dac13ecae0b2a71ebbb0223e0647d40d4d2f35335d5071adcd90878de1355d235e2aad5b87fdc828b62fdee6315f2137a557c3cabac96e5dcc63d6d641bd62b64714645e66fe6ebefb97c8e5d98cee9e988eef62402df091835b4d890eac3c88edb6be7ee1390bb076e9698374d72c87c743cea3cbc22529af538517b904bb6c0daf1e3b8d50a46aad38e1b8d25ccccde6b976f3718d0356c9b086ef8518f8d048397a5814b3e1634d3695180ac788f2fcfc2f8200d662b6ae4478733e6f097dbbaa302c10cafcf94e39c555208712f5ce7ac900f4982774000681681b79ba013307850a70dfc009578cd199a0932c5aeb04d606a7fd2a125bc056f28c30751a10f316792e9fb0496bb4d6f68a71c3c571b71f68ae49f96d63c3a0f069ddf2f2492120b06966c2997e604e970c521a5ebb04ebb95e2b39c6a3be4b3809e2fe64c7b94b6a0f6f7854da416ede56d02791ba1f6bac6d1f86de7d43917c546e2bb7016e336e56429a485bedf44e0b72e7392f68c1eaa4a48a8355c139a180e566a8cf0036ea16eaee73894e7efb85b9366aa37715afaa8388c447439f30d87537dc81bca8e855fb855b58f9802ddda2ce931960c2e657bf666385f7c1bdc427a65b7bd935c6a1efa794891671c6155c026f56d12ff03ed92554af6a37f0137b3fa92c97aee66c3eb4a2fe45ed563163d922f32d3a91a70c8a7f941c48e72308182d43e5e43f6eb33f33f7d6676d224330d6bdf2bde711bdf763bf06417c652d3e09c8a5bb2b376728d2bcc6b18ba750cd8f5bdf0ca5b5d6b2540ee4a73dc9bd359f8336a26365840d5149d3d93e717964b03f8a735c7dc84b3e2ced983810e6a8ae0bc0aeb7c24735031bc183c6329a0efa552066db9a3ad23a2c9790a5d8b313680e1dbc8e2d451d8a31832ef2871217a6b1d23f1dbfd562e14ee18deac20546c9c7e8883214cdcacbe7ffa5e6c223ed47620aafdfa2df6904fcaaae226328a7858f9b0772523c92536591c56abea678987359ba84b8c5a4da261adf03c0e7d213cefa64ae9d13993709177fc9d41504317a8de3fbe552500fd2629335bb3e9589beb3777de6164b264f004815f180b03aa65bfbfef9f651f00525d0a1812987fcfb279f2a22832789e0aa19d2d98e126f49c317ec2af9c34afff273c7860f3f3d7f7a20a9bcc34deda3dd120912ae694fd57b5ccc342a1e5ae12614c48be9aaf96a85e05a1e03c2c3d67dab077aef2c13000d07b803caed74c04787aa6eb3107c98b9ec33fba336a0f11e5645befdd50d6c372b45fb37b097a8b3742f426112adda741dd187243aea24eb2bd202a87b7e1baed2fe681dc4cbdacf570f6440441859a769fbfb8d2fdea1f003c3b12950f76e01412f05c057867ab501b8bf45ab7aa1debad911089d0e2a4ec800fd48b214dcbe303fb387abaa8e1e3af1184756001bc3e093152e8fd7932f52520b275ad419f8959a73389b8ea438b36de34dcba41704e7bf92f3e0539bb87579469bc1e02a897347549637a1e85398ee0f9f097bdec46540c9bf0449840d118d90c67cbbf16eeb03a8c5b35d666f7045af2796e2b9efcf2aefbc08af81fc63958b2fce8f9d2a9072a3383b691717e4161f0342f5b945115b73a42a0e57b3df9bd12d84a41959e33b1cd07730b702637ebf18932bcaee769608c958ae754c279f9c8d350a66409afcf60718a7a855af092bdf377e3f2bee6fac2b3995a2b4360f674d4049280e23363b988324b7d0b94f92237988a2bbe768a48fc6d13b16ba5f5978b060825be72a1ec28d66f662e6ee323734f7b6621c3162dd79dea217494756a62fb54402c187a83b6471f99e78ac1cbdd362cf0e2346833cd3e43c7b918d9428b357885a748df1aef76cfe82dc6dbd0fcbd47f881ecab5835dfab42b90a1519421b7d26acb8473c29a6396d505ed566a97b18316d597baf5f11ce020c7a5892499f2b436150153db2dd6a1c5ca50adb32f1ddb48a59eae22a630b98bd65d8bd32865d1d6987b1dbbed8ad16a00449f45cbaeda77af186df1c1aad57688d3a2cb1c85c4c9f200b54da3150f6708cad043d546d47527a9dd81cbee9bdd303f5eda409b20f646f643998ee3dc090962b9229badc66c0ab247cb28d6c921f83d902a2e2603a98c80bffc06f32ab2a62df1bdd2772d6a1642399e5d01284b787937cc5878dea8922e8135bbe8ac3dd50cf925e37f2583f186f7ceec4a121c156d7a9b73197a2b904a23b8e720161909478db6b5fcbdff5f0c1f490cdd5b71a9b16b6c428924b5862034465146775147a2ce5faa174609aa254ec4eb0175cde3f34409a50b48228e5fc4c0274fe9f851ee26e731a8bf5b30b4b5a44d7e76876c3fe5eeee1fe5140c4e9eef905715aa221902287f6aa501a2387997fe78a0fe5002ec42de5a381ffeeae4713369add075ecef3fe189e1592301606aa877d806e53db42b97295fc0a5f1796f3590de773e609fe868237b8d3cce939820fbd57a91f6c044d817ffff3a68f5e6b4c50352e64c6db5616dd48f9777f0c0cfa06f5e814f40fc9ce997bb6db4c76774952354f05ca37af41cb202f643163b894452e1da08b805304ff71d9f33625689d207d6e2d0519ba8166462ba88517689fbe9e4dfbfd48a6fdb4ccf37031fbdd67868fd0f87c255d81bd0de2cbed7c98ab50de34c77fd176436eb295ecdbfbcfe8515bbe7892490df03b331864a113eda587244cd18d976aea5dddd614e372703c42539f4ad736ca083bbbceaadad8e53fd7caa9e283a99825efa36723e6ddc30ccd7ed39f4e6ef28509d95c68085181935555fc9f2ce98e41fc36c2e7aba9c30abf49a586469d6f3a8a35fa1f46af7362754847514f6eb0474ce5dbb6386ed01418eee28b7a33562cbd6b60d267e4a6eb3a58d637d45ece47e379ab2ed0a4dcede035b834a9b32cfbef976ffed210d2ed1e318e0834d09b608afa98db4833898a8668c0f4bd8f9e6d4227f847f5be702b4962807635e38c439f164d48f3960a687bdabf5bee7d5c23a601ed9948c1b1312239163130d9b191d8f05d61862664ce716a969245b394c417b9838992be380c82aa91f8263365e0e23bd7c8df979fa7597602e69227bcd928b91f96aaee48cc42966202ff125bc5a7c56ac99ccc84c5f49a015d9230db83bf669dce10ab656638b036608e7198233d802246185d6c634ae936850c00a8b2383665ff7c89e31a904dc835a4fbd0265aac86f586a363e320b7f2c3bfd6479b87602a3b7957809d48e7c5fcde2e1cf566af1d65297cfa88066a92cceccac2bd05525bfd4ff071f21012e2d2bacfa0bf1c936464949c25705bc5c3d0d40968a5f10f3bec6a94ff2a224194406984b6e3aaa07961f00da4034b7eeaabe8e81c74a6da9c7ec3f1508f0d1a72d193dae122bf566eef50eb3c98c69dca232ed1dfff6ad792fb234b705c9ce2efc038e78301caf58de65ac2b0eafce27c92a275798addf8d001c8e22c8fa228eb27a818d550a4bfc850490a576555e5c932b2444a43f02c00b9f7b95c0d2d3f63c4e0dd47b02845d036c7c868db80e981e6f6ef4820eeb8a8698bcc503a0eeb1524b0638fa817d32b5cdfb7dee29aa9c237123befb92f5f8ba078f54ae9887e113528abcfe057227b6357b0935c72d90819a21fae1bdc13a6d4e24ce1c510ad12a1d1ca7082f0373326865af37a136051672a6a449148c8794ba271e0955e2ddae5fff9a42bb75bac616aaf96d76d2b84b8497ba068cc1ae7feb939d171c7ed4db01b39038f83f00cb3bfa465245d04a15fb50d1177f8f4ab6e1778588aecad12d03bbec03ca9e6c6d5e2a34b8ea17f9be6e8d0df447c90b7c3533b87210fc0a9a11c973e24e47c884809a1f5078821e11a8bf436f32b61461d7b842bce09e7340dfbb27c262c7b7dc4a48e4304a166c0aaad0c430c107fc1e4989428e1f949d39233cd3614a05c140c6bfaa6756194705ba60beeea9fe7416a658c38f13abcd8d5f5cc9a9337a26278c13b21896fca1f5acf1d07befcf3429ab71f8f90288264dd54fedab33aba21415f78eeccc66c9846cab35ba75a5825fda461f7ced5b302152b7518d0472acfbe6652518a6d10d1c54fe51712193cb7f6c8b4ab3001047de0ecbd234ff6d3d4a9332d1f8a8fa89ac00a15281811caa317658e5c90398eaef1d3f2829e7b6383e339ca1091816cc3b8f2cd795f93ad044f27b7e47e0ad83cef5cab13ae8faa1d057b244718dae947625e187322a9d407b93e6e7970aa8a3fbaae4619f140ddc56302f45ac5d95374ad80d9eb87f3c3e5c80c363b7b86afc0a74a094d5ed97d4da52ec206f7582d4f732354cca2f4775aca2ccb786adb7632d90ab4d907ee766c7d470a77f6f11f4916f662fd5689cc3b36c7d90cc0fa8b6186b71e66a102510bb583f38d4740e57d19bfe78e2b91ff8da2161fe61e6b53e28eed4b4972ad5f59801f3bc978d17c9da5762d503670d44c58389cee288ea6826bbbc33187552b9095e28fb9fa1dc9a77367298ba8f60cc689112089cec7762f42d737cf7816f00e069dc854d1220e433ebf97a9aed432cecc4905c7ace32bbfe91051e19085de2c695c956be2d15e324693ca4eb51d7d3dd48e0be7aea98e78531c467e9eecc86adba30ac808968e4cceef3a26ad36bce39c3a480807bd8538cd03ecc56b32ab4206c88d6de615c7e794ba65cdf1b8d213942970bf6db86f9a9dde462730da9e8eaac388e5c95eba58b52650485519407ddc8447ebff393c19ecf8ad63b51eb53bf79b29ac50b4a9c0c3fb4f32ba2d7396edb619cba5e7925046537428f2471da5f808d6fa67e6ab2bda41b2f07957c3767186df40699fffe93f8e88bf6faf022b7a0d10794e65edc09869b533729722174dc45a5bd0a3ec4227213b97addd9d39d358ca2e4261b0fa7a7fe775a4953c5cfe077284760f238c04d7c13d02a0689e4d286e5bab0e89447e0b9319a5576a533ccc573676e86524df51b1fd98a98629a9b13d1ea14852a68c4ff5320e8b46d38d970128e5e336898a731cdc96c8bb297662672ef7e9b8107880af07bccedd91a3d7a333efc4b2d943e34856c238c0d8e984874e79a590ebadf7cd79a8df4c0ec5cd77b8a79a9c471213c695c4146866c172a1306b4a49c676d5eede7a230f1be94b064395d56999ea303b97e080fd5d7b78272c1ae93985c7de101c15b8293642335e3a7f89293dc6c0bad329828f24ef662e763c073659dcb140e1874bb0c17312d2ac619b2f77f39fd89957ebafd7fe825c4201089fb3a385069356b15a1b3dd279cbb7b86224ff18d2ed97602aaa54b7288761bf248b82cc37a4e9a5a6922627437f7df174790f27c52bdaf162b3c8668ccfcb1516cc1d7a38d2e12b8592ed6d2d4818c72459848aa3d449b31f046a66f4e7ae101b81cf9e725e818bab2d24dd3ee54d448d6c67673ecfb261d8a4465df20f8aa49af2755ff62b4756af51073d2d0cd318e7c51ac711d841b3867228231f5ce0861ccdf43f2dd0e0f4f525d4858d5ccb7e04b186766fe132c4fe371614bed409aeea1735e80fd6ad3ee82906ef2fe1e9c</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> note </tag>
            
            <tag> dev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pre-trained Models with Adapter</title>
      <link href="/2020/06/24/note-2020-ptm-with-adapter/"/>
      <url>/2020/06/24/note-2020-ptm-with-adapter/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Injecting Knowledge into Pre-trained Models through Adapter<br><a id="more"></a></p><p>什么是Adapter<sup id="fnref:0"><a href="#fn:0" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="AdapterHub: A Framework for Adapting Transformers. 2020.">[0]</span></a></sup></p><blockquote><p>“Adapter” refers to a set of newly introduced weights, typically within the layers of a transformer model. Adapters provide an alternative to fully fine-tuning the model for each downstream task, while maintaining performance. They also have the added benefit of requiring as little as 1MB of storage space per task!<br>Adapter，适配器指一组新引入的参数权重，通常在transformer模型的层内/间。适配器为每个下游任务提供了一种完全微调模型的替代方法，同时保持了性能。它们还有一个额外的好处，就是每个任务（的网络）只需要少量的存储空间.</p></blockquote><h2 id="Adapter的结构（Architecture）"><a href="#Adapter的结构（Architecture）" class="headerlink" title="Adapter的结构（Architecture）"></a>Adapter的结构（Architecture）</h2><ul><li>A Two-layer feed-forward neural network with a bottleneck<ul><li>Down-Projection</li><li>（may be a nonlinearity）</li><li>Up-Projection</li></ul></li><li>Layer Norm</li></ul><p>现有两种形式：Pfeiffer Architecture<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="AdapterFusion: Non-Destructive Task Composition for Transfer Learning. 2020.">[2]</span></a></sup> 和 Houlsby Architecture<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Parameter-Efficient Transfer Learning for NLP. ICML,2019.">[1]</span></a></sup></p><ul><li><img src="/images/note-2020-ptm-with-adapter/fig-arch.png" alt="arch"></li></ul><h2 id="Why-Adapter【AdapterHub】"><a href="#Why-Adapter【AdapterHub】" class="headerlink" title="Why Adapter【AdapterHub】"></a>Why Adapter【AdapterHub】</h2><blockquote><p> Adapters provide numerous benefits over fully fine- tuning a model such as scalability, modularity, and composition. </p></blockquote><h3 id="1-Task-specific-Layer-wise-Representation-Learning"><a href="#1-Task-specific-Layer-wise-Representation-Learning" class="headerlink" title="1\ Task-specific Layer-wise Representation Learning."></a>1\ Task-specific Layer-wise Representation Learning.</h3><ul><li>为了实现SotA的性能，大多数工作都对预训练模型进行整体微调；<ul><li>而Adapter仅通过适应/调节每层的表示，就展示出了与整体模型微调相当的效果；</li></ul></li></ul><h3 id="2-Small-Scalable-Shareable"><a href="#2-Small-Scalable-Shareable" class="headerlink" title="2\ Small, Scalable, Shareable"></a>2\ Small, Scalable, Shareable</h3><ul><li>微调整体模型，需要将完整的结果模型存储，存储开销大；<ul><li>可根据需要调整adapter内部的bottleneck size，极大地减少了需要存储的新的参数量；</li></ul></li></ul><h3 id="3-Modularity-of-Representations"><a href="#3-Modularity-of-Representations" class="headerlink" title="3\ Modularity of Representations"></a>3\ Modularity of Representations</h3><ul><li>adapter学习编码任务相关信息，而不需要指定参数；</li><li>adapter可作为模块化组建的原因：<ul><li>由于适配器的封装布局（encapsulated placement），其中周围的参数是固定的，在每一层，适配器被迫学习与变压器模型的后续层兼容的输出表示。</li></ul></li><li>MAD-X的工作，成功将任务、语言独立训练的adapters进行了组合；</li></ul><h3 id="4-Non-Interfering-Composition-of-Information-（不冲突的信息组合）"><a href="#4-Non-Interfering-Composition-of-Information-（不冲突的信息组合）" class="headerlink" title="4\ Non-Interfering Composition of Information. （不冲突的信息组合）"></a>4\ Non-Interfering Composition of Information. （不冲突的信息组合）</h3><ul><li>跨任务共享信息</li><li>多任务学习的问题：<ul><li>灾难性遗忘（catastrophic forgetting）<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://blog.csdn.net/u013468614/article/details/95623987">[3]</span></a></sup><ul><li>序列迁移过程中，早期学习的知识被覆盖；</li></ul></li><li>灾难性干扰（catastrophic interference）<ul><li>增加新任务时，原有任务的性能下降；</li></ul></li></ul></li><li>Adapter封装的特性，不同的任务信息存储在了各自的参数中</li></ul><h2 id="引入Adapter的优势"><a href="#引入Adapter的优势" class="headerlink" title="引入Adapter的优势"></a>引入Adapter的优势</h2><ul><li>减少了任务相关的（训练）参数量，不需要调整PLM参数；</li><li>不会在re-training整体模型时产生灾难性遗忘；</li><li>尽可能在减少参数量的同时，保持与MTL相同的性能；<ul><li>可以序列化训练多任务；</li><li>增加新的任务时，不需要重新训练之前所有的子任务；</li></ul></li><li>预训练/下游任务上学习到的知识可以以模块化的方式进行迁移；</li></ul><h2 id="Adapter-Pre-train-Fine-tune-过程中的N个要点"><a href="#Adapter-Pre-train-Fine-tune-过程中的N个要点" class="headerlink" title="Adapter Pre-train / Fine-tune 过程中的N个要点"></a>Adapter Pre-train / Fine-tune 过程中的N个要点</h2><ul><li>与原始的PLM相比，需要调整的参数；<ul><li>PLM是否fixed，训练adapter；</li></ul></li><li>Adapter的预训练任务？</li><li>PLM+Adapter组合之后，在下游任务上的使用，Task Specific Layer是什么？</li><li>在哪些阶段训练；<ul><li>多任务训练？单任务训练？序列型训练？</li></ul></li></ul><h2 id="NLP领域相关工作"><a href="#NLP领域相关工作" class="headerlink" title="NLP领域相关工作"></a>NLP领域相关工作</h2><ol><li>Parameter-Efficient Transfer Learning for NLP. ICML,2019.</li><li>BERT and PALs: Projected Attention Layers for Efﬁcient Adaptation in Multi-Task Learning. ICML,2019.</li><li>K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters. 2020.</li><li>Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers. 2020.</li><li>AdapterFusion: Non-Destructive Task Composition for Transfer Learning. 2020. (与MAD-X同一作)</li><li>MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer. 2020. (与AdapterFuison同一作)</li><li>Out-rageously Large Neural Networks: The Sparsely- Gated Mixture-of-Experts Layer. ICLR,2017.</li><li>Simple, scalable adaptation for neural machine translation. EMNLP,2019.</li><li>Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work? ACL,2020.</li></ol><h2 id="其他领域-CV-相关工作"><a href="#其他领域-CV-相关工作" class="headerlink" title="其他领域(CV)相关工作"></a>其他领域(CV)相关工作</h2><ol><li>Learning multiple visual domains with residual adapters. NeurIPS,2017.</li><li>On Self Modulation For Generative Adversarial Networks. ICLR,2019.</li></ol><!-- References:http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&mid=2247484460&idx=1&sn=87f3f00e6730bb363d3b77fd092c3e3a&chksm=97aee2fea0d96be8a3c0c07a2891bd802fa78e02cb73026a088dd5b67177ea3507cf0166915d&mpshare=1&scene=1&srcid=&sharer_sharetime=1583897742597&sharer_shareid=63af200aa81df8e5ac8dfb942e96e35d#rd --><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:0"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">0.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">AdapterHub: A Framework for Adapting Transformers. 2020.<a href="#fnref:0" rev="footnote"> ↩</a></span></li><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Parameter-Efficient Transfer Learning for NLP. ICML,2019.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">AdapterFusion: Non-Destructive Task Composition for Transfer Learning. 2020.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://blog.csdn.net/u013468614/article/details/95623987<a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> pre-trained-lm </tag>
            
            <tag> network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文摘|人工智能从感知智能向认知智能演进</title>
      <link href="/2020/06/24/excerpt-20200619-damotrend-1/"/>
      <url>/2020/06/24/excerpt-20200619-damotrend-1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>阿里巴巴达摩院2020十大科技趋势 Part 1<br><a id="more"></a></p><blockquote><p>文章来源: <a href="https://yq.aliyun.com/download/3881?do=login&amp;accounttraceid=57dfe29465bc4163982fe4b0217e6b97uiey" target="_blank" rel="noopener">link</a><br>阿里巴巴达摩院2020十大科技趋势 Part 1</p></blockquote><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>人工智能已经在“听、说、看”等感知智能领域达到或超越了人类水准，但在需要外部知识、逻辑推理或者领域迁移的认知智能领域还处于初级阶段。<br>认知智能将从认知心理学、脑科学及人类社会历史中汲取灵感，并结合跨领域知识图谱、因果推理、持续学习等技术，建立稳定获取和表达知识的有效机制，让知识能够被机器理解和运用，实现从感知智能到认知智能的关键突破。</p><h2 id="趋势解读"><a href="#趋势解读" class="headerlink" title="趋势解读"></a>趋势解读</h2><p>近些年来，人工智能已经在感知智能上取得了长足的进步，甚至在许多领域已经达到货超出了人类的水准，解决了“听、说、看”的问题。但对于需要外部知识、逻辑推理或者领域迁移等需要“思考和反馈”的问题，仍然存在诸多难题去攻破。<br>相较于感知智能（人工智能1.0），人工智能2.0将更多机遇数据，自动将非结构化的数据转变为结构化的知识，做到真正意义上的认知智能。探索如何保持大数据智能优势的同时，赋予机器常识和因果逻辑推理能力，实现认知智能，成为当下人工智能研究的核心。<br>认知智能的机制设计非常重要，包括如何建立有效的机制来稳定获取和表达知识，如何让知识能够被所有模型理解和运用。这需要从认知心理学、脑科学以及人类社会的发展历史中汲取更多的灵感，并结合跨领域知识图谱、因果推理、持续学习等研究领域的发展进行突破。<br>认知智能将结合人脑的推理过程，解决复杂的阅读理解问题和少样本的知识图谱推理问题，协同结构化的推理过程和非结构的语义理解。它也需要解决多模态预训练问题，帮助机器获得多模感知能力，赋能海量任务。</p><p>大规模图神经网络将被认为是推动认知智能发展强有力的推理方法。<br>图神经网络将深度神经网络从处理传统非结构化数据（如图像、语言和文本序列等）推广到更高层次的结构化数据（如图结构等）。大规模的图数据可以表达丰富并蕴含逻辑关系的人类常识和专家规则，图节点定义了可理解的符号化知识，不规则图拓扑结构表达了图节点直接的依赖、从属、逻辑规则等推理关系。<br>以保险和金融风险评估为例，一个完备的AI系统需要分析个人的履历、行为习惯、健康程度等，还需要通过其父母、亲友、同事、同学之间的来往数据和相互评价进一步进行信用评估和推断。基于图结构的学习系统能够利用用户之间、用户与产品之间的交互，做出非常准确的因果和关联推理。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>未来人工智能热潮能否进一步打开天花板，形成更大的产业规模，认知智能的突破是关键。<br>认知智能可以帮助机器跨越模态理解数据，学习到最接近人脑认知的“一般表达”，获得类似于人脑的多模感知能力，有望带来颠覆性的产业价值。<br>认知智能的出现使得AI系统主动了解事物发展的背后规律和因果关系，而不再只是简单的统计拟合，从而进一步推动实现下一代具有认知能力的AI系统。</p>]]></content>
      
      
      
        <tags>
            
            <tag> ai </tag>
            
            <tag> report </tag>
            
            <tag> cognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Knowledge Enriched Pre-trained Models</title>
      <link href="/2020/05/13/nlu-kn-e-ptm/"/>
      <url>/2020/05/13/nlu-kn-e-ptm/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><strong>KnE-PTM</strong><br><a id="more"></a></p><p>E: Enriched / Enhanced</p><h2 id="Main-Related-Work"><a href="#Main-Related-Work" class="headerlink" title="Main Related Work"></a>Main Related Work</h2><ol><li>Pre-trained Models for Natural Language Processing: A Survey. 2020.</li><li>ERNIE(THU)<ul><li>ERNIE: enhanced language representation with informative entities. In ACL, 2019.</li></ul></li><li>KnowBERT<ul><li>Knowledge enhanced contextual word representations. EMNLP-IJCNLP, 2019.</li></ul></li><li>K-BERT<ul><li>K-BERT: Enabling language representation with knowledge graph. AAAI,2020.</li></ul></li><li>KEPLER<ul><li>KEPLER: A uniﬁed model for knowledge embedding and pre-trained language representation. 2019.</li></ul></li><li>WKLM<ul><li>Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model. ICLR, 2020.</li></ul></li></ol><h2 id="Introduction-of-KnE-PTM"><a href="#Introduction-of-KnE-PTM" class="headerlink" title="Introduction of KnE-PTM"></a>Introduction of KnE-PTM</h2><p>PTMs通常是从general-purpose large-scale text corpora中学习universal language representation，但是缺少 domain-specific knowledge。</p><p>从外部知识库向PTM中引入领域知识是一种有效的方式。</p><h3 id="Note1-Pre-training-的优势："><a href="#Note1-Pre-training-的优势：" class="headerlink" title="Note1: Pre-training 的优势："></a>Note1: Pre-training 的优势：</h3><ol><li>在大规模文本语料库上进行预训练，可以学习通用语言表示并辅助下游任务；</li><li>预训练提供了更好的模型初始化，通常会带来更好的泛化性能，并加速对目标任务的收敛；</li><li>预训练可以看作是一种正则化，以避免模型在小数据集上过拟合；</li></ol><h3 id="Note2-Pre-training-任务："><a href="#Note2-Pre-training-任务：" class="headerlink" title="Note2: Pre-training 任务："></a>Note2: Pre-training 任务：</h3><ol><li>LM with Maximum-likelihood estimation (MLE)；</li><li>Masked Language Modeling (MLM)<ul><li>uni/bidirectional MLM</li><li>Seq2Seq MLM</li><li>E-MLM： span boundary objective、span order recovery</li></ul></li><li>Permuted Language Modeling (PLM)</li><li>Denoising Autoencoder (DAE)；<ul><li>token masking</li><li>token deletion</li><li>text infilling</li><li>sentence permutation</li><li>document rotation</li></ul></li><li>Contrastive Learning (CTL)<ul><li>Deep InfoMax</li><li>Replaced Token Detection (RTD)</li><li>Next Sentence Prediction (NSP)</li><li>Sentence Order Prediction (SOP)</li></ul></li></ol><h3 id="外部知识库类型及相关工作："><a href="#外部知识库类型及相关工作：" class="headerlink" title="外部知识库类型及相关工作："></a>外部知识库类型及相关工作：</h3><ul><li>引入 linguistic 知识的工作：<ul><li>LIBERT<sup id="fnref:01"><a href="#fn:01" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Informing unsupervised pretraining with external linguistic knowledge. 2019.">[01]</span></a></sup><ul><li>lexical semantic relations：wordnet、babelnet；</li><li>add task：lexical relation classiﬁcation (LRC) -&gt; linguistic constraints task.</li></ul></li><li>Sentilr<sup id="fnref:02"><a href="#fn:02" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Linguistic knowledge enhanced language representation for sentiment analysis. 2019.">[02]</span></a></sup><ul><li>将 MLM 扩展为 Label-aware MLM，引入每个词的情感极性；</li></ul></li><li>KnowBERT.</li><li>K-adapter<sup id="fnref:03"><a href="#fn:03" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="K-adapter: Infusing knowledge into pre-trained models with adapters. 2020.">[03]</span></a></sup></li></ul></li><li>引入 semantic 知识（实际上是语言学知识）<ul><li>SenseBERT<sup id="fnref:04"><a href="#fn:04" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="SenseBERT: Driving some sense into BERT. 2019.">[04]</span></a></sup><ul><li>利用wordnet，在预测 masked tokens 的同时，还预测其在 wordnet 中的supersenses；</li></ul></li></ul></li><li>引入 commonsense 知识<ul><li>A knowledge-enhanced pretraining model for commonsense story generation. 2020.<ul><li>针对特定下游任务</li></ul></li></ul></li><li>引入 factual 知识<ul><li>ERNIE:-THU<ul><li>利用预训练的KGE通过实体提及链接到文本，增强文本表示；</li></ul></li><li>KnowBERT.<ul><li>联合BERT训练和entity linking任务；</li></ul></li><li>K-BERT.<ul><li>显式地从KG中抽取三元组扩展输入句子（树形）</li></ul></li><li>WKLM.<ul><li>通过实体替换识别任务，鼓励模型感知事实型知识；</li></ul></li><li>KEPLER.<ul><li>联合优化 KE 和 LM 的目标函数；</li><li>通过 实体表示，引入KG的结构信息；</li></ul></li></ul></li><li>引入 domain-specific 知识<sup id="fnref:05"><a href="#fn:05" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Integrating graph contextualized knowledge into pre-trained language models. 2019.">[05]</span></a></sup><sup id="fnref:06"><a href="#fn:06" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="BioBERT: a pre-trained biomedical language representation model for biomedical text mining. 2019.">[06]</span></a></sup><sup id="fnref:07"><a href="#fn:07" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="SciBERT: A pretrained language model for scientiﬁc text. EMNLP,2019.">[07]</span></a></sup><sup id="fnref:08"><a href="#fn:08" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="PatentBERT: Patent classiﬁcation with ﬁne-tuning a pre-trained BERT model. 2019.">[08]</span></a></sup></li></ul><h3 id="引入知识的方法"><a href="#引入知识的方法" class="headerlink" title="引入知识的方法"></a>引入知识的方法</h3><ul><li>在 pre-training 阶段引入外部知识<ul><li>BERT之前，早期工作，联合学习 KGE 和 word embeddin；</li><li>BERT之后，增加 auxiliary pre-training 任务：<ul><li>pros.</li><li>cons.：<ul><li>更新PTM的参数，导致在引入<strong>多种类型</strong>的知识时发生灾难性遗忘；<ul><li>仅适配单一知识；</li></ul></li></ul></li></ul></li></ul></li><li>引入外部知识而不从头训练PTM<ul><li>在下游任务中应用：<ul><li>K-BERT：侧重输入格式的处理</li><li>KT-NET：结合预训练的KGE完成QA任务；</li></ul></li><li>intermediate pre-training 的方式；<ul><li>针对下游任务的需求，</li></ul></li></ul></li><li>将LM扩展为KG-conditioned LM (knowledge graph conditioned LM)<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Barack’s wife hillary: Using knowledge graphs for fact-aware language modeling. ACL, 2019.">[14]</span></a></sup><sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Latent relation language models. 2019.">[15]</span></a></sup></li></ul><h2 id="Probe-Knowledge-in-LM"><a href="#Probe-Knowledge-in-LM" class="headerlink" title="Probe Knowledge in LM"></a>Probe Knowledge in LM</h2><h3 id="Probe-Linguistic-Knowledge"><a href="#Probe-Linguistic-Knowledge" class="headerlink" title="Probe Linguistic Knowledge"></a>Probe Linguistic Knowledge</h3><ul><li>BERT 在 句法任务上表现出色<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="What do you learn from context? probing for sentence structure in contextualized word representations. ICLR, 2019.">[1]</span></a></sup><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Linguistic knowledge and transferability of contextual representations. NAACL,2019.">[2]</span></a></sup> ;<ul><li>eg: POS Tag、Constituent Label</li><li>BERT 在 语义任务和细粒度句法任务上稍逊色；</li></ul></li><li>BERT 的已知能力：<ul><li>subject-verb agreement <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Assessing BERT’s syntactic abilities. 2019.">[3]</span></a></sup>；</li><li>semantic roles <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. TACL,2020.">[4]</span></a></sup>；</li><li>encode sentence structure (dependency trees, constituency trees)<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="A structural probe for ﬁnding syntax in word representations. NAACL,2019.">[5]</span></a></sup><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="What does BERT learn about the structure of language? ACL,2019.">[6]</span></a></sup>；</li></ul></li></ul><h3 id="Probe-World-Knowledge"><a href="#Probe-World-Knowledge" class="headerlink" title="Probe World Knowledge"></a>Probe World Knowledge</h3><ul><li>采用 <code>fill-in-the-blank</code> 的形式构造输入查询 BERT；<ul><li>LAMA (Language Model Analysis) <sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Language models as knowledge bases? EMNLP,2019.">[8]</span></a></sup> <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="How can we know what language models know? 2019.">[9]</span></a></sup> <sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="BERT is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised QA. 2019.">[10]</span></a></sup> <sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Negated LAMA: birds cannot ﬂy. 2019.">[11]</span></a></sup></li><li>relational knowledge <sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Inducing relational knowledge from BERT. 2019.">[12]</span></a></sup>;</li><li>commonsense knowledge <sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Commonsense knowledge mining from pretrained models. EMNLP,2019.">[13]</span></a></sup>；</li></ul></li></ul><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:01"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">01.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Informing unsupervised pretraining with external linguistic knowledge. 2019.<a href="#fnref:01" rev="footnote"> ↩</a></span></li><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">What do you learn from context? probing for sentence structure in contextualized word representations. ICLR, 2019.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:02"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">02.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Linguistic knowledge enhanced language representation for sentiment analysis. 2019.<a href="#fnref:02" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Linguistic knowledge and transferability of contextual representations. NAACL,2019.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:03"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">03.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">K-adapter: Infusing knowledge into pre-trained models with adapters. 2020.<a href="#fnref:03" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Assessing BERT’s syntactic abilities. 2019.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. TACL,2020.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:04"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">04.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">SenseBERT: Driving some sense into BERT. 2019.<a href="#fnref:04" rev="footnote"> ↩</a></span></li><li id="fn:05"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">05.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Integrating graph contextualized knowledge into pre-trained language models. 2019.<a href="#fnref:05" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">A structural probe for ﬁnding syntax in word representations. NAACL,2019.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:06"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">06.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">BioBERT: a pre-trained biomedical language representation model for biomedical text mining. 2019.<a href="#fnref:06" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">What does BERT learn about the structure of language? ACL,2019.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction. 2020.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:07"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">07.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">SciBERT: A pretrained language model for scientiﬁc text. EMNLP,2019.<a href="#fnref:07" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Language models as knowledge bases? EMNLP,2019.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:08"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">08.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">PatentBERT: Patent classiﬁcation with ﬁne-tuning a pre-trained BERT model. 2019.<a href="#fnref:08" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">How can we know what language models know? 2019.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">BERT is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised QA. 2019.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Negated LAMA: birds cannot ﬂy. 2019.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Inducing relational knowledge from BERT. 2019.<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Commonsense knowledge mining from pretrained models. EMNLP,2019.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Barack’s wife hillary: Using knowledge graphs for fact-aware language modeling. ACL, 2019.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Latent relation language models. 2019.<a href="#fnref:15" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> NLU </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> nlu </tag>
            
            <tag> survey </tag>
            
            <tag> pre-trained-lm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2020 | Unsupervised Commonsense Question Answering with Self-Talk</title>
      <link href="/2020/04/26/paper-2020-2004-05483-self-talk/"/>
      <url>/2020/04/26/paper-2020-2004-05483-self-talk/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: <a href="http://arxiv.org/abs/2004.05483" target="_blank" rel="noopener">Unsupervised Commonsense Question Answering with Self-Talk</a><br><em>Authors</em>: Vered Shwartz, Peter West, Ronan Le Bras , Chandra Bhagavatula, Yejin Choi.<br><em>Org.</em>: AI2<br><em>Published</em>: null<br><em>Code</em>: <a href="https://github.com/vered1986/self_talk" target="_blank" rel="noopener">https://github.com/vered1986/self_talk</a></p></blockquote><p>近期arxiv更新的一篇来自AI2的工作：基于self-talk进行无监督的常识问答</p><p>self-talk，启发自 inquiry-based discovery learning <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Jerome S Bruner. 1961. The act of discovery. Harvard educational review, 31:21–32.">[1]</span></a></sup>，核心是构造一些query，来挖掘KB中的相关知识，得到的相关知识作为探索的结果，用于辅助目标任务。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p><img src="/images/paper-2020-2004-05483-self-talk/1.png" alt="1"><br><img src="/images/paper-2020-2004-05483-self-talk/2.png" alt="2"></p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="/images/paper-2020-2004-05483-self-talk/3.png" alt="3"><br><img src="/images/paper-2020-2004-05483-self-talk/4.png" alt="4"><br><img src="/images/paper-2020-2004-05483-self-talk/5.png" alt="5"><br><img src="/images/paper-2020-2004-05483-self-talk/6.png" alt="6"><br><img src="/images/paper-2020-2004-05483-self-talk/7.png" alt="7"><br><img src="/images/paper-2020-2004-05483-self-talk/8.png" alt="8"><br><img src="/images/paper-2020-2004-05483-self-talk/9.png" alt="9"><br><img src="/images/paper-2020-2004-05483-self-talk/10.png" alt="10"><br><img src="/images/paper-2020-2004-05483-self-talk/11.png" alt="11"></p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="/images/paper-2020-2004-05483-self-talk/12.png" alt="12"><br><img src="/images/paper-2020-2004-05483-self-talk/13.png" alt="13"><br><img src="/images/paper-2020-2004-05483-self-talk/14.png" alt="14"></p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Jerome S Bruner. 1961. The act of discovery. Harvard educational review, 31:21–32.<a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> note </tag>
            
            <tag> pre-trained-lm </tag>
            
            <tag> qa </tag>
            
            <tag> commonsense </tag>
            
            <tag> zero-shot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2020 | Natural Language QA Approaches using Reasoning with External Knowledge</title>
      <link href="/2020/04/02/paper-2020-survey-nlqa-r-with-kn/"/>
      <url>/2020/04/02/paper-2020-survey-nlqa-r-with-kn/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: <a href="http://arxiv.org/abs/2003.03446" target="_blank" rel="noopener">Natural Language QA Approaches using Reasoning with External Knowledge</a><br><em>Authors</em>: Chitta Baral, Pratyay Banerjee, Kuntal Pal, Arindam Mitra<br><em>Org.</em>: Arizona State University, Microsoft;<br><em>Published</em>: unpublished<br><em>Another Version</em>: <a href="https://github.com/CogIntLab-ASU/NLQASurvery/blob/master/IJCAI__PRICAI__2020__Reasoning_approaches_in_NLQA_that_requires_external_knowledge.pdf" target="_blank" rel="noopener">link</a></p></blockquote><h2 id="1-Introduction-and-Motivation"><a href="#1-Introduction-and-Motivation" class="headerlink" title="1. Introduction and Motivation"></a>1. Introduction and Motivation</h2><ul><li>Various NLQA datasets<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="The website https://quantumstat.com/dataset/dataset.html has a large list of NLP and QA datasets.">[1]</span></a></sup>, research on knowledge acquisition in the NLQA context, and their use in various NLQA models have brought the issue of <strong>NLQA using “reasoning” with external knowledge</strong> to the forefront.</li><li>Understanding text often requires knowledge beyond what is explicitly stated in the text.</li></ul><p>Examples from various datasets illustrate the need for “reasoning” with external knowledge in NLQA.:</p><ul><li>Winograd</li><li>bAbI<ul><li>To answer the above question one needs knowledge about:<ul><li>directions and their opposites;</li><li>the effect of actions of going in speciﬁc directions;</li><li>composing actions (i.e., planning) to achieve a goal.</li></ul></li></ul></li><li>ProPara<ul><li>Processes are actions with duration. Reasoning about processes, where they occur and what they change is somewhat more complex.</li></ul></li><li>LifeCycleQA<ul><li>The knowledge needed in many question answering domain can be present in unstructured textual form. But often speciﬁc words and phrases in those texts have “deeper meaning” that may not be easy to learn from examples.</li></ul></li><li>OpenBookQA / SocialIQA / PiQA<ul><li>some datasets where it is explicitly stated that external knowledge is needed to answer questions in those datasets.</li><li>DARPA MCS (machine commonsense) program Allen AI has developed 5 different QA datasets where reasoning with common sense knowledge (which are not given) is required to answer questions correctly.</li></ul></li><li>LSAT / GMAT<ul><li>One of the most challenging natural language QA task is solving grid puzzles.</li><li>Building a system to solve them is a challenge as it requires precise understanding of several clues of the puzzle, leaving very little room for error.</li></ul></li></ul><h2 id="2-Knowledge-Repositories-and-their-creation"><a href="#2-Knowledge-Repositories-and-their-creation" class="headerlink" title="2. Knowledge Repositories and their creation"></a>2. Knowledge Repositories and their creation</h2><p>Repositories of Unstructured Knowledge:</p><ul><li>Any natural language text or book or even the web can be thought of as a source of unstructured knowledge.<ul><li>Wikipedia Corpus: 4.4M articles;</li><li>Toronto BookCorpus: 11K books;</li></ul></li><li>unstructured commonsense knowledge<ul><li>Aristo Reasoning Challenge (ARC): 14M science sentences;</li><li>WikiHow Text Summarization: 230K articles and summaries extracted from the online WikiHow website;</li><li>ROCStories</li></ul></li></ul><p>Repositories of Structured Knowledge:</p><ul><li>Yago / NELL / DBPedia / ConceptNet</li><li>Wikitionary: multilingual dictionary describing words using deﬁnitions and descriptions with examples</li><li>WordNet</li><li>ATOMIC / VerbPyhsics / WebChild<ul><li>collections of commonsense knowledge.</li></ul></li></ul><h2 id="3-Reasoning-with-external-knowledge-Models-and-Architectures"><a href="#3-Reasoning-with-external-knowledge-Models-and-Architectures" class="headerlink" title="3. Reasoning with external knowledge: Models and Architectures"></a>3. Reasoning with external knowledge: Models and Architectures</h2><h3 id="3-1-Extracting-the-External-Knowledge"><a href="#3-1-Extracting-the-External-Knowledge" class="headerlink" title="3.1 Extracting the External Knowledge"></a>3.1 Extracting the External Knowledge</h3><p>1 - Neural Language Models</p><p>2 - Word Vectors</p><p>3 - Information/Knowledge Retrieval:</p><ul><li>The knowledge retrieval step consists of:<ul><li>search keyword identiﬁcation;</li><li>initial retrieval from the search engine;</li><li>then depending on the task, a knowledge re-ranking step;</li></ul></li></ul><p>4 - Semantic Knowledge Ranking/Retrieval:</p><ul><li>The knowledge sentences retrieved through IR are re-ranked further using <strong>Semantic Knowledge Ranking/Retrieval</strong> (<strong>SKR</strong>) models;</li><li>Neural Networks are used to rank knowledge sentences:<ul><li>These neural networks are trained on the task of semantic textual similarity (STS), knowledge relevance classiﬁcation or natural language inference (NLI).</li></ul></li></ul><p>5 - Existing Structured Knowledge Sources</p><p>6 - Hand Coded Knowledge</p><p>7 - Knowledge learned using Inductive Logic Programming (ILP)</p><h3 id="3-2-Models-and-Architecture-for-NLQA-with-External-Knowledge"><a href="#3-2-Models-and-Architecture-for-NLQA-with-External-Knowledge" class="headerlink" title="3.2 Models and Architecture for NLQA with External Knowledge"></a>3.2 Models and Architecture for NLQA with External Knowledge</h3><blockquote><p>NLQA systems with external knowledge can be grouped based on how knowledge is expressed (structured, free text, implicit in pre-trained neural networks, or a combination) and the type of reasoning module (symbolic, neural or mixed).</p></blockquote><p>1 - Structured Knowledge and Symbolic Reasoner</p><p>2 - Neural Implicit Knowledge with Neural Reasoners</p><p>3 - Structured Knowledge and Neural Reasoners</p><ul><li>Structured knowledge can be in the form of trees (abstract syntax tree, dependency tree, constituency tree), graphs, concepts or rules.</li><li>Tree-based LSTM</li><li>GNN</li></ul><p>4 - Free text Knowledge and Neural Reasoners</p><ul><li>Memory Network</li></ul><p>5 - Free text Knowledge and Mixed Reasoners</p><ul><li>neuro-symbolic approaches</li></ul><p>6 - Combination of Knowledge and Mixed Reasoners</p><h2 id="4-Discussion-How-Much-Reasoning-the-Models-are-Dong"><a href="#4-Discussion-How-Much-Reasoning-the-Models-are-Dong" class="headerlink" title="4. Discussion: How Much Reasoning the Models are Dong?"></a>4. Discussion: How Much Reasoning the Models are Dong?</h2><ul><li>Commonsense Reasoning</li><li>Multi-Hop Reasoning</li><li>Abductive Reasoning</li><li>Quantitative and Qualitative Reasoning</li><li>Non-Monotonic Reasoning</li></ul><h2 id="5-Conclusion-and-Future-Directions"><a href="#5-Conclusion-and-Future-Directions" class="headerlink" title="5. Conclusion and Future Directions"></a>5. Conclusion and Future Directions</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">The website https://quantumstat.com/dataset/dataset.html has a large list of NLP and QA datasets.<a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> dataset </tag>
            
            <tag> survey </tag>
            
            <tag> reasoning </tag>
            
            <tag> qa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 | Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering</title>
      <link href="/2020/04/01/paper-2019ai2-comet-csqa/"/>
      <url>/2020/04/01/paper-2019ai2-comet-csqa/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Title: <a href="https://arxiv.org/abs/1911.03876" target="_blank" rel="noopener">Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering</a><br>Authors: Antoine Bosselut, Yejin Choi<br>Org.: AI2<br>Published: null</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>理解叙事体裁的文本 (narratives) 需要对文本中描述情境 (situation) 的隐式因果 (causal and effect) 、状态 (state) 进行动态推理。<br>而仅根据文本中明确说明的信息只能得到平凡的情境细节。</p><p>以 “they went to the club” 这句 statement 为例，其背后还蕴含了相关的常识信息 (commonsense expectations)，如:</p><ul><li>they had to get dressed;</li><li>they were going dancing;</li><li>they likely had drinks;</li></ul><p>这其中<strong>涉及到/需要关于社会 (social) 和物理 (physical) 世界的丰富背景知识</strong>。</p><p>相关工作侧重于：利用从大规模知识库中检索得到的知识来增强神经网络模型；</p><ul><li>这些工作的前提是: 可以通过实体链接，联结文本和 KB ;</li><li>但是，通过规范化输入中的实体，会丢失输入中的关键上下文，导致抽取得到的是语义无关知识；</li></ul><p>面临的核心挑战是：<strong>如何按需访问上下文相关的知识并对其进行推理</strong>。</p><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><p>本文针对 zero-shot commonsense QA 进行研究，将该任务转化为：<strong>在动态产生的知识图上进行概率推理</strong>；</p><ul><li>与之前的工作 (从静态KB中检索已存在的知识进行知识整合) 不同，本文侧重于：上下文相关的知识可能不存在于现有的KB中；</li><li>提出了一种，使用生成式神经常识知识模型 (COMET) 按需生成上下文相关知识的方法。</li><li>本文方法的重点：生成知识 -&gt; 构建知识图</li><li>本文方法的优势：不需要规范实体来连接到一个静态的知识图谱，知识模型可以直接用于查询知识 (生成需要的知识)</li></ul><p>实验数据集：</p><ul><li>socialIQA</li><li>story commonsense</li></ul><p>补充：本文是在 COMET<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="COMET: Commonsense Transformers for Automatic Knowledge Graph Construction. ACL,2019.">[1]</span></a></sup> 工作的基础之上展开的，也可以看做是 COMET 在 Commonsense QA 领域的应用。</p><h2 id="Method-Dynamic-Construction-of-Intermediate-Knowledge-Graphs"><a href="#Method-Dynamic-Construction-of-Intermediate-Knowledge-Graphs" class="headerlink" title="Method: Dynamic Construction of Intermediate Knowledge Graphs"></a>Method: Dynamic Construction of Intermediate Knowledge Graphs</h2><p>给定一个 raw context，COMET 生成常识推理 (即，生成与context中描述情境相关的世界知识) ，此推理作为额外的 context 用于 a) 给候选答案打分；b) 生成更多的常识推理。</p><p>COMET 生成的常识推理，连接 raw context 和候选答案，从而动态的构建起一个 KG ：</p><ul><li>raw context 为 KG 的 root 节点；</li><li>候选答案为 leaf 节点；</li><li>生成的常识推理为推理路径；</li><li>可以用COMET生成常识推理的打分作为路径/边的权重；</li></ul><p>最后，基于这个动态构建的 KG 进行概率推理，确定最终的答案。</p><p>整体的流程图：<br><img src="/images/paper-2019ai2-comet-csqa/fig-2.png" alt="arch"></p><h3 id="1-Building-the-Graph"><a href="#1-Building-the-Graph" class="headerlink" title="1. Building the Graph"></a>1. Building the Graph</h3><p>生成 COMET 推理：</p><ul><li>COMET输入：context $c$ 和 ATOMIC relation type $r$；</li><li>输出：candidates $\mathcal{G}$, 每个 $g \in \mathcal{G}$ 的打分 $\phi_g$</li><li>计算：$\phi_g = \frac{1}{|g|}\sum_{s=1}^{|g|}log P(y_s | y_{&lt;s}, c, r)$<ul><li>$y_s$ 是 $g$ 中的token；</li><li>$|g|$ 是 长度；</li></ul></li><li>每生成一个 $g$ 可以看成是基于 context 的 <strong>1-hop 推理</strong>；</li><li>根据 Markov 假设，将一步操作进行扩展，关于 context 的 l-hop 推理：<ul><li>$\phi_g^l = \phi_g^{l-1} + \frac{1}{|g^l|}\sum_{s=1}^{|g^l|}log P(y_s | y_{&lt;s}, g^{l-1}, r)$</li></ul></li><li>初始状态：<ul><li>$g^0 = c$, $\phi_g^0 = 0$</li></ul></li></ul><p>计算答案打分：</p><ul><li>对于任意 $g\in \mathcal{G}$, 可以利用上述公式为每个后续答案计算一个分数：<ul><li>$\phi_a = \frac{1}{|a|}\sum_{t=1}^{|a|} log P(x_t | x_{&lt;t}, q, g)$</li></ul></li><li>对于每一个 $g \in \mathcal{G}^l$ , 都可以计算一个答案打分：<ul><li>$\phi_{ga}^l = \frac{1}{|\mathcal{G}^l|} \sum_{m=1}^{|\mathcal{G}^l|} \gamma_g \phi_g^{m,l} + \gamma_a \phi_a^{m,l}$</li><li>$|\mathcal{G}^l|$ 是 l-hop 推理生成的推理数量</li><li>对于生成的 $g_m^l \in \mathcal{G}^l$<ul><li>$\phi_g^{m,l}$ 对应为路径打分;</li><li>$\phi_a^{m,l}$ 对应为答案打分;</li><li>$\gamma_g, \gamma_a$ 为超参数；</li></ul></li></ul></li><li>$\phi_{ga}^l$ 可以看成是答案 $a$ 在推理路径 $\{c \rightarrow g^1 \rightarrow … \rightarrow g^l \}$ 下的打分；</li><li>定义基于生成推理的最大似然：<ul><li>(随着推理步数/推理层级 $L$ 的增加，生成的推理数量 $|\mathcal{G}^l|$ 也会增加，将很难为每个生成推理计算打分 $\phi_{ga}$ )</li><li>$\phi_{ga_{max}}^l = max_{m\in [0,|\mathcal{G}^l|)} \gamma_g \phi_{g}^{m,l} + \gamma_a \phi_a^{m,l}$</li><li>$[0,|\mathcal{G}^l|)$ 是所有可被索引的生成推理的范围；</li></ul></li></ul><h3 id="2-Evaluating-the-Graph"><a href="#2-Evaluating-the-Graph" class="headerlink" title="2. Evaluating the Graph"></a>2. Evaluating the Graph</h3><p>Probabilistic Reasoning</p><ul><li>当不同推理层级答案打分 $\{\phi_{ga}^l\}_0^L$ 计算出来后，可以通过计算边缘概率获得每个答案的最终打分，并选择得分最高的作为最终答案：<ul><li>$log P(a|q,c) \propto \phi_{ens} = \sum_{l=0}^L \beta_{ga}^l \phi_{ga}^l$</li><li>$\hat{a} = argmax_{a\in \mathcal{A}} \phi_{ens}$</li><li>$\beta_{ga}^l$ 是超参数，用于缩放每一hop的打分对最终的贡献；</li><li>$\phi_{ga}^0$ 是直接利用context计算的答案打分；</li></ul></li></ul><p>Overcoming Answer Priors</p><ul><li>考虑到某些候选答案在某些问题上出现的概率很高，而且与上下文无关；</li><li>利用PMI重新定义 $\phi_a$：<ul><li>$\phi_a \propto PMI(a,g|q)$</li><li>$\phi_a = \frac{1}{|a|} \sum_{t=1}^{|a|} (log P(x_t | x_{&lt;t},q,g) - log P(x_t | x_{&lt;t}, q))$</li></ul></li></ul><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>训练COMET: 使用 GPT2 (345M)作为 PLM 模型，解码策略默认为 argmax decoding；</p><p>超参数选择:</p><ul><li>$\gamma_g = \gamma_a = 1.0$</li><li>在 SocialIQA中: $\beta^l = 1.0$</li><li>在 Story Commonsense中: $\beta^l = 0.0$</li></ul><p>实验结果与分析：参见论文</p><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">COMET: Commonsense Transformers for Automatic Knowledge Graph Construction. ACL,2019.<a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> tg </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> pre-trained-lm </tag>
            
            <tag> commonsense </tag>
            
            <tag> zero-shot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Note | Capsule Network</title>
      <link href="/2020/04/01/note-nn-capsule/"/>
      <url>/2020/04/01/note-nn-capsule/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Capsule Network 相关学习资料：<br><a id="more"></a></p><p>视频：</p><ul><li><a href="https://www.bilibili.com/video/BV1eW411Q7CE?p=1" target="_blank" rel="noopener">李宏毅-capsule network</a></li></ul><p>blog:</p><ul><li><a href="https://kexue.fm/archives/4819" target="_blank" rel="noopener">1-苏剑林-揭开迷雾，来一顿美味的Capsule盛宴</a></li><li><a href="https://kexue.fm/archives/5112" target="_blank" rel="noopener">2-苏剑林-再来一顿贺岁宴：从K-Means到Capsule</a></li><li><a href="https://kexue.fm/archives/5155" target="_blank" rel="noopener">3-苏剑林-三味Capsule：矩阵Capsule与EM路由</a></li><li><a href="https://www.tinymind.cn/articles/61" target="_blank" rel="noopener">https://www.tinymind.cn/articles/61</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> ML-NN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nn </tag>
            
            <tag> ml </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EMNLP2019 | Heterogeneous Graph Attention Networks for Semi-Supervised Short Text Classification</title>
      <link href="/2020/03/09/paper-emnlp2019-hgat/"/>
      <url>/2020/03/09/paper-emnlp2019-hgat/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: <a href="https://www.aclweb.org/anthology/D19-1488" target="_blank" rel="noopener">Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification</a><br><em>Authors</em>: Linmei Hu, Tianchi Yang, Chuan Shi, Houye Ji, Xiaoli Li.<br><em>Org.</em>: BUPT, IIR(Singapore).<br><em>Published</em>: EMNLP, 2019.</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Towards Shor Text Classification tasks, there are following Challenges:</p><ol><li>short text are semantically sparse and ambigous, lacking contexts <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Xuan-Hieu Phan, Le-Minh Nguyen, and Susumu Horiguchi. 2008. Learning to classify short and sparse text & web with hidden topics from largescale data collections. In WWW, pages 91–100. ACM.">[1]</span></a></sup>;</li><li>the labeled training data is limited, which leads to traditional and neural supervised methods ineffective;</li><li>need to <strong>capture the importance of different information</strong> that is incoporated to address sparsity at multiple granularity levels, and <strong>reduce the weights of noisy information</strong> to achieve more accurate classification results;</li></ol><h3 id="This-work"><a href="#This-work" class="headerlink" title="This work"></a>This work</h3><p>Propose a novel <strong>Heterogeneous GNN based method</strong> for <strong>semi-supervised</strong> short text classification:</p><ul><li>make full use of both limited labeled data and large unlabeled data by allowing information propagation through our automatically constructed graph;</li><li>propose a flexible <strong>HIN</strong> framwork to model short texts;<ul><li>incorporate any <strong>additional information</strong> (e.g., entities and topics);</li><li>capture the <strong>rich relations among the texts and the additional information</strong>;</li></ul></li><li>propose <strong>Heterogeneous Graph Attention networks</strong> (HGAT) to embed the HIN for the short text classifiaction;<ul><li>consider heterogeneity of different node types;</li><li>a new <strong>dual-level</strong> attention mechanism;<ul><li><strong>node</strong>-level: capture the importance of different neighboring nodes (reducing the weights of noisy information);</li><li><strong>type</strong>-level: capture the importance of different node (information) types to a current node;</li></ul></li></ul></li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="HIN-for-Short-Texts"><a href="#HIN-for-Short-Texts" class="headerlink" title="HIN for Short Texts"></a>HIN for Short Texts</h3><p>An example of HIN: <img src="/../images/paper-emnlp2019-hgat/1-hin-example.png" alt="hin-ex"></p><p>additional information:</p><ul><li>topics;</li><li>entities;</li></ul><p>Notation:</p><ul><li>HIN: $\mathcal{G} = (\mathcal{V},\mathcal{E})$<ul><li>$\mathcal{V} = D \cup T \cup E$<ul><li>Short Texts: $D = {d_1, …, d_m}$</li><li>Topics: $T={t_1, …, t_K}$</li><li>Entities: $E = {e_1,…,e_n}$</li></ul></li></ul></li></ul><p>HIN constructing:</p><p>Topic: LDA</p><ul><li>each topic $t_i = (\theta_1,…,\theta_w)$<ul><li>w denotes the Vocabulary size</li><li>is represented probability distribution over words</li></ul></li><li>Nodes: for each document, top-$P$ topics with the largest probabilities.</li><li>Edges: the edge between a document and a topic is built if the document is assigned to the topic.</li></ul><p>Entity: </p><ul><li>use TAGME<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://sobigdata.d4science.org/group/tagme/">[2]</span></a></sup> to recognize the entities in document and map them to wikipedia;</li><li>entity representation:<ul><li>take an entity as a whole word and learn the entity embeddings using word2vec 2based on the Wikipedia corpus.</li></ul></li><li>Edge:<ul><li>edge between a document and an entity is built if the document contains the entity.</li><li>relations between entities.<ul><li><strong>to further enrich the semantics of short texts and advance the information propagation;</strong></li><li>if the similarity score (cosine similarity) between two entities is above a predeﬁned threshold $\delta$, build an edge between them.</li></ul></li></ul></li></ul><h3 id="HGAT"><a href="#HGAT" class="headerlink" title="HGAT"></a>HGAT</h3><p>HGAT model: <img src="/../images/paper-emnlp2019-hgat/2-hgat.png" alt="hgat"></p><h4 id="1-Heterogeneous-Graph-Convolution"><a href="#1-Heterogeneous-Graph-Convolution" class="headerlink" title="1.Heterogeneous Graph Convolution:"></a>1.Heterogeneous Graph Convolution:</h4><p>Node Features: $X\in \mathbb{R}^{|v| \times q}$, $x_v \in \mathbb{R}^q$</p><p>Adjacency Matrix: $A^{\prime} = A + I$ (add self-connections)</p><p>Degree Matrix: $M$, $M_{ii} = \sum_j A^{\prime}_{ij}$</p><p>GCN Layer-wise propagation rule:</p><ul><li>$H^{(l+1)} = \sigma (\tilde{A} \cdot H^{(l)} \cdot W^{(l)})$<ul><li>$\tilde{A} = M^{- \frac{1}{2}} A^{\prime}  M^{- \frac{1}{2}}$ (symmetric normalized adjacency matrix)</li><li>$\sigma$ = ReLU</li><li>$H^{(l)} \in \mathbb{R}^{|v| \times q}$</li></ul></li></ul><p>Node Features:</p><ul><li>For document: use TF-IDF as features</li><li>For topic: word distribution $s_t = \{\theta_i\}_{i=[1,w]}$</li><li>For entity: concat its embedding and TF-IDF vector of its wikipedia description text;</li></ul><p>GCN-HIN Layer-wise propagation rule:</p><ul><li>$H^{(l+1)} = \sigma (\sum_{\tau \in \Tau}  \tilde{A}_{\tau} \cdot H^{(l)}_{\tau} \cdot W^{(l)}_{\tau})$<ul><li>$\Tau$: heterogeneous types</li><li>$\tilde{A} \in \mathbb{R}^{|V| \times |V_{\tau}|}$ is the submatrix of $\tilde{A}$;</li></ul></li></ul><h4 id="2-Dual-level-Attention-Mechanism"><a href="#2-Dual-level-Attention-Mechanism" class="headerlink" title="2.Dual-level Attention Mechanism"></a>2.Dual-level Attention Mechanism</h4><p>principle:</p><ul><li>the neighboring nodes of the same type may carry more useful information.</li><li>different neighboring nodes of the same type could also have different importance.</li></ul><p>Type-level: <strong>learns the weights of different types of neighboring nodes</strong>.</p><ul><li><p>embedding of the type $\tau$: $h_{\tau} = \sum_{v\prime} \tilde{A}_{vv^{\prime}} h_{v^{\prime}} $</p><ul><li>sum of the neighboring node features $h_{v^{\prime}}$  </li><li>$v^{\prime} \in \mathcal{N}_v$ are with the type $\tau$;</li></ul></li><li><p>type-level attention scores:</p><ul><li>$ a_{\tau} = \sigma( \mu^{T}_{\tau} \cdot [h_v || h_{\tau}] )$<ul><li>$h_v$ =  current node embedding</li><li>$||$ = concatenate</li><li>$\sigma$ = Leaky ReLU</li></ul></li></ul></li><li>normalizing: $\alpha_{\tau} = \frac{ exp(a_{\tau}) }{ \sum_{\tau^{\prime} \in \Tau} exp(a_{\tau^{\prime}}) }$</li></ul><p>Node-level: <strong>capture the importance of different neighboring nodes and reduce the weights of noisy nodes.</strong></p><ul><li>a specific node $v$ with the type $\tau$ and its neighboring node $v^{\prime} \in \mathcal{N}_v$ with type $\tau^{\prime}$</li><li>node-level attention scores:<ul><li>$b_{vv^{\prime}} = \sigma ( v^T \cdot \alpha_{\tau^{\prime}} [h_v || h_{v^{\prime}}] )$</li></ul></li><li>normalize: $\beta_{vv^{\prime}} = \frac{ exp(vv^{\prime}) }{ \sum_{i\in \mathcal{N}_v}exp(vi) }$</li></ul><p>add Dual-level into GCN-HIN:</p><ul><li>$H^{(l+1)} = \sigma (\sum_{\tau \in \Tau}  \mathcal{B}_{\tau} \cdot H^{(l)}_{\tau} \cdot W^{(l)}_{\tau})$<ul><li>$\mathcal{B}_{\tau}$: attention matrix from $\beta_{vv^{\prime}}$</li></ul></li></ul><h4 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h4><p>$Z = softmax(H^{(L)})$</p><ul><li>$H^{(L)}$ = short text embeddings </li></ul><p>Objective:</p><ul><li>$L = - \sum_{i\in D_{train}} \sum_{j=1}^C Y_{ij}\cdot log Z_{ij} + \eta ||\Theta||_2 $<ul><li>$C$ = number of classes</li><li>$\Theta$ = model parameters</li><li>$\eta$ = regularization factor</li></ul></li></ul><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Xuan-Hieu Phan, Le-Minh Nguyen, and Susumu Horiguchi. 2008. Learning to classify short and sparse text &amp; web with hidden topics from largescale data collections. In WWW, pages 91–100. ACM.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://sobigdata.d4science.org/group/tagme/<a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> note </tag>
            
            <tag> gnn </tag>
            
            <tag> text-classification </tag>
            
            <tag> semi-supervised </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Retrieval Approach for Open-Book style Question Answering</title>
      <link href="/2020/03/05/papers-ir-obqa/"/>
      <url>/2020/03/05/papers-ir-obqa/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><!-- # Retrieval Approach for Open-Book style Question Answering --><a id="more"></a><h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><ol><li>Alignment over Heterogeneous Embedding for Question Answering. NAACL,2019.</li><li>Careful Selection of Knowledge to solve Open Book Question Answering. ACL,2019.</li><li>Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering. EMNLP,2019.</li></ol><h2 id="AHE"><a href="#AHE" class="headerlink" title="AHE"></a>AHE</h2><blockquote><p><em>Title</em>: <a href="https://aclweb.org/anthology/papers/N/N19/N19-1274/" target="_blank" rel="noopener">Alignment over Heterogeneous Embedding for Question Answering</a><br><em>Authors</em>: Vikas Yadav, Steven Bethard, Mihai Surdeanu<br><em>Orig.</em>: University of Arizona<br><em>Published</em>: NAACL,2019.<br><em>Code</em>: <a href="https://github.com/vikas95/AHE" target="_blank" rel="noopener">https://github.com/vikas95/AHE</a></p></blockquote><h3 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>some methods have fallen out of focus (strong unsupervised benchmarks), when comparing with neural approaches;<ul><li>e.g.: alignment approaches</li></ul></li><li>previous work adapted alignment methods to operate over word representations<ul><li>rely on uncontextualized word representations (GloVe)</li><li>—&gt; <strong>alignment approaches are more meaningful today after the advent of contextualized word representations</strong></li></ul></li></ul><h3 id="2-This-Work"><a href="#2-This-Work" class="headerlink" title="2.This Work"></a>2.This Work</h3><p><strong>Alignment over Heterogeneous Embeddings</strong> (AHE): a simple, fast, and <strong>mostly unsupervised</strong> approach for <strong>non-factoid</strong> question answering (QA)</p><ul><li>uses an off-the-shelf information retrieval (IR) component to retrieve likely supporting paragraphs from a knowledge base (KB) given a question and candidate answer.</li><li>aligns each word in the <strong>question and candidate answer</strong> with the most similar word in the <strong>retrieved supporting paragraph</strong>;</li><li>weighs each alignment score with the IDF of the corresponding question/answer term;</li><li><strong>AHE’s overall alignment score is the sum of the IDF weighted scores of each of the question/answer term.</strong></li><li>AHE’s similarity/alignment function operates over embeddings that model the underlying text at <strong>different levels of abstraction</strong>:<ul><li>character: FLAIR -&gt; unsupervised</li><li>word: BERT/GloVe -&gt; unsupervised</li><li>sentence: InferSent -&gt; supervised</li><li>capture substantially different semantic information;</li></ul></li><li>a simple meta-classifier: learns how much to trust the predictions over each representation, further improves the performance of unsupervised AHE;</li><li>Target Dataset:<ul><li>ARC: multiple-choice QA;</li><li>WikiQA: answer selection (select the sentences containing the correct answers);</li></ul></li></ul><h3 id="3-Method"><a href="#3-Method" class="headerlink" title="3.Method"></a>3.Method</h3><p>The core component of our approach computes the score of a candidate answer by aligning two texts.</p><p>Inputs: </p><ul><li>multiple-choice task: <ul><li>sent1: question concatenated with the candidate answer;</li><li>sent2: supporting paragraph;</li></ul></li><li>answer selection task:<ul><li>sent1: question;</li><li>sent2: candidate answer;</li></ul></li></ul><p>Overall architecture:</p><ul><li><img src="/../images/papers-ir-obqa/image-20200304214221063.png" alt="image-20200304214221063"></li></ul><h4 id="3-1-Retrieve-Supporting-Paragraphs"><a href="#3-1-Retrieve-Supporting-Paragraphs" class="headerlink" title="3.1 Retrieve Supporting Paragraphs"></a>3.1 Retrieve Supporting Paragraphs</h4><ul><li>retrieve supporting information from external KBs using Lucene, an off-the-shelf IR system<ul><li>query: question + answer candidate</li><li>ranking function: BM25</li><li>top C (C=20)</li><li>retrieve corpus: ARC corpus</li><li>IDF of each query term $q_i$:<ul><li>$idf(q_i) = log(\frac{N-docfreq(q_i) + 0.5}{docfreq(q_i) + 0.5})$<ul><li>$N$ is the number of documents;</li><li>$docfreq(q_i)$ is the number of documents that contain $q_i$ ;</li></ul></li></ul></li></ul></li></ul><h4 id="3-2-Alignment-Method"><a href="#3-2-Alignment-Method" class="headerlink" title="3.2 Alignment Method"></a>3.2 Alignment Method</h4><p>Alignment Algorithm:<br><img src="/../images/papers-ir-obqa/image-20200304220331985.png" alt="image-20200304220331985"></p><ol><li>compute <strong>each query token</strong> with <strong>every token</strong> in the given KB paragraph using <strong>cosine similarity</strong> of the two embedding vectors;<ul><li>$cosSim(q_i, p_k) = \frac{\overrightarrow{q_i} \cdot \overrightarrow{p_k}}{ ||\overrightarrow{q_i}|| \cdot ||\overrightarrow{p_k}|| }$ </li></ul></li><li>max-pooling layer over cosine similarity matrix is used to retrieve the most similar token in the supporting passage for each query token;<ul><li>$align(q_i,P_j) = max_{k=1}^{|P_j|} cosSim(q_i,p_k)$</li></ul></li><li>max-pooled vector of similarity scores is multiplied with the vector containing the IDF values of the query tokens;</li><li>the resultant vector is summed to produce the overall alignment score $s$ for the given query $Q_a$ and the supporting paragraph $P_j$;<ul><li>$s(Q_a, P_j) = \sum_{i=1}^{|Q_a|} idf(q_i) \cdot align(q_i, P_j)$</li></ul></li></ol><p>InferSent: $s(Q_a, P_j) = softmax(\overrightarrow{Q_a} \cdot \overrightarrow{P_j})$</p><p>Aggregate the retrieved paragraph:</p><ol><li>Max: $S(cand_a) = max_{j=1}^{C}( s(Q_a, P_j) )$</li><li>Weighted Average: $S(cand_a) = \sum_{j=1}^{C} \frac{1}{j}( s(Q_a, P_j) )$</li></ol><ul><li>On ARC:<ul><li>the max strategy is better for ARC Challenge;</li><li>the weighted average is better for ARC Easy;</li></ul></li></ul><h4 id="3-3-Multiple-Representation-of-Text"><a href="#3-3-Multiple-Representation-of-Text" class="headerlink" title="3.3 Multiple Representation of Text"></a>3.3 Multiple Representation of Text</h4><p>Four different embedding representations that model the text at different levels of abstraction:</p><ul><li>character, word, and sentence</li></ul><p>Character-based Embeddings:</p><ul><li>FLAIR contextual character language model <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649.">[1]</span></a></sup><ul><li>$w_i^{FLAIR} = [h_{t_i+1-1}^f; h_{t_i-1}^b] \in \mathbb{R}^{4096}$</li><li>forward and backward of LSTM</li><li>$t_i$ the character offset of the $i^{th}$ token;</li></ul></li></ul><p>Word-based Embeddings:</p><ul><li>BERT<ul><li>$w_i^{BERT} = [Layer_{-1},…,Layer_{-4}] \in \mathbb{R}^{4096}$</li><li>the last four layers</li></ul></li><li>GloVE<ul><li>dimension = 300</li></ul></li></ul><p>Sentence-based:</p><ul><li>InferSent:<ul><li>trained InferSent on our data by maximizing the inference probability from the input query to the supporting paragraph.</li><li>dimension:<ul><li>WikiQA: 128</li><li>ARC: 384</li></ul></li></ul></li></ul><h4 id="3-4-Ensemble-Strategies-over-Representations"><a href="#3-4-Ensemble-Strategies-over-Representations" class="headerlink" title="3.4 Ensemble Strategies over Representations"></a>3.4 Ensemble Strategies over Representations</h4><p>Aggregate the scores of candidate answers over the four different embedding representations using an unsupervised variant of the NoisyOr formula to compute the overall score for answer candidate $i$:</p><script type="math/tex; mode=display">NoisyOr_M(i) = 1- (\prod_{m=0}^{M}(1-\alpha^m \ast S_i^m))</script><ul><li>$M$, total number of representations</li><li>$S_i^m$, score of answer candidate $i$ under representation $m$</li><li>$\alpha^m$, a hyperparameter used to dampen peaky distributions of answer probabilities;<ul><li>=1</li><li>infersent = 0.2</li></ul></li></ul><p>Supervised Meta Multi-classifier: aims to learn the aggregation function directly from data </p><ul><li>2 fully connected dense layers of hidden size 16 and K, and a softmax layer;</li><li>input vector size = $M \times K$</li><li>K is the maximum number of candidate answers;</li><li>activation function: tanh;</li><li>for ARC, use an extra position in the input vector to indicate the grade of the corresponding exam question (provided in the dataset) with the intuition that the meta-classiﬁer will learn to trust different representations for different grade levels.</li></ul><h3 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4.Experiments"></a>4.Experiments</h3><p>Reference to the Paper</p><h2 id="Careful-Selection"><a href="#Careful-Selection" class="headerlink" title="Careful Selection"></a>Careful Selection</h2><blockquote><p><em>Title</em>: Careful Selection of Knowledge to solve Open Book Question Answering<br><em>Authors</em>: Pratyay Banerjee, Kuntal Kumar Pal, Arindam Mitra, Chitta Baral<br><em>Orig.</em>: Arizona State University<br><em>Published</em>: ACL,2019.</p></blockquote><h3 id="1-Motivation-1"><a href="#1-Motivation-1" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><p>Towards <strong>Open Book Question Answering</strong> task:</p><ul><li>questions are expected to be answered with respect to a given set of open book facts, and common knowledge about a topic;<ul><li>e.g.: OpenBookQA</li><li>the focus is not on memorization but on deeper understanding of the materials and its application to new situations <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP.">[2]</span></a></sup><sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Tony Jenkins. 1995. Open Book Assessment in Computing Degree Programmes. Citeseer.">[3]</span></a></sup><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="J Landsberger. 1996. Study guides and strategies.">[4]</span></a></sup>;</li></ul></li><li>requires deeper reasoning involving linguistic understanding as well as reasoning with common knowledge;</li></ul><p><strong>Challenges in OpenBookQA</strong> (The open book part is much larger (than a small paragraph) and is not complete as additional common knowledge may be required.):</p><ol><li>finding <strong>the relevant facts in an open book</strong> is a challenge;</li><li>finding <strong>the relevant common knowledge using the IR </strong>front end is bigger challenge;<ul><li>can be misled by distractions;</li></ul></li><li><strong>reasoning involving both facts from open book and common knowledge</strong> leads to <strong>multi-hop reasoning with respect to natural language text</strong>, which is also a challenge.</li></ol><h3 id="2-This-Work-1"><a href="#2-This-Work-1" class="headerlink" title="2.This Work"></a>2.This Work</h3><p>Address the first two challenges:</p><ul><li>improve on knowledge extraction from the OpenBook present in the dataset.<ul><li>use semantic textual similarity models that are trained with different datasets for this task;</li></ul></li><li>propose natural language abduction to generate queries for retrieving missing knowledge；</li><li>use Information Gain based Re-ranking to reduce distractions and remove redundant information;</li><li>provide an analysis of the dataset and the limitations of BERT Large model for such a question answering task.</li></ul><p>Key Approach:</p><ol><li>abductive information retrieval (IR);</li><li>information gain based re-ranking;</li><li>passage selection and weighted scoring;</li></ol><p>A key aspect of this work’s approach is to <strong>accurately hunt the needed knowledge facts from the OpenBook knowledge corpus and hunt missing common knowledge using IR</strong>.</p><h3 id="3-Method-1"><a href="#3-Method-1" class="headerlink" title="3.Method"></a>3.Method</h3><p>Approach Overall:</p><p><img src="/../images/papers-ir-obqa/image-20200304233158435.png" alt="image-20200304233158435"></p><p>The proposed approach involves six main modules: </p><ol><li>Hypothesis Generation, </li><li>OpenBook Knowledge Extraction, </li><li>Abductive Information Retrieval, </li><li>Information Gain based Re-ranking, </li><li>Passage Selection,</li><li>Question Answering.</li></ol><p>Example of the proposed approach:</p><p><img src="/../images/papers-ir-obqa/image-20200304233053774.png" alt="image-20200304233053774"></p><p>Hypothesis Generation:</p><ul><li>generate a hypothesis $H_{ij}$ for the $i$-th question and $j$-th answer option ($j\in \{1,2,3,4\}$);</li></ul><p>OpenBook Knowledge Extraction:</p><ul><li>retrieve appropriate knowledge $F_{ij}$  for a given hypothesis using semantic textual similarity, from the OpenBook knowledge corpus $F$;</li></ul><p>Abductive Information Retrieval:</p><ul><li>system abduces missing knowledge from $H_{ij}$ and $F_{ij}$;</li><li>system formulates queries to perform IR to retrieve missing knowledge $K_{ij}$.</li></ul><p>Information Gain based Re-ranking and Passage Selection:</p><ul><li>create a knowledge passage $P_{ij}$</li></ul><p>Question Answering:</p><ul><li>system uses $P_{ij}$ to answer the questions using a BERT Large based MCQ model;</li></ul><h4 id="3-1-Hypothesis-Generation"><a href="#3-1-Hypothesis-Generation" class="headerlink" title="3.1 Hypothesis Generation"></a>3.1 Hypothesis Generation</h4><ol><li>for questions with wh words, use the rule-based model<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming question answering datasets into natural language inference datasets. arXiv preprint arXiv:1809.02922.">[5]</span></a></sup>.</li><li>for the rest of the questions, concatenate the questions with each of the answers to produce the four hypotheses.</li></ol><h4 id="3-2-OpenBook-Knowledge-Extraction"><a href="#3-2-OpenBook-Knowledge-Extraction" class="headerlink" title="3.2 OpenBook Knowledge Extraction"></a>3.2 OpenBook Knowledge Extraction</h4><ul><li>a textual similarity model is trained in a supervised fashion on two datasets<ul><li>use large-cased BERT</li><li>STS-B</li><li>gold OpenBookQA facts<ul><li>first find the similarity of the OpenBook $F$ facts with respect to each other using the BERT model trained on STS-B dataset;</li><li>assign a score of $5.0$ for the gold $\hat{F}_i$ fact for a hypothesis;</li><li>then sample different facts from the OpenBook and assign the STS-B similarity scores between the sampled fact and the gold fact $\hat{F}_i$ as the target score for that fact $F_{ij}$ and $H_{ij}$.</li></ul></li></ul></li></ul><h4 id="3-3-Natural-Language-Abduction-and-IR"><a href="#3-3-Natural-Language-Abduction-and-IR" class="headerlink" title="3.3 Natural Language Abduction and IR"></a>3.3 Natural Language Abduction and IR</h4><p>To search for the missing knowledge, need to know what we are missing.</p><ul><li>use abduction to figure that out;</li><li>Abduction: both the observation (hypothesis) and the domain knowledge (known fact) is represented in a formal language from which a logical solver abduces possible explanations (missing knowledge)</li><li>propose three models</li></ul><p><strong>Word Symmetric Difference</strong> Model</p><ul><li>$K_{ij} = (H_{ij} \cup F_{ij}) \backslash (H_{ij} \cap F_{ij}), \forall j \in \{1,2,3,4\}$</li></ul><p><strong>Supervised</strong> <strong>Bag of Words</strong> Model</p><ul><li>select words which satisfy: $P(w_n \in K_{ij}) &gt; \theta$<ul><li>$w_n \in \{ H_{ij} \cup F_{ij} \}$</li></ul></li><li>To learn this probability, create a task:<ul><li>where the words similar (cosine similarity useing SpaCy）to the words in the gold missing knowledge $\hat{K}_i$ (provided in the dataset) are labelled as positive class;</li><li>all the other words not present in $\hat{K}_i$ but in $H_{ij} \cup F_{ij}$ are labelled as negative class.</li><li>train a binary classifier using BERT Large with one additional feed forward network for classification.</li></ul></li><li>$\theta = 0.4 $</li></ul><p><strong>CopyNet Seq2Seq</strong> Model</p><ul><li>to generate, instead of predict, the missing knowledge given, the hypothesis H and knowledge fact from the corpus F.</li><li>choose from multiple generated knowledge which provided maximum overlap score:<ul><li>$overlap_score = \frac{\sum_i count((\hat{H}_{i} \cup F_{i}) \cap K_i )}{\sum_i count(\hat{K}_i)}$</li></ul></li></ul><p><strong>Word Union</strong> Model:</p><ul><li>To see if abduction helps, compare the above models with a Word Union Model</li><li>use the set of unique words from both the hypothesis and OpenBook knowledge as candidate keywords.<ul><li>$K_{ij} = (H_{ij} \cup F_{ij})$</li></ul></li></ul><h4 id="3-4-Information-Gain-based-Re-ranking"><a href="#3-4-Information-Gain-based-Re-ranking" class="headerlink" title="3.4 Information Gain based Re-ranking"></a>3.4 Information Gain based Re-ranking</h4><p>Observation: BERT QA model gives a higher score if similar sentences are repeated, leading to wrong classification.</p><ul><li>Thus: introduce Information Gain based Re-ranking to remove redundant information.</li><li>use the same BERT Knowledge Extraction model (3.2) Trained on OpenBookQA data;<ul><li>to do an initial ranking of the retrieved missing knowledge $K$;</li><li>the scores of this knowledge extraction model is used as relevancy score, $rel$.</li></ul></li></ul><p>To extract the top-10 missing knowledge $K$, define a redundancy score $red_{ij}$:</p><ul><li>as the maximum cosine similarity $sim$ between <ul><li>the previously selected missing knowledge, in the previous iterations till $i$,</li><li>and the candidate missing knowledge ${K_j}$:</li></ul></li><li>$red_{ij}(K_j) = max(red_{i-1,j}(K_j), sim(K_i,K)j))$</li><li>$randscore = (1- red_{ij}(K_j)) \ast rel(K_j) $</li></ul><p>Missing Knowledge Selection:</p><ul><li>first take the missing knowledge with the highest $rel$ score.</li><li>from the subsequent iteration, we compute the redundancy score with the last selected missing knowledge for each of the candidates and then rank them using the updated $rankscore$.</li><li>select the top-10 missing knowledge for each $H_{ij}$;</li></ul><h4 id="3-5-Question-Answering"><a href="#3-5-Question-Answering" class="headerlink" title="3.5 Question Answering"></a>3.5 Question Answering</h4><h5 id="3-5-1-Question-Answering-Model"><a href="#3-5-1-Question-Answering-Model" class="headerlink" title="3.5.1 Question-Answering Model"></a>3.5.1 Question-Answering Model</h5><p>finetune BERT MCQA</p><p>To be within the restrictions of BERT max input length, create a passage for each of the answer options, and score for all answer options against each passage (Corpus $F$).</p><ul><li>for each answer option $A_j$ (?), create a passage $P_j$ and score against each of the answer options $A)i$</li><li>To compute the ﬁnal score for the answer, we sum up each individual scores.</li><li>$Pr(Q,A_i) = \sum_{j=1}^4 score(P_j, Q, A_i)$</li><li>$A = argmax_A Pr(Q, A_i)$</li></ul><h5 id="3-5-2-Passage-Selection-and-Weighted-Scoring"><a href="#3-5-2-Passage-Selection-and-Weighted-Scoring" class="headerlink" title="3.5.2 Passage Selection and Weighted Scoring:"></a>3.5.2 Passage Selection and Weighted Scoring:</h5><ul><li>In the first round, score each of the answer options using a passage created from the selected knowledge facts from corpus F. <ul><li>For each question, ignore the passages of the answer options which are in the bottom two.</li></ul></li><li>In the second round, score for only those passages which are selected after adding the missing knowledge $K$.</li><li>Weighted Scoring<ul><li>Assume that the correct answer has the highest score in each round;</li><li>multiply the scores obtained after both rounds;</li></ul></li><li>Define the combined passage selected scores and weighted scores as follows:<ul><li>$Pr(F, Q, A_i) = \sum_{j=1}^4 score(P_j, Q, A_i)$<ul><li>$P_j$ from extracted OpenBook knowledge $F$</li><li>the top-2 passages are selected based on the scores of $Pr(F, Q, A_i)$</li></ul></li><li>$Pr(F\cup K, Q, A_i) = \sum_{k=1}^4 \delta \ast score(P_k, Q, A_i)$<ul><li>$\delta=1$ for the top two scores;</li><li>$\delta=0$ for the rest;</li><li>$P_k$ is the passage created using both the facts and missing knowledge</li></ul></li></ul></li><li>Final weighted score:<ul><li>$wPR(Q,A_i) = Pr(F, Q, A_i) \ast Pr(F\cup K, Q, A_i)$</li></ul></li><li>$A = argmax_{A} wPr(Q,A_i)$</li></ul><h3 id="4-Experiments-1"><a href="#4-Experiments-1" class="headerlink" title="4.Experiments"></a>4.Experiments</h3><p>Reference to the Paper</p><h2 id="AutoROCC"><a href="#AutoROCC" class="headerlink" title="AutoROCC"></a>AutoROCC</h2><blockquote><p><em>Title</em>: <a href="https://www.aclweb.org/anthology/D19-1260" target="_blank" rel="noopener">Quick and (not so) Dirty: Unsupervised Selection of Justiﬁcation Sentences for Multi-hop Question Answering</a><br><em>Authors</em>: Vikas Yadav, Steven Bethard, Mihai Surdeanu<br><em>Orig.</em>: University of Arizona<br><em>Published</em>: EMNLP,2019.<br><em>Code</em>: <a href="https://github.com/vikas95/AutoROCC" target="_blank" rel="noopener">https://github.com/vikas95/AutoROCC</a></p></blockquote><h3 id="1-Motivation-2"><a href="#1-Motivation-2" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>For complex NLP task, such as QA, human readable explanations of the inference process have been proposed as a way to interpret QA models;</li><li>The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement;<ul><li>more effort must be dedicated to explaining their inference process.</li></ul></li></ul><h4 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h4><p>The selection of justification sentences can be classified into roughly four categories:</p><ol><li>supervised approaches that require training data to learn how to select justification sentences;<ol><li>use entailment resources to train components for selecting sentences for QA <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Harsh Trivedi, Heeyoung Kwon, Tushar Khot, Ashish Sabharwal, and Niranjan Balasubramanian. 2019. Repurposing entailment for multi-hop question answering tasks. arXiv preprint arXiv:1904.09380.">[6]</span></a></sup>;</li><li>explicitly focused on training sentence selection components for QA models <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sewon Min, Victor Zhong, Richard Socher, and Caiming Xiong. 2018. Efﬁcient and robust question answering from minimal context over documents. arXiv preprint arXiv:1805.08092.">[7]</span></a></sup><sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. 2018. Denoising distantly supervised open-domain question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17361745.">[8]</span></a></sup><sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hai Wang, Dian Yu, Kai Sun, Jianshu Chen, Dong Yu, Dan Roth, and David McAllester. 2019. Evidence sentence extraction for machine reading comprehension. arXiv preprint arXiv:1902.08852.">[9]</span></a></sup>;</li><li>when the gold justification sentences are not provided:<ul><li>retrieve justifications from structured KBs;</li><li>IR systems coupled with denoising components <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hai Wang, Dian Yu, Kai Sun, Jianshu Chen, Dong Yu, Dan Roth, and David McAllester. 2019. Evidence sentence extraction for machine reading comprehension. arXiv preprint arXiv:1902.08852.">[9]</span></a></sup>;</li></ul></li></ol></li><li>treat justification as latent variables and learn jointly how to answer questions and how to select justification from questions and answers alone;<ul><li>rely on reinforcement learning or PageRank to learn how to select justification sentences without explicit training data;</li><li>use end-to-end (mostly RNNs with attention mechanisms) QA architectures for learning to pay more attention on better justification sentences;</li></ul></li><li>rely on information retrieval to select justification sentences;</li><li>do not use justification sentences at all;</li></ol><p>The detailed works please refer to the original paper.</p><h3 id="2-This-Work-2"><a href="#2-This-Work-2" class="headerlink" title="2.This Work"></a>2.This Work</h3><p>Propose an <strong>unsupervised, non-parametric strategy</strong> (<strong>ROCC</strong>) for the <strong>selection of justification sentences</strong> from <strong>unstructured knowledge bases</strong> for <strong>Multi-Hop QA</strong>:</p><ol><li>maximizes the <strong>Relevance</strong> of the selected sentences;</li><li>minimizes the <strong>Overlap</strong> between the selected facts;</li><li>maximizes the <strong>Coverage</strong> of both question and answer;</li></ol><ul><li>can be coupled with any supervised QA approach;</li><li>account for more intentional knowledge aggregation;</li></ul><p>Target Dataset: (a) ARC; (b) MultiRC;</p><p>Example:<br><img src="/../images/papers-ir-obqa/image-20200305122332298.png" alt="image-20200305122332298"></p><p>Brief introduction of the ROCC:</p><ul><li>first creating $\left( \begin{array}{c} n \\ k \end{array} \right)$ justification sets from the top-$n$ sentences selected by BM25 IR model;<ul><li>$k$ ranges from $2$ to $n$;</li><li>$n = 20$;</li></ul></li><li>then ranking them all by a formula that combines the three <strong>criteria</strong> above;</li><li>the set with the top score becomes the set of justifications output by ROCC for a given question and candidate answer;</li></ul><h3 id="3-Method-2"><a href="#3-Method-2" class="headerlink" title="3.Method"></a>3.Method</h3><p>ROCC, coupled with a QA system, operates in the following steps:<br><img src="/../images/papers-ir-obqa/image-20200305180954446.png" alt="image-20200305180954446"></p><h4 id="3-1-Retrieval-of-candidate-justification-sentences"><a href="#3-1-Retrieval-of-candidate-justification-sentences" class="headerlink" title="3.1 Retrieval of candidate justification sentences"></a>3.1 Retrieval of candidate justification sentences</h4><p>The overall score for a given justification set $P_i$ is calculated as:</p><ul><li>$S(P_i) = \frac{R}{\epsilon + O(P_i) } \cdot (\epsilon + C(A)) \cdot (\epsilon + C(Q))$<ul><li>$\epsilon = 1$ to avoid zeros;</li></ul></li></ul><p><strong>Relevance</strong> (R)</p><ul><li>use BM25 IR model to estimate the relevance of each justification sentence to a given question and candidate answer;</li><li>IR query: concatenate the question and candidate answer;</li><li>the arithmetic mean of BM25 scores over all sentences in a given justification set gives the value of R for the entire set;</li></ul><p><strong>Overlap</strong> (O)</p><ul><li>to ensure <strong>diversity</strong> and <strong>complementarity</strong> between justification sentences;</li><li>overlap between <strong>all sentence pairs</strong> in a given group;</li><li><strong>minimizing</strong> this score <strong>reduces redundancy</strong> and <strong>encourages the aggregated sentences</strong> to <strong>address different parts</strong> of the question and answer:</li><li>$O(S) = \frac{ \sum_{s_i \in S} \sum_{s_j \in S - s_i} \frac{ |t(s_i) \cap t(s_j)| }{ max(|t(s_i)|,|t(s_j)|) } }{ \left( \begin{array}{c} |S| \\ 2 \end{array} \right) }$<ul><li>$S$ is the given set of justification sentences;</li><li>$s_i$ is the $i^{th}$ sentence in $S$;</li><li>$t(s_i)$ is the set of unique terms in sentence $s_i$;</li><li>divide by $\left( \begin{array}{c} n \\ k \end{array} \right)$ to normalize across different sizes of justification sets;</li></ul></li></ul><p><strong>Coverage</strong> (C)</p><ul><li>complementing the overlap score, Coverage measures t<strong>he lexical coverage of the question and the answer texts</strong> by the given set of justification $S$;</li><li>$C$ is <strong>weighted by the IDF</strong> of questions and answer terms;</li><li><strong>maximizing</strong> this value <strong>encourages the justifications to address more of the meaningful content</strong> mentioned in the question ($X=Q$) and the answer ($X=A$);</li><li>$C(X) = \frac{ \sum_{t=1}^{|C_t(X)|} IDF[C_t(X)[t]] }{ |t(X)| }$<ul><li>$C_t(X) = \bigcup_{s_i \in S} t(X) \cap t(s_i)$</li><li>$t(X)$ is the unique terms in $X$;</li><li>$C_t(X)$ is the set of all unique terms in $X$ that are present in any of the sentences of the given justification set.</li></ul></li></ul><h4 id="3-2-Generation-of-candidate-justification-sets"><a href="#3-2-Generation-of-candidate-justification-sets" class="headerlink" title="3.2 Generation of candidate justification sets"></a>3.2 Generation of candidate justification sets</h4><p>ROCC ranks <strong>sets</strong> of justification sentences rather than individual sentences.</p><ul><li>create candidate justification sets by generating $\left( \begin{array}{c} n \\ k \end{array} \right)$ groups of sentences from the previous top-$n$ sentences, using multiple values of $k$;</li></ul><h4 id="3-3-Ranking-of-candidate-justification-sets"><a href="#3-3-Ranking-of-candidate-justification-sets" class="headerlink" title="3.3 Ranking of candidate justification sets"></a>3.3 Ranking of candidate justification sets</h4><p>ROCC score estimates <strong>the likelihood that this group of justification explains</strong> the given answer.</p><ul><li>rank the justification sets in descending order of ROCC score;</li><li>choose the top set as the group of justification that is the output of ROCC for the given question and answer.</li><li>In MultiRC, rearrange the justification sentences according to their original indexes in the given passage to bring coherence in the selected sequence of sentences.</li></ul><h4 id="3-4-Answer-Classification"><a href="#3-4-Answer-Classification" class="headerlink" title="3.4 Answer Classification"></a>3.4 Answer Classification</h4><p>ROCC can be coupled with any supervised QA component for answer classification (BERT in this work).</p><ul><li>Justification sentences in the reading comprehension (MultiRC) come from the same passage and their sequence is likely to be coherent;<ul><li><strong>concatenate</strong> them into a single passage;</li><li>use a <strong>single</strong> BERT instance for classification.</li></ul></li><li>Justification sentences retrieved from an external KB (ARC) may not form a coherent passage when aggregated;<ul><li>classify each justification sentence <strong>separately</strong> (together with the question and candidate answer);</li><li>then <strong>average</strong> all these scores to produce a single score for the candidate answer;</li></ul></li><li>The first text consists of the concatenated question and answer, and the second text consists of the justification text.</li></ul><h3 id="4-Experiments-2"><a href="#4-Experiments-2" class="headerlink" title="4.Experiments"></a>4.Experiments</h3><p>AutoROCC: $k$ is selected automatically.</p><p>Alignment ROCC</p><ul><li>to understand the dependence between ROCC and exact lexical match:<ul><li>compare the justification selection performance of ROCC when its score components are computed based on <strong>lexical match</strong> vs. <strong>semantic alignment match</strong></li></ul></li></ul><p>Reference to the Paper</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Tony Jenkins. 1995. Open Book Assessment in Computing Degree Programmes. Citeseer.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">J Landsberger. 1996. Study guides and strategies.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming question answering datasets into natural language inference datasets. arXiv preprint arXiv:1809.02922.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Harsh Trivedi, Heeyoung Kwon, Tushar Khot, Ashish Sabharwal, and Niranjan Balasubramanian. 2019. Repurposing entailment for multi-hop question answering tasks. arXiv preprint arXiv:1904.09380.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sewon Min, Victor Zhong, Richard Socher, and Caiming Xiong. 2018. Efﬁcient and robust question answering from minimal context over documents. arXiv preprint arXiv:1805.08092.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. 2018. Denoising distantly supervised open-domain question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17361745.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Hai Wang, Dian Yu, Kai Sun, Jianshu Chen, Dong Yu, Dan Roth, and David McAllester. 2019. Evidence sentence extraction for machine reading comprehension. arXiv preprint arXiv:1902.08852.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">* lexical match: the approach used throughout the paper up to this point;* semantic alignment match: relaxes the requirement for lexical match;* i.e., two tokens are considered to be matched when the cosine similarity of their embedding vectors is larger than a margin;* Table 7 shows the alignment-based ROCC indeed performs better than the ROCC that relies on lexical match;* but the improvements are not large;<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vikas Yadav, Rebecca Sharp, and Mihai Surdeanu. 2018. Sanity check: A strong alignment and information retrieval baseline for question answering. In The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, pages 1217–1220. ACM.<a href="#fnref:10" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> note </tag>
            
            <tag> ir </tag>
            
            <tag> open-book </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2020 | 以transformer作为语言软推理机</title>
      <link href="/2020/03/04/paper-2020-rover/"/>
      <url>/2020/03/04/paper-2020-rover/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: <a href="https://arxiv.org/abs/2002.05867v1" target="_blank" rel="noopener">Transformers as Soft Reasoners over Language</a><br><em>Authors</em>: Peter Clark, Oyvind Tafjord, Kyle Richardson<br><em>Org.</em>: AI2;<br><em>Published</em>: unpublished<br><em>Demo</em>: <a href="https://rule-reasoning.apps.allenai.org/" target="_blank" rel="noopener">https://rule-reasoning.apps.allenai.org/</a></p></blockquote><h2 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h2><p>使系统具备基于显式提供的知识的推理能力是AI研究的长期目标，但是构建<strong>合适的表示</strong>被证明是极具挑战的。</p><h2 id="2-This-Work"><a href="#2-This-Work" class="headerlink" title="2. This Work"></a>2. This Work</h2><p>本文尝试探索，是否transformer可以被训练用来进行直接推理（或模仿推理），但是使用语言表达的规则（rules expressed in language），从而绕过形式表示（formal representation）。</p><p>本文还刻画其推理能力的程度。</p><p>本文使用一个合成的数据，来测试不断增加的推理复杂性（规则数量、否定、链接深度）</p><p>本文的发现，为transformer提供了一个新的角色：transformer可以作为一个有限的<strong>软定理证明器</strong>（soft theorem prover），在语言的explicit theories上运行；</p><p>也为在QA上的explainability，correctability，counterfactual reasoning（可解释性、可修正性、反事实推理）提供了新的可能；</p><p>本文围绕下面几个问题展开：</p><ul><li>Can transformers learn to reason with explicit rules?<ul><li>(Table.1)</li><li>在合成数据集中，在测试集上的效果高达99%；</li><li>在需要更深推理的测试集上效果可以达到95%；</li></ul></li><li>Can the trained model solve hand-authored reasoning problems?<ul><li>(Table.4)</li><li>zero shot，90%；</li></ul></li><li>Do the results transfer to theories expressed in more natural language?<ul><li>(Table.5)</li><li>自然语言更多时，zero shot，66%；</li></ul></li><li>Can the model identify which facts an answer depends on?<ul><li>(Fig.9)</li><li>模型可以对其结论产生解释；</li></ul></li><li>Can other neural architectures learn to reason?<ul><li>(Table.6)</li><li>对比了BERT（95%）、ESIM（80%），不限结构；</li></ul></li></ul><h2 id="3-Dataset-Generation"><a href="#3-Dataset-Generation" class="headerlink" title="3. Dataset Generation"></a>3. Dataset Generation</h2><p>合成了5个数据集，每个example由三个元素构成（context，statement，answer）：</p><ul><li>context：由fact和rule构成，</li><li>statement：相当于问题，</li><li>answer：为True/False，<ul><li>True表示statement可以由context中演绎（deductive）出来；</li><li>False表示statement不符合 closed-world assumption（CWA，封闭世界假定，当前不是已知的事物都为假）；</li></ul></li></ul><p>再生成样例之前，先生成logic形式的small theory（facts+rules）</p><p>5个数据集以推理深度进行区分：</p><ul><li>从D=0到D&lt;=5；</li><li>D=0表示，仅通过context就可以推理出答案；</li></ul><p>Theory Generation时考虑两种fact：</p><ul><li>atributes：is($e_i$,$a_j$)<ul><li>例如：<code>is(Alan, Big)</code></li></ul></li><li>relations：$r_k(e_i,e_k)$<ul><li>例如：<code>eats(Dog, Rabbit)</code></li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>模型采用RoBERTa，在RACE上进行微调；</p><p>RoBERTa的输入为：<code>[CLS]context[SEP]statement[SEP]</code></p><ul><li><code>[CLS]</code>位的输出映射到一个logit，当其大于0时，输出True，反之，输出False；</li><li>用交叉熵作为目标函数；</li></ul><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><p>TBU</p>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> note </tag>
            
            <tag> pre-trained-lm </tag>
            
            <tag> reasoning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TG | Seq2Seq with LM using Cold Fusion</title>
      <link href="/2020/02/11/paper-2018-cold-fusion/"/>
      <url>/2020/02/11/paper-2018-cold-fusion/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: <a href="http://www.isca-speech.org/archive/Interspeech_2018/abstracts/1392.html" target="_blank" rel="noopener">Cold Fusion: Traning Seq2Seq Models Together with Language Models</a><br><em>Authors</em>: Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, Adam Coates<br><em>Org.</em>: Baidu Research<br><em>Published</em>: Interspeech, 2018.</p></blockquote><h2 id="Research-Topic-Problem"><a href="#Research-Topic-Problem" class="headerlink" title="Research Topic/Problem"></a>Research Topic/Problem</h2><p>如何在seq2seq模型训练时利用PLM提升模型的效果（相当于引入无标注数据）</p><p>引入PLM的优势/目的：</p><ul><li>提升生成文本的fluency；</li><li>可以充分利用大规模无标注数据；</li></ul><p>最基本的向Seq2Seq模型引入PLM的方法<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Towards better decoding and language model integration in sequence to sequence models. 2016.">[1]</span></a></sup><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sequence to sequence learning with neural networks. NIPS,2014.">[2]</span></a></sup><sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="On using monolingual corpora in neural machine translation. 2015.">[3]</span></a></sup>：</p><ul><li>分别训练seq2seq和PLM模型，然后在测试推理阶段，通过结合两个模型的输出指导beam search；</li></ul><p>存在的限制：</p><ul><li>seq2seq模型完全基于有标注的序列进行训练，decoder可以看做隐式的LM，通过序列的标注信息进行习，decoder的很大一部分容量都被用于学习冗余的信息；</li><li>Seq2Seq的decoder中的剩余语言模型偏向于并行语料库的训练标签；</li></ul><p>导致：</p><ul><li>a) 很难适应于新的领域；</li><li>b) 为了适应新的领域，Seq2Seq模型必须首先学会忽略语言模型中的隐含知识；</li></ul><h2 id="Background-amp-Related-Work"><a href="#Background-amp-Related-Work" class="headerlink" title="Background &amp; Related Work"></a>Background &amp; Related Work</h2><p>输入source序列：$\mathbb{x}=\{x_1,…,x_T\}$</p><p>中间状态：$\mathbb{h}$</p><p>输出target序列：$\mathbb{y}=\{y_1,…,y_K\}$</p><p>预测推理：$\hat{y}=argmax_y logp(y|x)$</p><p>将seq2seq的decoder与LM结合的方法：</p><p>1、<strong>Shallow Fusion</strong><sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Towards better decoding and language model integration in sequence to sequence models. 2016.">[1]</span></a></sup><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Google’s neural machine translation system: Bridging the gap between human and machine translation. 2016.">[4]</span></a></sup></p><p>只在推理阶段利用LM输出的概率，计算如下：</p><script type="math/tex; mode=display">\hat{y}=argmax_y logp(y|x) + \lambda logp_{LM}(y)</script><p>其中，$p_{LM}(y)$ 即为LM关于序列$y$的概率；</p><p>2、<strong>Deep Fusion</strong><sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="On using monolingual corpora in neural machine translation. 2015.">[3]</span></a></sup></p><p>通过引入带有参数的门控模块，结合decoder和LM的隐状态，加强decoder与LM的连接；</p><p>在训练时，先是分开训练，然后在结合起来学习门控模块的参数；</p><p>Deep Fusion的劣势：</p><ul><li>task-specific模型的训练和LM的训练时分开的；</li><li>decoder需要从有标注训练数据中学习一个LM，由于有标注训练数据的限制，其训练与训练LM存在很大的不平衡，且相当大一部分decoder的能力被浪费；</li><li>因此，Fusion机制需要克服这种偏置，以结合新的语言信息；</li></ul><h2 id="Cold-Fusion"><a href="#Cold-Fusion" class="headerlink" title="Cold Fusion"></a>Cold Fusion</h2><p>Cold Fusion启发自Deep Fusion，鼓励seq2seq中的decoder在训练中学习利用LM，使seq2seq模型可以利用无限的无监督文本数据，使其能够快速适用于new domain，此外，decoder只需要学习任务相关的信息，因此训练更快；</p><p>与Deep Fusion的区别：seq2seq模型从头开始和一个fixed PLM共同训练；</p><p>在此情况下，seq2seq模型在训练中可以一直感知LM，使用LM中的language-specific信息，并且只捕获与任务相关（有助于将source映射到target）的信息，这样的解耦方式，可以增加模型的effective capacity，即使小型decoder也可以需要较好的性能；</p><h3 id="Fusion-Mechanism"><a href="#Fusion-Mechanism" class="headerlink" title="Fusion Mechanism"></a>Fusion Mechanism</h3><p>1、门控计算的输入：a) seq2seq的隐状态 $s_t$，和b) LM的隐状态 $s_t^{LM}$；</p><ul><li>融合层根据input的不确定性，决定多大程度上依赖LM；</li></ul><p>2、采用 fine-grained (FG) 门控机制<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Words or characters? ﬁne-grained gating for reading comprehension. arXiv preprint arXiv:1611.01724, 2016.">[5]</span></a></sup></p><ul><li>对语言模型状态的每个隐藏节点使用不同的门值；</li><li>使得语言模型的集成更加灵活性，因为融合算法可以在每个时间步长中选择更需要强调的语言模型的哪些方面；</li></ul><p>3、用LM的概率替换LM的隐状态；【？？】</p><ul><li>$s_t^{LM}$ 的分布和动态性，在不同的LM和数据上差别很大；</li><li>将token的分布映射到一个common 嵌入空间；</li></ul><p>Cold Fuison Layer的计算过程：</p><script type="math/tex; mode=display">h_t^{LM}=DNN(l_t^{LM})</script><script type="math/tex; mode=display">g_t = \sigma(W[s_t;h_t^{LM}]+b)</script><script type="math/tex; mode=display">s_t^{CF}=[s_t;g_t\circ h_t^{LM}]</script><script type="math/tex; mode=display">r_t^{CF}=DNN(s_t^{CF})</script><script type="math/tex; mode=display">\hat{P}(y_t|x,y_{<t})=softmax(r_t^{CF})</script><p>其中，$l_t^{LM}$ 为 LM 的logit输出；</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Towards better decoding and language model integration in sequence to sequence models. 2016.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sequence to sequence learning with neural networks. NIPS,2014.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">On using monolingual corpora in neural machine translation. 2015.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Google’s neural machine translation system: Bridging the gap between human and machine translation. 2016.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Words or characters? ﬁne-grained gating for reading comprehension. arXiv preprint arXiv:1611.01724, 2016.<a href="#fnref:5" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> tg </tag>
            
            <tag> note </tag>
            
            <tag> pre-trained-lm </tag>
            
            <tag> seq2seq </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 | K-Adapter - Infusing Knowledge into Pre-Trained Models with Adapters</title>
      <link href="/2020/02/10/paper-2019-k-adapter/"/>
      <url>/2020/02/10/paper-2019-k-adapter/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: <a href="https://arxiv.org/abs/2002.01808" target="_blank" rel="noopener">K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters</a><br><em>Authors</em>: Ruize Wang, et. al.<br><em>Org.</em>: MSRA, Fudan University.<br><em>Published</em>: unpublished</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>本文研究问题：向大型预训练（语言）模型中注入知识。</p><p>尽管预训练模型（GPT、BERT、XLNET等模型）取得了很大的进展，最近的一些研究<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA. arXiv preprint arXiv:1911.03681, 2019.">[1]</span></a></sup><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Negated LAMA: Birds cannot ﬂy. arXiv preprint arXiv:1911.03343, 2019.">[2]</span></a></sup><sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="oLMpics On what Language Model Pre-training Captures. arXiv preprint arXiv:1912.13283, 2019.">[3]</span></a></sup>表明，以无监督方式训练的语言模型很难捕获丰富的知识；</p><p>先前的工作<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="ERNIE: Enhanced Language Representation with Informative Entities. In ACL, pp. 1441–1451, 2019.">[4]</span></a></sup><sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Informing unsupervised pretraining with external linguistic knowledge. arXiv preprint arXiv:1909.02339, 2019.">[5]</span></a></sup><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sensebert: Driving some sense into bert. arXiv preprint arXiv:1908.05646, 2019.">[6]</span></a></sup><sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Knowledge enhanced contextual word representations. In EMNLP, pp. 43–54, 2019.">[7]</span></a></sup><sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Integrating Graph Contextualized Knowledge into Pre-trained Language Models. In AAAI, 2020.">[8]</span></a></sup><sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model. In ICLR, 2020.">[9]</span></a></sup>主要集中与通过设计knowledge-driven的训练目标来增强standard LM的训练目标，然后通过多任务学习的方式更新模型的全部参数；</p><p>这样的方式存在几点限制：</p><ul><li>无法进行终身学习（continual learning）<ul><li>模型的参数在引入新知识的时候需要<strong>重新训练</strong>；</li><li>对于已经学到的知识来说，会造成<strong>灾难性遗忘</strong>（catastrophic forgetting）；</li></ul></li><li>模型产生的是耦合的表示（entangled representations）<ul><li>为进一步探究引入不同知识的作用/影响带来困难；</li></ul></li></ul><p>Note：这篇文章中Related Work部分对于<strong>向PLM中注入知识</strong>这一方向进行了很好的梳理，推荐阅读原文Sec. 2，相关工作的区别主要在于 <strong>a) knowledge sources</strong> 和 <strong>b) training objective</strong>；</p><p>Table-1：K-Adapter 和 先前工作的对比；</p><ul><li><img src="/../images/paper-2019-k-adapter/t-1.png" alt="t-1"></li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><p>基于上述问题，本文提出了 <strong>K-Adapter</strong>，一种灵活、简便的向PLM中注入知识的方法，可以进行持续知识融合以及产生解耦的表示，保留了PLM产生的原始表示，可以引入多种知识；</p><p>Adapter：可以看做是一个 knowledge-specific 模型，可以作为一个<strong>插件</strong>，加在PLM外部，输入包含PLM中间层输出的隐状态，一种知识类型对应于一个Adapter，一个PLM可以连接多个Adapter；</p><p>本文引入的知识类型（具体引入知识的任务形式，在模型部分进行介绍）：</p><ul><li>factual knowledge，将Wikipedia文本对齐到Wikidata三元组；</li><li>linguistic knowledge，对web文本进行依存分析得到；</li></ul><p>本文贡献：提出了一个K-Adapter模型；</p><ul><li>通过知识适配器可以同时引入factual knowledge和linguistic knowledge；<ul><li>相应地，最终的模型包含一个PLM和两个Adapter；</li></ul></li><li>在下游任务（包含），以及在LAMA上的实验表明，与RoBERTa相比，K-Adapter 可以捕获更丰富的factual和commonsense知识；</li></ul><p>本文工作与先前工作的3个不同之处：</p><ol><li>同时考虑了fact-related和linguistic-related的目标函数（为了同时引入两种类型的知识）；</li><li>在注入知识的过程中，原始的PLM参数没有变化；</li><li>可以支持持续学习，不同的知识适配器的学习是解耦的（独立的），后续再加入知识不会对已加入的知识产生影响；</li></ol><h2 id="K-ADAPTER"><a href="#K-ADAPTER" class="headerlink" title="K-ADAPTER"></a>K-ADAPTER</h2><p>本文中的PLM使用RoBERTa；</p><p>图：(a) 基于多任务学习向PLM引入知识的框架，(b) 本文通过知识适配器引入知识的框架；</p><p><img src="/../images/paper-2019-k-adapter/framework.png" alt="framework"></p><h3 id="3-1-Adapter-结构"><a href="#3-1-Adapter-结构" class="headerlink" title="3.1 Adapter 结构"></a>3.1 Adapter 结构</h3><p>Adapter的具体结构如Fig.2所示：</p><ul><li><img src="/../images/paper-2019-k-adapter/arch-adapter.png" alt="arch"></li></ul><p>每个Adapter模型包含 K个adapter层，每个adapter层包含：</p><ul><li>N个transformer层；</li><li>2个映射层；</li><li>1个残差连接；</li></ul><p>与PLM的<strong>连接位置</strong>：将adapter层连接到PLM中不同的transformer层上</p><p>与PLM的<strong>连接方式</strong>：</p><ul><li>当前adapter层的输入：a) transformer层输出的隐藏层，b) 前一个adapter层的输出，这两个表示进行concat；</li><li>Adapter模型的输出：a) PLM最后一层的隐藏层输出，和 b) 最后一个adapter层的输出，进行concat作为最终的输出；</li></ul><p>预训练-微调阶段：</p><ul><li>不同的Adapter在不同的预训练任务上分别进行训练；</li><li>对于不同的下游任务，K-Adapter采用和RoBERTa相同的微调方式；<ul><li>只使用一种Adapter时，Adapter模型的最终输出作为task-specific层的输入；</li><li>使用多种Adapter时，将多个Adapter模型的输出进行concat作为task-specific层的输入；</li></ul></li></ul><p>预训练设置：</p><ul><li>使用RoBERTa-Large模型，335M参数；</li><li>adapter层中的transformer层数 $N=2$，维度 $H_A=768$，self-attention head $A_A = 12$；</li><li>adapter层中的两个映射层维度分别是 1024 和 768；</li><li>adapter层连接在RoBERTa模型的 $\{0,11,23\}$层上；</li><li>不同的adapter层之间不共享参数；</li><li>Adapter模型参数量：42M；</li></ul><h3 id="3-2-Factual-Adapter"><a href="#3-2-Factual-Adapter" class="headerlink" title="3.2 Factual Adapter"></a>3.2 Factual Adapter</h3><p>Factual Knowledge 主要来源于文本中实体之间的关系；<br>数据集：T-REx，将Wikipedia摘要和wikidata三元组进行了对齐；<br>数据集规模：5.5M句子，430种关系；<br>FacAdapter的预训练任务：关系分类，给定context和一对实体，对其间关系标签进行分类；<br>预训练任务相关的模块：引入了pooling层；<br>FacAdapter的参数随机初始化；</p><h3 id="3-3-Linguistic-Adapter"><a href="#3-3-Linguistic-Adapter" class="headerlink" title="3.3 Linguistic Adapter"></a>3.3 Linguistic Adapter</h3><p>Linguistic Knowledge 主要来源于文本中词之间的依存关系；<br>数据集：BookCorpus，利用Standford Parser进行处理，<br>数据集规模：1M；<br>LinAdapter的预训练任务：依存关系分类，预测给定句子中每个token在依存分析结果中的father index<br>预训练任务相关的模块：引入了pooling层；<br>LinAdapter的参数随机初始化；</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在三类下游任务上验证K-Adapter的性能：a) relation classification，b) entity typing，c) question answer；</p><p>主要关注在QA任务上的结果；</p><ul><li><img src="/../images/paper-2019-k-adapter/exp-qa.png" alt="exp-qa"></li></ul><p>针对 LAMA query 的生成（预测被Mask位置的词）结果：</p><ul><li><img src="/../images/paper-2019-k-adapter/exp-lama.png" alt="exp-lama"></li></ul><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><ul><li>目前这版的K-Adapter模型，在融合多种知识的adapter产生表示上的方法比较heuristic；</li><li>RoBERTa自身已经具备很强的general 语言知识/语言学知识，LinAdapter对于下游任务的提升并不明显，额外引入的知识和PLM自身的知识有复合的部分。</li><li>两个训练K-Adapter的任务，是否适用于学到事实型知识和语言学知识？</li><li>如何针对不同的知识设计不同的学习/预训练任务？</li></ul><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA. arXiv preprint arXiv:1911.03681, 2019.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Negated LAMA: Birds cannot ﬂy. arXiv preprint arXiv:1911.03343, 2019.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">oLMpics On what Language Model Pre-training Captures. arXiv preprint arXiv:1912.13283, 2019.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">ERNIE: Enhanced Language Representation with Informative Entities. In ACL, pp. 1441–1451, 2019.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Informing unsupervised pretraining with external linguistic knowledge. arXiv preprint arXiv:1909.02339, 2019.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sensebert: Driving some sense into bert. arXiv preprint arXiv:1908.05646, 2019.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Knowledge enhanced contextual word representations. In EMNLP, pp. 43–54, 2019.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Integrating Graph Contextualized Knowledge into Pre-trained Language Models. In AAAI, 2020.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model. In ICLR, 2020.<a href="#fnref:9" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> note </tag>
            
            <tag> pre-trained-lm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Probing Language Models with CommonSense</title>
      <link href="/2019/12/23/papers-prob-lm-csk/"/>
      <url>/2019/12/23/papers-prob-lm-csk/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><!-- # Probing LM with CommonSense --><p>References:</p><ol><li>Do Language Models have Common Sense? [ <a href="https://openreview.net/forum?id=rkgfWh0qKX" target="_blank" rel="noopener">paper</a> ]</li><li>Commonsense Knowledge Mining from Pretrained Models. [ <a href="https://www.aclweb.org/anthology/D19-1109" target="_blank" rel="noopener">paper</a> ]</li><li>Evaluating Commonsense in Pre-trained Language Models. [ <a href="http://arxiv.org/abs/1911.11931" target="_blank" rel="noopener">paper</a> ]</li><li>Why Do Masked Neural LMs Still Need Common Sense Knowledge. [ <a href="http://arxiv.org/abs/1911.03024" target="_blank" rel="noopener">paper</a> ]</li></ol><h2 id="Do-Language-Models-have-Common-Sense-2019"><a href="#Do-Language-Models-have-Common-Sense-2019" class="headerlink" title="Do Language Models have Common Sense? 2019."></a>Do Language Models have Common Sense? 2019.</h2><blockquote><p>Authors: Trieu H. Trinh, Quoc V. Le<br>Org.: Google</p></blockquote><p>本文工作：</p><ul><li>展示能够证明LM可以学习捕获一定常识知识的证据，并且这种知识可以很容易的捕获到；</li><li>本文的主要观察：LM可以计算任意statement的概率，这个概率可以用于评价一个statement的<strong>真实性</strong>；</li></ul><p>本文方法: LM Scoring；</p><ul><li>与其他增加特定分类层的做法不同，本文对于MLM模型不进行任何模块的增加，而是增加了一项额外的training objective，联合原始的LM训练object，在数据集上对LM进行微调；</li><li>$Loss_{new} = Loss_{LM} + max(0, log(Perp_{positive}) - log(Perp_{negative}) + \alpha)$<ul><li>$\alpha = 0.5$</li></ul></li></ul><p>Excerpt:</p><ul><li>Namely, language models are trained on text corpora, which encodes human knowledge in the form of natural language.</li></ul><h2 id="Commonsense-Knowledge-Mining-from-Pretrained-Models-EMNLP-2019"><a href="#Commonsense-Knowledge-Mining-from-Pretrained-Models-EMNLP-2019" class="headerlink" title="Commonsense Knowledge Mining from Pretrained Models. EMNLP,2019."></a>Commonsense Knowledge Mining from Pretrained Models. EMNLP,2019.</h2><blockquote><p>Authors: Joshua Feldman, Joe Davison, Alexander M. Rush<br>Org.: Harvard University</p></blockquote><p>动机：</p><ul><li>inferring commonsense knowledge 是NLP的关键挑战，但是训练数据很稀疏，之前的工作表明，有监督学习下的commonsense mining在新的数据上的效果不好。</li><li>之前的利用常识知识增强机器学习模型方法：<ul><li>1、直接用常识知识库对模型进行增强，虽然包含了高质量的信息，但是缺少覆盖度；</li><li>2、常识知识库补全，由提高资源的覆盖度为驱动；</li></ul></li></ul><p>本文方法</p><ul><li><p>本文设计了一种利用大型预训练的双向语言模型生成常识知识的方法；</p><ul><li>通过转换 relational triples 为 masked sentence，可以使用PBLM通过估计两个实体的点互信息，对三元组的有效性进行排序；</li><li>由于<strong>没有对PBLM的权重进行更新</strong>，本文的方法不会对知识库覆盖的常识知识产生偏置；</li></ul></li><li><p>本文尝试利用语言模型的世界知识来直接识别常识事实；</p><ul><li>利用MLM来确定再一个可能的关系下，两个实体之间的点互信息；</li><li>构建以句子的形式知识候选候选片段，可以用语言模型来近似一个文本的似然；</li></ul></li><li><p>给定一个常识 <code>head-relation-tail</code> 三元组：$x = (h,r,t)$，确定一个数值 $y\in \mathbb{R}$ 来反应一个给定三元组反映真实知识的确信度；</p><ul><li><code>head</code> 和 <code>tail</code> 都是任意长度词序列：<ul><li>$h = \{ h_1, h_2, …, h_n\}$</li><li>$t = \{t_1, t_2, …, t_m \}$</li></ul></li><li>关系 $r \in \mathbb{R}$ 属于一个已知的关系集合；</li><li>目标：确定一个函数 $f$ 映射 relational triples 到一个有效性打分；<ul><li>$f(x) = \sigma(\tau (x))$</li><li>$\tau$  是句子生成函数；</li><li>$\sigma$ 是打分模型；</li></ul></li></ul></li><li><p>本文方法依赖于两种类型的PLM：标准的单向模型和Masked双向模型；</p></li><li><p>Coherency Ranking (一致性打分)</p><ul><li>将三元组根据模板转化为一个自然语句，可以产生一个候选句子集合，根据预训练单向语言模型 $P_{coh}$ 选择具有最高log-likelihood打分的候选句子；<ul><li>$S^\ast = argmax_{S\in \mathcal{S}} [logP_{coh}(S)]$</li></ul></li></ul></li><li><p>Scoring Generated Triples</p><ul><li>$PMI(t,h|r) = log p(t|h, r) - log p(t|r)$</li><li>使用MLM模型 $P_{cmp}$ 计算打分;</li><li>$p(t|h,r) = P_{cmp} (w_i = t| w_{1:i-1}, w_{i+1:m})$<ul><li>由于 <code>tail</code> 可能是由$j$个词构成，使用贪婪近似的方法来估计其概率。</li><li>首先 mask 所有 <code>tail</code> 中的词，然后计算每个的概率；</li><li>然后找到最大概率的词的概率 $p_k$ ，重复 $j$ 次；</li><li>最终的条件似然：$p(t|h,r) = \prod _{k=1}^{j} p_k$</li></ul></li><li>边缘概率 $p(t|r)$ 计算类似，只不过在此时将 <code>head</code> 也全都mask掉；</li><li><img src="/../images/papers-prob-lm-csk/image-20191220000101665.png" alt="image-1"></li></ul></li><li><p>最终的分数：</p><ul><li>$PMI_{\lambda}(t,h|r) = \lambda log p (t|h,r) - log p(t|r)$</li><li>$\lambda$ 是超参数；</li><li>由于PMI的计算是对称的，我们还可以计算得到 $PMI_{\lambda}(h,t|r)$ 打分；</li></ul></li><li><p>最终是将两个打分进行平均，以减少我们估计的方差；</p></li></ul><h4 id="Cited"><a href="#Cited" class="headerlink" title="Cited"></a>Cited</h4><ul><li>A recent study shows how to attain common sense knowledge from pretrained MNLMs without additional training (Feldman, Davison, and Rush 2019).</li></ul><h2 id="Evaluating-Commonsense-in-Pre-trained-Language-Models-AAAI-2020"><a href="#Evaluating-Commonsense-in-Pre-trained-Language-Models-AAAI-2020" class="headerlink" title="Evaluating Commonsense in Pre-trained Language Models. AAAI,2020."></a>Evaluating Commonsense in Pre-trained Language Models. AAAI,2020.</h2><blockquote><p>Authors: Xuhui Zhou, Yue Zhang, Leyang Cui, Dandan Huang<br>Org.: UW, Westlake University, Zhejiang University</p></blockquote><p>Motivation:</p><ul><li>目前已经有很多工作对 Contextualize Representation（CR） 的 syntactic、semantic、word sense 知识进行了研究，并解释了为什么 CR 适用于这些任务；<ul><li>但是鲜有工作调查 CR 中包含的 Commonsense Knowledge；</li></ul></li><li>本文主要研究 GPT、BERT、XLNET、RoBERTa 的常识能力（Commonsense ability），通过在几个 challenging benchmarks 上进行测试，我们发现：<ul><li>1、语言模型及其变体是提升模型常识能力的 effective objectives；</li><li>2、双向上下文和更大的训练集是加分项；</li><li>3、在需要更多推理步的任务上，现有的语言模型效果表现不佳；</li><li>通过构造 dual test cases 测试模型的鲁棒性，结果表明，LMs对于这些cases表示出了困惑，LMs只是学到了surface层面的常识而不是的deep level；</li></ul></li></ul><p>本文的方法：</p><ul><li><p>benchmark包括：Conjunction Acceptability、Sense Making、WSC、SWAG、HellaSwag、Sense Making with Reasoning、Argument Reasoning Comprehension；</p><ul><li>对上述数据集进行分类和重组，划分为 word-level 和 sentence-level 两组，从这两个层次来测试模型；<ul><li>word-level：CA、WSC、SM；<ul><li>关注 名词、动词、形容词、副词、代词、连词；</li></ul></li><li>sentence-level：SMR、SWAG、HellaSwag、ARCT；<ul><li>关注 常识推理；</li></ul></li></ul></li></ul></li><li><p><strong>常识能力</strong>可以分为两个方面：</p><ul><li>拥有常识能力的模型应该具备关于世界的基础知识；</li><li>拥有常识能力的模型应该具备基于常识知识的推理能力；</li></ul></li><li><p>实验设计（Experiment Design）</p><ul><li>从 单向上下文LM模型 和 双向上下文LM模型 两类分别考虑一个句子的打分；</li><li>假设句子 $S$ 有 $n$ 个词组成 $S=\{w_1, …, w_{k-1}, w_k, w_{k+1}, …, w_n\}$，定义其得分为：<script type="math/tex">Score(S)=\frac{\sum_{k=1}^n log(P_\theta (w_k | context_k))}{n}</script><ul><li>分母上的 $n$ 是为了消除句子长度的影响；</li></ul></li><li>对于单向模型：<ul><li>$context_k = S_{&lt;k} = \{w_1, …, w_{k-1}\}$</li></ul></li><li>对于双向模型：<ul><li>$context_k = S_{-k}$ ，表示移除 $S$ 中的第 $k$ 个词，被替换为 <code>[MASK]</code>；</li></ul></li><li>直觉上，$P_{\theta}(w_k | context_k)$ 可以解释为 在给定 $context_k$ 的条件下 $w_k$ （出现）的可能性；</li><li>在计算完句子打分之后，模型选择得分高的结果作为预测输出；</li></ul></li><li><p>Robustness Test，通过几种方法构造dual test样例</p><ul><li>add、delete、replace、swap two words；</li><li>理论上，具备相关常识知识的模型，应该对于一对dual test样例给出<strong>一致</strong>的预测结果。</li><li>为了更好的调查模型给出poor consistency的原因，计算模型是如何在 $S_{correct}$ 和 $S_{incorrect}$ 之间做出选择的， 两个 $S$ 拥有相同的词数；<ul><li>$q_k = log( \frac{P_{\theta} (w_k | S_{correct} - \{w_k\}) }{ P_{\theta} (w_k | S_{incorrect} - \{w_k\}) })$， $1 \leq k \leq n$</li><li>$Q = \sum_k q_k$<ul><li>看 $Q$ 是否大于0</li></ul></li><li>打分方法参考【A simple method for commonsense reasoning. 2018.】</li></ul></li></ul></li></ul><h2 id="Why-Do-Masked-Neural-LMs-Still-Need-Common-Sense-Knowledge"><a href="#Why-Do-Masked-Neural-LMs-Still-Need-Common-Sense-Knowledge" class="headerlink" title="Why Do Masked Neural LMs Still Need Common Sense Knowledge?"></a>Why Do Masked Neural LMs Still Need Common Sense Knowledge?</h2><blockquote><p>Authors: Sunjae Kwon et al.<br>Org.: Korea Advanced Institute of Science and Technology</p></blockquote><p>MNLM = masked neural language model</p><ul><li>本文的工作：本文的重点是通过理解单词之间的语义关系，来验证基于MNLM的RC模型可以回答多少或处理多少复杂的RC任务。 针对这个问题，我们提出了以下有关MNLM的常识理解的问题<ul><li>1、MNLM是否理解不同的常识知识类型，尤其是关于 relations of attributes；</li><li>2、MNLM是否理解 两个相关关系的关联（a relationship between two related relations）；</li><li>3、基于MNLM的RC模型如何解决不同难度水平的问题？</li><li>4、对于基于MNLM的RC模型，最难的RC任务是什么？</li></ul></li><li>针对问题1和问题2，设计了一个 knowledge probing test 来分析MNLM是否理解结构化的常识知识，如外部知识库中的语义三元组；<ul><li>实验结果反映，MNLM可以部分地理解不同类型的常识知识，并且发现还有很多知识尚未训练到，不能准确地分别relations；</li></ul></li><li>针对问题3和问题4，首先根据context和question的lexical overlapping定义了一个RC问题的难度；<ul><li>lexical variation（词汇多样性）是一个RC难度的关键的决定因素；</li><li>问题需要常识知识的对基于MNLM的RC模型是个挑战；</li></ul></li><li>基于上述的结果，本文提出了一个改善MNLM的方法，通过外部知识库加入知识：<ul><li>任务数据集：SQuAD</li><li>manual，</li><li>automatic，提出了一个神经网络结构来引入知识；<ul><li><img src="/../images/papers-prob-lm-csk/image-20191217153433549.png" alt="image-2"></li></ul></li></ul></li><li>本文贡献：<ul><li>提出了一个 knowledge probing 测试，度量训练的 MNLM 中包含多少常识知识；<ul><li>发现，MNLM，并没有完全、准确地理解知识；</li></ul></li><li>通过审查基于MNLM的RC模型的结果，发现目前的MNLM在解决需要常识知识的问题上存在决定性的瓶颈；</li><li>实验性的证实了，可以通过手工/自动地方式向MNLM中补充常识知识；</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> note </tag>
            
            <tag> commonsense </tag>
            
            <tag> lm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Wluper Blog - Dialogue System</title>
      <link href="/2019/11/19/note-20191119-wluper-blog-dialogue/"/>
      <url>/2019/11/19/note-20191119-wluper-blog-dialogue/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Blog series about Dialogue Systems by wluper:</p><h2 id="Part-0-Overview"><a href="#Part-0-Overview" class="headerlink" title="Part 0: Overview"></a>Part 0: Overview</h2><p>An overview of Conversational AI</p><ul><li><a href="https://medium.com/voice-tech-podcast/an-overview-of-conversational-ai-174adc295f1e" target="_blank" rel="noopener">https://medium.com/voice-tech-podcast/an-overview-of-conversational-ai-174adc295f1e</a></li></ul><h2 id="Part-1-State-Tracking"><a href="#Part-1-State-Tracking" class="headerlink" title="Part 1: State Tracking"></a>Part 1: State Tracking</h2><p>Not another Conversation AI report, But an overview on SOTA and Emerging Dialogue Management.</p><ul><li><a href="https://medium.com/wluper/not-another-conversational-ai-report-4a094337e6f1" target="_blank" rel="noopener">https://medium.com/wluper/not-another-conversational-ai-report-4a094337e6f1</a></li></ul><h2 id="Part-2-Dialogue-Policies"><a href="#Part-2-Dialogue-Policies" class="headerlink" title="Part 2: Dialogue Policies"></a>Part 2: Dialogue Policies</h2><p>How do Dialogue Systems decide what to say or which actions to take?</p><ul><li><a href="https://medium.com/wluper/how-do-dialogue-systems-decide-what-to-say-or-which-actions-to-take-b32ca223aff1" target="_blank" rel="noopener">https://medium.com/wluper/how-do-dialogue-systems-decide-what-to-say-or-which-actions-to-take-b32ca223aff1</a></li></ul><h2 id="Part-3-Common-Sense"><a href="#Part-3-Common-Sense" class="headerlink" title="Part 3: Common Sense"></a>Part 3: Common Sense</h2><p>TBU</p><h2 id="Part-4-Dynamic-Memory"><a href="#Part-4-Dynamic-Memory" class="headerlink" title="Part 4: Dynamic Memory"></a>Part 4: Dynamic Memory</h2><p>TBU</p><h2 id="Part-5-Learning"><a href="#Part-5-Learning" class="headerlink" title="Part 5: Learning"></a>Part 5: Learning</h2><p>TBU</p><h2 id="More-by-Wluper"><a href="#More-by-Wluper" class="headerlink" title="More by Wluper"></a>More by Wluper</h2><ul><li>Methods to grow your own data sets for Conversational AI. <a href="https://medium.com/wluper/methods-to-grow-your-own-data-sets-for-conversational-ai-1bfeac9508ef?source=collection_home---4------0-----------------------" target="_blank" rel="noopener">link</a></li></ul><p>Appendix:<br>Full list of references and links: <a href="https://wluper.com/appendix" target="_blank" rel="noopener">https://wluper.com/appendix</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP-Resource </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> tutorial </tag>
            
            <tag> dialogue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EMNLP 2019 Highlight and Resource</title>
      <link href="/2019/11/19/note-20191119-emnlp2019-resource/"/>
      <url>/2019/11/19/note-20191119-emnlp2019-resource/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>part of this post refers from: <a href="http://newsletter.ruder.io/issues/highlights-of-emnlp-2019-ethics-in-nlp-vol-2-ai-and-journalism-206147" target="_blank" rel="noopener">NLP News by Sebastian Ruder</a></p></blockquote><h2 id="Some-Tutorials"><a href="#Some-Tutorials" class="headerlink" title="Some Tutorials"></a>Some Tutorials</h2><ul><li><p><strong>Data Collection and End-to-End Learning for Conversational AI</strong></p><ul><li>by <strong>PolyAI</strong></li><li><a href="http://www.polyai.com/wp-content/uploads/2019/11/EMNLP19-PolyAI-Tutorial.pdf?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank" rel="noopener">link</a></li><li><a href="https://pan.baidu.com/s/14iOourCh445rMJf93-oCTA" target="_blank" rel="noopener">backup link</a></li></ul></li><li><p><strong>Graph-based Deep Learning in Natural Language Processing</strong></p><ul><li>by <strong>Shikhar Vashishth</strong></li><li><a href="https://github.com/svjan5/GNNs-for-NLP" target="_blank" rel="noopener">code link</a></li><li><a href="https://pan.baidu.com/s/1hBNhQbQuQP-wunRk6I6Mqw" target="_blank" rel="noopener">slide link</a></li></ul></li><li><p><strong>Semantic Specialization of Distributional Word Vectors</strong></p><ul><li>by <strong>Goran Glavas, Deoardo Ponti, Ivan Vulic</strong></li><li><a href="https://docs.google.com/presentation/d/1QwD6Vd-SWJJWdR-QmAHWYDlxfHHeKTmEznDdIZg5aag/edit#slide=id.g622360cd48_0_4" target="_blank" rel="noopener">google doc link</a></li><li><a href="https://pan.baidu.com/s/19gPEPNZeLAso5zIQ07NzFQ" target="_blank" rel="noopener">backup link</a></li></ul></li><li><p>Topic: <strong>A SOTA-less, novelty-less journey into Neural Sequence Models</strong></p><ul><li>by <strong>Kyunghyun Cho</strong></li><li><a href="https://drive.google.com/file/d/1HGzv6n9hAj-GL63POUZCO6nCrIHF9y35/view" target="_blank" rel="noopener">google drive link</a></li><li><a href="https://pan.baidu.com/s/1zK3mICpU7NqdNRuFBjlNTw" target="_blank" rel="noopener">backup link</a></li></ul></li><li><p>Topic：<strong>cross-lingual and cross-domain learning</strong></p><ul><li>by <strong>Barbara Plank</strong> </li><li>at the DeepLo 2019 workshop</li><li><a href="https://www.dropbox.com/s/myle46vl64nasg8/Deeplo-talk-2019.pdf?dl=0&amp;utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank" rel="noopener">dropbox link</a></li><li><a href="https://pan.baidu.com/s/1r7Dr_rwoglSUwLdqgZ50eQ" target="_blank" rel="noopener">backup link</a></li></ul></li></ul><h2 id="Highlight-Blogs"><a href="#Highlight-Blogs" class="headerlink" title="Highlight Blogs"></a>Highlight Blogs</h2><ul><li>Michael Galkin’s two-part blog posts about <strong>Knowledge Graphs &amp; NLP</strong><ul><li><a href="https://medium.com/@mgalkin/knowledge-graphs-nlp-emnlp-2019-part-i-e4e69fd7957c" target="_blank" rel="noopener">Part 1</a></li><li><a href="https://medium.com/@mgalkin/knowledge-graphs-nlp-emnlp-2019-part-ii-56f5b03ad9ba" target="_blank" rel="noopener">Part 2</a></li></ul></li><li>Naver Labs Europe’s blog<ul><li><a href="https://europe.naverlabs.com/blog/emnlp2019-what-we-saw-and-liked/?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank" rel="noopener">link</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP-Resource </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> tutorial </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Papers on MRC with Graph Neural Networks</title>
      <link href="/2019/11/12/mrc-paper-with-gnn/"/>
      <url>/2019/11/12/mrc-paper-with-gnn/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>A list of recent papers about <strong>Machine Reading Comprehension with Graph Neural Networks</strong> (<strong>MRC-GNN</strong>).</p><p>Update on <strong>Nov. 12, 2019</strong>.</p><a id="more"></a><p>Divide the application of GNN in the MRC tasks into the following categories according to the role of GNN in the MRC models:</p><ul><li>encoding the external structured knowledge;</li><li>process multi-hop reasoning;</li><li>gather information over multiple paragraphs;</li><li>mixture way;</li></ul><p>TBU</p>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> paper </tag>
            
            <tag> mrc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Papers on Multiple-Hop Machine Reading Comprehension</title>
      <link href="/2019/11/08/mrc-paper-list-multi-hop/"/>
      <url>/2019/11/08/mrc-paper-list-multi-hop/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>A list of recent papers about <strong>Multiple-Hop Machine Reading Comprehension</strong> (<strong>MHMRC</strong>).</p><p>Contributed by <a href="https://github.com/XingLuxi" target="_blank" rel="noopener">Luxi Xing</a> and <a href="https://github.com/IndexFziQ" target="_blank" rel="noopener">Yuqiang Xie</a>.</p><p>Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China. </p><p>Update on <strong>Nov. 08, 2019</strong>.</p><a id="more"></a><style>    table th:nth-of-type(1){    width: 70%;    }</style><h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a><a href="#content">Content</a></h2><ol><li>Benchmark-Datasets</li><li>WikiHop Tasks</li><li>HotpotQA Tasks</li></ol><h2 id="Benchmark-Datasets"><a href="#Benchmark-Datasets" class="headerlink" title="Benchmark Datasets"></a><a href="#content">Benchmark Datasets</a></h2><ol><li><p>[<strong>WikiHop</strong>] Constructing Datasets for Multi-hop Reading Comprehension Across Documents. TACL,2018. [<a href="https://www.aclweb.org/anthology/Q18-1021/" target="_blank" rel="noopener">paper</a> / <a href="http://qangaroo.cs.ucl.ac.uk/" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Johannes Welbl, Pontus Stenetorp, Sebastian Riedel</em></p></li><li><p><strong>HotpotQA</strong>: A Dataset for Diverse, Explainable Multi-hop Question Answering. EMNLP,2018. [<a href="https://www.aclweb.org/anthology/D18-1259/" target="_blank" rel="noopener">paper</a> / <a href="https://hotpotqa.github.io/" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D. Manning</em></p></li></ol><h2 id="WikiHop-Tasks"><a href="#WikiHop-Tasks" class="headerlink" title="WikiHop Tasks"></a><a href="#content">WikiHop Tasks</a></h2><!-- | Title | Publish | Links || :---- | :---: | :---:|| Neural Models for Reasoning over Multiple Mentions using Coreference | NAACL<br>2018<br>short | [paper](https://arxiv.org/abs/1804.05922) | --><ol><li>Neural Models for Reasoning over Multiple Mentions using Coreference. NAACL,2018,short. <a href="https://arxiv.org/abs/1804.05922" target="_blank" rel="noopener">paper</a>.</li><li>Exploring graph-structured passage representation for multihop reading comprehension with graph neural networks. 2018.</li><li>Exploiting explicit paths for multihop reading comprehension. 2018.</li><li>BAG: Bi-directional Attention Entity Graph Convolutional Network for Multi-hop Reasoning Question Answering. NAACL,2019.</li><li>Question Answering by Reasoning Across Documents with Graph Convolutional Networks. NAACL,2019.</li><li>Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs. ACL,2019.</li><li>Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question Answering. ICLR,2019.</li><li>Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension. ACL,2019.</li></ol><h2 id="HotpotQA-Tasks"><a href="#HotpotQA-Tasks" class="headerlink" title="HotpotQA Tasks"></a><a href="#content">HotpotQA Tasks</a></h2><!-- | Title | Publish | Links || :---- | :---: | :---:|| Cognitive Graph for Multi-Hop Reading Comprehension at Scale | ACL<br>2019 | [paper](https://www.aclweb.org/anthology/P19-1259/) || Dynamically Fused Graph Network for Multi-hop Reasoning | ACL<br>2019 | [paper](https://www.aclweb.org/anthology/P19-1617/) || Answering while Summarizing: Multi-task Learning for Multi-Hop QA with Evidence Extraction | ACL<br>2019 | [paper](https://www.aclweb.org/anthology/P19-1225/) || Compositional Questions Do Not Necessitate Multi-hop Reasoning | ACL<br>2019 | [paper](https://www.aclweb.org/anthology/P19-1416/) || Multi-hop Reading Comprehension through Question Decomposition and Rescoring | ACL<br>2019 | [paper](https://www.aclweb.org/anthology/P19-1613/) || Answering Complex Open-domain Questions Through Iterative Query Generation | EMNLP<br>2019 | [paper](https://www.aclweb.org/anthology/D19-1261.pdf) || Revealing the Importance of Semantic Retrieval for Machine Reading at Scale | EMNLP<br>2019 | [paper](https://www.aclweb.org/anthology/D19-1258/) || Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks | EMNLP<br>2019<br>TextGraphs-13 | [paper](https://www.aclweb.org/anthology/D19-5306/) || Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning | EMNLP<br>2019 | [paper](https://www.aclweb.org/anthology/D19-1455/) || Multi-step Entity-centric Information Retrieval for Multi-Hop Question Answering | EMNLP<br>2019<br>MRQA | [paper](https://www.aclweb.org/anthology/D19-5816/) | --><ol><li>Cognitive Graph for Multi-Hop Reading Comprehension at Scale. ACL,2019. <a href="https://www.aclweb.org/anthology/P19-1259/" target="_blank" rel="noopener">paper</a>.</li><li>Dynamically Fused Graph Network for Multi-hop Reasoning. ACL,2019. <a href="https://www.aclweb.org/anthology/P19-1617/" target="_blank" rel="noopener">paper</a>.</li><li>Answering while Summarizing: Multi-task Learning for Multi-Hop QA with Evidence Extraction. ACL,2019. <a href="https://www.aclweb.org/anthology/P19-1225/" target="_blank" rel="noopener">paper</a>.</li><li>Compositional Questions Do Not Necessitate Multi-hop Reasoning. ACL,2019. <a href="https://www.aclweb.org/anthology/P19-1416/" target="_blank" rel="noopener">paper</a>.</li><li>Multi-hop Reading Comprehension through Question Decomposition and Rescoring. ACL,2019. <a href="https://www.aclweb.org/anthology/P19-1613/" target="_blank" rel="noopener">paper</a>.</li><li>Answering Complex Open-domain Questions Through Iterative Query Generation. EMNLP,2019. <a href="https://www.aclweb.org/anthology/D19-1261.pdf" target="_blank" rel="noopener">paper</a>.</li><li>Revealing the Importance of Semantic Retrieval for Machine Reading at Scale. EMNLP,2019. <a href="https://www.aclweb.org/anthology/D19-1258/" target="_blank" rel="noopener">paper</a>.</li><li>Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks. EMNLP,2019,TextGraphs-13. <a href="https://www.aclweb.org/anthology/D19-5306/" target="_blank" rel="noopener">paper</a>.</li><li>Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning. EMNLP,2019. <a href="https://www.aclweb.org/anthology/D19-1455/" target="_blank" rel="noopener">paper</a>.</li><li>Multi-step Entity-centric Information Retrieval for Multi-Hop Question Answering. EMNLP,2019,MRQA. <a href="https://www.aclweb.org/anthology/D19-5816/" target="_blank" rel="noopener">paper</a>.</li><li>Multi-Hop Paragraph Retrieval for Open-Domain Question Answering. ICLR,2019.</li><li>Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA. ACL,2019.</li><li>Multi-hop Reading Comprehension through Question Decomposition and Rescoring. ACL,2019.</li><li>Simple yet Effective Bridge Reasoning for Open-Domain Multi-Hop Question Answering. MRQA,2019.</li><li>Answering Complex Open-domain Questions Through Iterative Query Generation. EMNLP,2019.</li><li>Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents. 2019.</li><li>Propagate-Selector: Detecting Supporting Sentences for Question Answering via Graph Neural Networks. 2019.</li><li>Multi-hop Question Answering via Reasoning Chains. 2019.</li><li>A Road-map Towards Explainable Question Answering A Solution for Information Pollution. 2019.</li><li>Hierarchical Graph Network for Multi-hop Question Answering. 2019.</li><li>Multi-Paragraph Reasoning with Knowledge-enhanced Graph Neural Network. 2019.</li><li>Neural Module Networks for Reasoning over Text. 2019.</li><li>Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering. 2019.</li></ol>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> paper </tag>
            
            <tag> mrc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Papers on Knowledge-based Machine Reading Comprehension</title>
      <link href="/2019/10/23/mrc-knowledge-paper-list/"/>
      <url>/2019/10/23/mrc-knowledge-paper-list/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>A list of recent papers about <strong>Knowledge-based Machine Reading Comprehension</strong> (<strong>KMRC</strong>).</p><p>Update on <strong>April. 9, 2021</strong>.<br><a id="more"></a></p><p>Contributed by <a href="https://github.com/XingLuxi" target="_blank" rel="noopener">Luxi Xing</a>.</p><p>Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China. </p><p>We recommend to follow this link (<a href="https://github.com/XingLuxi/KMRC-Research-Archive" target="_blank" rel="noopener">https://github.com/XingLuxi/KMRC-Research-Archive</a>) to get more information.</p><p>( we will continuously update this list.)</p><h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a><a href="#content">Content</a></h2><!-- - [Content](#content)- [Survey Papers](#survey-papers)- [Cloze Style Tasks](#cloze-style-tasks)- [Span Extraction Tasks](#span-extraction-tasks)- [Multiple Choice Tasks](#multiple-choice-tasks)- [Generation Tasks](#generation-tasks)- [Benchmark Datasets](#benchmark-datasets)- [Other Paper List About MRC](#other-paper-list-about-mrc) --><ol><li>Survey Papers</li><li>Benchmark-Datasets</li><li>Cloze-Style-Tasks</li><li>Span-Extraction-Tasks</li><li>Multiple-Choice-Tasks</li><li>Generation-Tasks</li></ol><p>Note: papers about KBQA will be not included in this list.</p><style>    table th:nth-of-type(1){    width: 60%;    }</style><h2 id="Survey-Papers"><a href="#Survey-Papers" class="headerlink" title="Survey Papers"></a><a href="#content">Survey Papers</a></h2><blockquote><p>For more detail, can refer to <a href="https://github.com/XingLuxi/KMRC-Research-Archive/blob/master/content/surveys.md" target="_blank" rel="noopener">this link</a></p></blockquote><ol><li><p><strong>Neural Machine Reading Comprehension: Methods and Trends</strong>. 2019. [<a href="https://arxiv.org/abs/1907.01118" target="_blank" rel="noopener">paper</a>]</p><p> Authors: <em>Shanshan Liu, Xin Zhang, Sheng Zhang, Hui Wang, Weiming Zhang</em></p></li><li><p><strong>Machine Reading Comprehension: a Literature Review</strong>. 2019. [<a href="https://arxiv.org/abs/1907.01686" target="_blank" rel="noopener">paper</a>]</p><p> Authors: <em>Xin Zhang, An Yang, Sujian Li, Yizhong Wang</em></p></li><li><p><strong>Natural Language QA Approaches using Reasoning with External Knowledge</strong>. 2020. [<a href="http://arxiv.org/abs/2003.03446" target="_blank" rel="noopener">paper</a>]</p><p> Authors: <em>Chitta Baral, Pratyay Banerjee, Kuntal Pal, Arindam Mitra</em></p></li></ol><h2 id="Cloze-Style-Tasks"><a href="#Cloze-Style-Tasks" class="headerlink" title="Cloze Style Tasks"></a><a href="#content">Cloze Style Tasks</a></h2><div class="table-container"><table><thead><tr><th style="text-align:left">Title</th><th style="text-align:center">Publish</th><th style="text-align:left">Tasks</th><th style="text-align:center">Links</th></tr></thead><tbody><tr><td style="text-align:left">Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension</td><td style="text-align:center">ACL<br>2017</td><td style="text-align:left">SCT</td><td style="text-align:center"><a href="http://aclweb.org/anthology/D17-1216" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions</td><td style="text-align:center">EMNLP<br>2017</td><td style="text-align:left">Rare Entity Prediction</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/D17-1086" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge</td><td style="text-align:center">ACL<br>2018</td><td style="text-align:left">Common Nouns</td><td style="text-align:center"><a href="http://aclweb.org/anthology/P18-1076" target="_blank" rel="noopener">paper</a> <br> <a href="http://xingluxi.github.io/2019/01/09/paper-knreader/">note</a></td></tr><tr><td style="text-align:left">A Multi-Attention based Neural Network with External Knowledge for Story Ending Predicting Task</td><td style="text-align:center">COLING<br>2018</td><td style="text-align:left">SCT</td><td style="text-align:center"><a href="http://www.aclweb.org/anthology/C18-1149" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Incorporating Structured Commonsense Knowledge in Story Completion</td><td style="text-align:center">AAAI<br>2018</td><td style="text-align:left">SCT</td><td style="text-align:center"><a href="https://arxiv.org/abs/1811.00625" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Story Ending Prediction by Transferable BERT</td><td style="text-align:center">IJCAI<br>2019</td><td style="text-align:left">SCT</td><td style="text-align:center"><a href="https://arxiv.org/abs/1905.07504" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Toward Better Storylines with Sentence-Level Language Models</td><td style="text-align:center">ACL<br>2020</td><td style="text-align:left">SCT</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/2020.acl-main.666" target="_blank" rel="noopener">paper</a></td></tr></tbody></table></div><h2 id="Span-Extraction-Tasks"><a href="#Span-Extraction-Tasks" class="headerlink" title="Span Extraction Tasks"></a><a href="#content">Span Extraction Tasks</a></h2><div class="table-container"><table><thead><tr><th style="text-align:left">Title</th><th style="text-align:center">Publish</th><th style="text-align:left">Tasks</th><th style="text-align:center">Links</th></tr></thead><tbody><tr><td style="text-align:left">Dynamic Integration of Background Knowledge in Neural NLU Systems</td><td style="text-align:center">2018</td><td style="text-align:left">SQuAD/<br>TriviaQA</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1706.02596.pdf" target="_blank" rel="noopener">paper</a><br><a href="http://xingluxi.github.io/2019/03/06/paper-2018-refinewordemb/">note</a></td></tr><tr><td style="text-align:left">Explicit Utilization of General Knowledge in Machine Reading Comprehension</td><td style="text-align:center">ACL<br>2019</td><td style="text-align:left">SQuAD</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/P19-1219" target="_blank" rel="noopener">paper</a> <br> <a href="http://xingluxi.github.io/2019/07/17/paper-acl2019-kar/">note</a></td></tr><tr><td style="text-align:left">Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension</td><td style="text-align:center">ACL<br>2019</td><td style="text-align:left">SQuAD/<br>ReCoRD</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/P19-1226/" target="_blank" rel="noopener">paper</a> <br> <a href="http://xingluxi.github.io/2019/07/29/paper-acl2019-kt-net/">note</a></td></tr><tr><td style="text-align:left">Machine Reading Comprehension Using Structural Knowledge Graph-aware Network</td><td style="text-align:center">EMNLP<br>2019<br>short</td><td style="text-align:left">ReCoRD</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/D19-1602" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left"><code>+</code> SG-Net: Syntax-Guided Machine Reading Comprehension</td><td style="text-align:center">AAAI<br>2020</td><td style="text-align:left">SQuAD2.0<br>RACE</td><td style="text-align:center"><a href="http://arxiv.org/abs/1908.05147" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left"><code>+</code> Semantics-aware BERT for Language Understanding</td><td style="text-align:center">AAAI<br>2020</td><td style="text-align:left">SQuAD2.0</td><td style="text-align:center"><a href="http://arxiv.org/abs/1909.02209" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left"><code>+</code> Entities as Experts: Sparse Memory Access with Entity Supervision</td><td style="text-align:center">2020</td><td style="text-align:left">TriviaQA</td><td style="text-align:center"><a href="http://in.arxiv.org/abs/2004.07202" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left"><code>+</code> Semantics-Aware Inferential Network for Natural Language Understanding</td><td style="text-align:center">2020</td><td style="text-align:left">*</td><td style="text-align:center"><a href="http://arxiv.org/abs/2004.13338" target="_blank" rel="noopener">paper</a></td></tr></tbody></table></div><ul><li><code>+</code> indicates works regarding injecting knowledge to improve performance on the datasets not included in this summary.</li></ul><h2 id="Multiple-Choice-Tasks"><a href="#Multiple-Choice-Tasks" class="headerlink" title="Multiple Choice Tasks"></a><a href="#content">Multiple Choice Tasks</a></h2><div class="table-container"><table><thead><tr><th style="text-align:left">Title</th><th style="text-align:center">Publish</th><th style="text-align:left">Tasks</th><th style="text-align:center">Links</th></tr></thead><tbody><tr><td style="text-align:left">Yuanfudao at SemEval-2018 Task 11: Three-way Attention and Relational Knowledge for Commonsense Machine Comprehension</td><td style="text-align:center">SemEval<br>2018</td><td style="text-align:left">SemEval-2018 Task 11</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/S18-1120/" target="_blank" rel="noopener">paper</a> <br> <a href="https://github.com/intfloat/commonsense-rc" target="_blank" rel="noopener">code</a></td></tr><tr><td style="text-align:left">Improving Question Answering by Commonsense-Based Pre-Training</td><td style="text-align:center">AAAI<br>2019</td><td style="text-align:left">ARC/<br>OpenBookQA/<br>SemEval-2018 Task 11</td><td style="text-align:center"><a href="https://arxiv.org/abs/1809.03568" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Improving Machine Reading Comprehension with General Reading Strategies</td><td style="text-align:center">NAACL<br>2019</td><td style="text-align:left">ARC/ OpenBookQA/ MCTest/<br>SemEval-2018 Task 11/ SCT/ MultiRC</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/N19-1270/" target="_blank" rel="noopener">paper</a> <br> <a href="https://github.com/nlpdata/strategy/" target="_blank" rel="noopener">code</a></td></tr><tr><td style="text-align:left">Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict Human Needs</td><td style="text-align:center">NAACL<br>2019</td><td style="text-align:left">story commonsense</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/papers/N/N19/N19-1368/" target="_blank" rel="noopener">paper</a> <br> <a href="https://github.com/debjitpaul/Multi-Hop-Knowledge-Paths-Human-Needs" target="_blank" rel="noopener">code</a></td></tr><tr><td style="text-align:left">Incorporating Relation Knowledge into Commonsense Reading Comprehension with Multi-task Learning</td><td style="text-align:center">CIKM<br>2019</td><td style="text-align:left">SemEval-2018 Task 11 / SCT</td><td style="text-align:center"><a href="https://arxiv.org/abs/1908.04530" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Explain Yourself! Leveraging Language Models for Commonsense Reasoning</td><td style="text-align:center">ACL<br>2019</td><td style="text-align:left">CommonsenseQA</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/P19-1487.pdf" target="_blank" rel="noopener">paper</a> <br> <a href="https://github.com/salesforce/cos-e" target="_blank" rel="noopener">code</a> <br> <a href="http://xingluxi.github.io/2019/07/10/paper-acl2019-cos-e/">note</a></td></tr><tr><td style="text-align:left">Careful Selection of Knowledge to solve Open Book Question Answering</td><td style="text-align:center">ACL<br>2019</td><td style="text-align:left">OpenBookQA</td><td style="text-align:center"><a href="https://arxiv.org/abs/1907.10738" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Improving Question Answering with External Knowledge</td><td style="text-align:center">EMNLP<br>MRQA<br>2019</td><td style="text-align:left">ARC/<br>OpenBookQA</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1902.00993.pdf" target="_blank" rel="noopener">paper</a> <br> <a href="http://xingluxi.github.io/2019/08/26/paper-2019-edl-md/">note</a></td></tr><tr><td style="text-align:left">KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning</td><td style="text-align:center">EMNLP<br>2019</td><td style="text-align:left">CommonsenseQA</td><td style="text-align:center"><a href="https://arxiv.org/abs/1909.02151" target="_blank" rel="noopener">paper</a> <br> <a href="https://github.com/INK-USC/KagNet" target="_blank" rel="noopener">code</a> <br> <a href="https://zhuanlan.zhihu.com/p/81917730" target="_blank" rel="noopener">note</a></td></tr><tr><td style="text-align:left">What’s Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering</td><td style="text-align:center">EMNLP<br>2019</td><td style="text-align:left">OpenBookQA</td><td style="text-align:center"><a href="https://arxiv.org/abs/1909.09253" target="_blank" rel="noopener">paper</a> <br> <a href="https://zhuanlan.zhihu.com/p/85852386" target="_blank" rel="noopener">note</a></td></tr><tr><td style="text-align:left">BIG MOOD: Relating Transformers to Explicit Commonsense Knowledge</td><td style="text-align:center">EMNLP<br>COIN<br>2019</td><td style="text-align:left">MCScripts v2</td><td style="text-align:center"><a href="http://arxiv.org/abs/1910.07713" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models</td><td style="text-align:center">2019</td><td style="text-align:left">CSQA<br>WSC</td><td style="text-align:center"><a href="https://arxiv.org/abs/1908.06725v1" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Abductive Reasoning as Self-Supervision for Common Sense Question Answering</td><td style="text-align:center">2019</td><td style="text-align:left">Swag<br>HellaSwag</td><td style="text-align:center"><a href="http://arxiv.org/abs/1909.03099" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Exploring ways to incorporate additional knowledge to improve Natural Language Commonsense Question Answering</td><td style="text-align:center">2019</td><td style="text-align:left">ANLI/<br>SocialIQA</td><td style="text-align:center"><a href="http://arxiv.org/abs/1909.08855" target="_blank" rel="noopener">paper</a><br><a href="http://xingluxi.github.io/2019/09/26/paper-kn-mcqa-1909-08855/">note</a></td></tr><tr><td style="text-align:left">Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering</td><td style="text-align:center">AAAI<br>2020</td><td style="text-align:left">CSQA</td><td style="text-align:center"><a href="http://arxiv.org/abs/1909.05311" target="_blank" rel="noopener">paper</a><br><a href="http://xingluxi.github.io/2019/09/17/paper-csqa-1909-05311/">note</a></td></tr><tr><td style="text-align:left">Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering</td><td style="text-align:center">2019</td><td style="text-align:left">SIQA</td><td style="text-align:center"><a href="https://arxiv.org/abs/1911.03876" target="_blank" rel="noopener">paper</a><br><a href="https://xingluxi.github.io/2020/04/01/paper-2019ai2-comet-csqa/">note</a></td></tr><tr><td style="text-align:left">K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters</td><td style="text-align:center">2020</td><td style="text-align:left">CosmosQA</td><td style="text-align:center"><a href="http://arxiv.org/abs/2002.01808" target="_blank" rel="noopener">paper</a><br><a href="https://xingluxi.github.io/2020/02/10/paper-2019-k-adapter/">note</a></td></tr><tr><td style="text-align:left">Unsupervised Commonsense Question Answering with Self-Talk</td><td style="text-align:center">2020</td><td style="text-align:left">CSQA<br>SIQA</td><td style="text-align:center"><a href="http://arxiv.org/abs/2004.05483" target="_blank" rel="noopener">paper</a><br><a href="https://xingluxi.github.io/2020/04/26/paper-2020-2004-05483-self-talk/">note</a></td></tr><tr><td style="text-align:left">L2R2: Leveraging Ranking for Abductive Reasoning</td><td style="text-align:center">SIGIR<br>2020</td><td style="text-align:left">ANLI</td><td style="text-align:center"><a href="http://arxiv.org/abs/2005.11223" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering</td><td style="text-align:center">ACL<br>2020</td><td style="text-align:left">MultiRC<br>QASC</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/2020.acl-main.414" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Logic-Guided Data Augmentation and Regularization for Consistent Question Answering</td><td style="text-align:center">ACL<br>2020</td><td style="text-align:left">WIQA<br>QuaREL</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/2020.acl-main.499" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering</td><td style="text-align:center">2020</td><td style="text-align:left">CSQA<br>OBQA</td><td style="text-align:center"><a href="http://arxiv.org/abs/2005.00691" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Fusing Context Into Knowledge Graph for Commonsense Reasoning</td><td style="text-align:center">2020</td><td style="text-align:left">CSQA</td><td style="text-align:center"><a href="http://arxiv.org/abs/2012.04808" target="_blank" rel="noopener">paper</a></td></tr></tbody></table></div><h2 id="Generation-Tasks"><a href="#Generation-Tasks" class="headerlink" title="Generation Tasks"></a><a href="#content">Generation Tasks</a></h2><p>Also known as <strong>Free-form Answer Tasks</strong></p><div class="table-container"><table><thead><tr><th style="text-align:left">Title</th><th style="text-align:center">Publish</th><th style="text-align:left">Tasks</th><th style="text-align:center">Links</th></tr></thead><tbody><tr><td style="text-align:left">Commonsense for Generative Multi-Hop Question Answering Tasks</td><td style="text-align:center">EMNLP<br>2018</td><td style="text-align:left">NarrativeQA/<br>WikiHop</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/D18-1454/" target="_blank" rel="noopener">paper</a> <br> <a href="https://github.com/yicheng-w/CommonSenseMultiHopQA" target="_blank" rel="noopener">code</a> <br> <a href="http://xingluxi.github.io/2019/02/21/paper-emnlp2018-mhpgm/">note</a></td></tr><tr><td style="text-align:left">COMET: Commonsense Transformers for Automatic Knowledge Graph Construction</td><td style="text-align:center">ACL<br>2019</td><td style="text-align:left">Atomic</td><td style="text-align:center"><a href="https://arxiv.org/abs/1906.05317" target="_blank" rel="noopener">paper</a><br><a href="https://github.com/atcbosselut/comet-commonsense" target="_blank" rel="noopener">code</a><br><a href="https://indexfziq.github.io/2019/07/03/COMET/" target="_blank" rel="noopener">note</a></td></tr><tr><td style="text-align:left">Incorporating External Knowledge into Machine Reading for Generative Question Answering</td><td style="text-align:center">EMNLP<br>2019</td><td style="text-align:left">MS MARCO</td><td style="text-align:center"><a href="http://arxiv.org/abs/1909.02745" target="_blank" rel="noopener">paper</a><br><a href="http://xingluxi.github.io/2019/10/13/paper-emnlp2019-keag/">note</a></td></tr><tr><td style="text-align:left">Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder</td><td style="text-align:center">EMNLP<br>2019</td><td style="text-align:left">Event2Mind</td><td style="text-align:center"><a href="http://arxiv.org/abs/1909.08824" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering</td><td style="text-align:center">2019</td><td style="text-align:left">SocialIQA<br>StoryCommonsense</td><td style="text-align:center"><a href="http://arxiv.org/abs/1911.03876" target="_blank" rel="noopener">paper</a></td></tr><tr><td style="text-align:left">Injecting Numerical Reasoning Skills into Language Models</td><td style="text-align:center">ACL<br>2020</td><td style="text-align:left">DROP</td><td style="text-align:center"><a href="https://www.aclweb.org/anthology/2020.acl-main.89" target="_blank" rel="noopener">paper</a></td></tr></tbody></table></div><h2 id="Benchmark-Datasets"><a href="#Benchmark-Datasets" class="headerlink" title="Benchmark Datasets"></a><a href="#content">Benchmark Datasets</a></h2><ol><li><p>[<strong>COPA</strong>] Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. AAAI,2011. [<a href="http://ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF" target="_blank" rel="noopener">paper</a> / <a href="http://people.ict.usc.edu/~gordon/copa.html" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Melissa Roemmele, Cosmin Adrian Bejan, Andrew S. Gordon</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p>[<strong>WSC</strong>] The Winograd Schema Challenge. AAAI,2011. [<a href="https://www.aaai.org/ocs/index.php/KR/KR12/paper/download/4492/4924" target="_blank" rel="noopener">paper</a> /<a href="https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Hector J. Levesque, Ernest Davis, Leora Morgenstern</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p>[<strong>ROCStories</strong>; <strong>SCT</strong>] A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories. NAACL,2016. [<a href="https://www.aclweb.org/anthology/N16-1098/" target="_blank" rel="noopener">paper</a> / <a href="https://www.cs.rochester.edu/nlp/rocstories/" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen</em></p><ul><li>Type: Cloze;</li></ul></li><li><p>[<strong>NarrativeQA</strong>] The NarrativeQA Reading Comprehension Challenge. TACL,2018. [<a href="https://arxiv.org/abs/1712.07040" target="_blank" rel="noopener">paper</a> / <a href="https://github.com/deepmind/narrativeqa" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, Edward Grefenstette</em></p><ul><li>Type: Generation;</li></ul></li><li><p>[<strong>SemEval-2018 Task 11</strong>] MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge. LERC,2018. [<a href="https://arxiv.org/abs/1803.05223" target="_blank" rel="noopener">paper</a> / <a href="http://www.sfb1102.uni-saarland.de/?page_id=2582" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Simon Ostermann, Ashutosh Modi, Michael Roth, Stefan Thater, Manfred Pinkal</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p>[<strong>story-commonsense</strong>] Modeling Naive Psychology of Characters in Simple Commonsense Stories. ACL,2018. [<a href="https://www.aclweb.org/anthology/P18-1213/" target="_blank" rel="noopener">paper</a> / <a href="http://uwnlp.github.io/storycommonsense" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Hannah Rashkin, Antoine Bosselut, Maarten Sap, Kevin Knight, Yejin Choi</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p><strong>Event2Mind</strong>: Commonsense Inference on Events, Intents, and Reactions. ACL,2018. [<a href="https://www.aclweb.org/anthology/P18-1043/" target="_blank" rel="noopener">paper</a> / <a href="https://uwnlp.github.io/event2mind/" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A. Smith, Yejin Choi</em></p><ul><li>Types: Generation;</li></ul></li><li><p><strong>ATOMIC</strong>: An Atlas of Machine Commonsense for If-Then Reasoning. AAAI,2019. [<a href="https://homes.cs.washington.edu/~msap/atomic/data/sap2019atomic.pdf" target="_blank" rel="noopener">paper</a> / <a href="https://homes.cs.washington.edu/~msap/atomic/" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Maarten Sap, Ronan LeBras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, Yejin Choi</em></p><ul><li>Types: Generation;</li></ul></li><li><p>[<strong>ARC</strong>] Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. 2018. [<a href="http://ai2-website.s3.amazonaws.com/publications/AI2ReasoningChallenge2018.pdf" target="_blank" rel="noopener">paper</a> / <a href="http://data.allenai.org/arc/" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p>[<strong>OpenBookQA</strong>] Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. EMNLP,2018. [<a href="https://www.aclweb.org/anthology/D18-1260/" target="_blank" rel="noopener">paper</a> / <a href="https://leaderboard.allenai.org/open_book_qa" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p><strong>ReCoRD</strong>: Bridging the Gap between Human and Machine Commonsense Reading Comprehension. 2018. [<a href="https://arxiv.org/abs/1810.12885" target="_blank" rel="noopener">paper</a> / <a href="https://sheng-z.github.io/ReCoRD-explorer/" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme</em></p><ul><li>Type: Cloze;</li></ul></li><li><p><strong>CommonsenseQA</strong>: A Question Answering Challenge Targeting Commonsense Knowledge. NAACL,2019. [<a href="https://www.aclweb.org/anthology/N19-1421/" target="_blank" rel="noopener">paper</a> / <a href="https://www.tau-nlp.org/commonsenseqa" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p><strong>ChID</strong>: A Large-scale Chinese IDiom Dataset for Cloze Test. ACL,2019. [<a href="https://www.aclweb.org/anthology/P19-1075" target="_blank" rel="noopener">paper</a> / <a href="https://github.com/chujiezheng/ChID-Dataset" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Chujie Zheng, Minlie Huang, Aixin Sun</em></p><ul><li>Type: Cloze;</li></ul></li><li><p>[<strong>sense-making</strong>] Does it Make Sense? And Why? A Pilot Study for Sense Making and Explanation. ACL,2019. [<a href="https://www.aclweb.org/anthology/P19-1393/" target="_blank" rel="noopener">paper</a> / <a href="https://github.com/wangcunxiang/SenMaking-and-Explanation" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, Tian Gao</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p><strong>HellaSwag</strong>: Can a Machine Really Finish Your Sentence? ACL,2019. [<a href="https://arxiv.org/abs/1905.07830" target="_blank" rel="noopener">paper</a> / <a href="https://rowanzellers.com/hellaswag/" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p><strong>SocialIQA</strong>: Commonsense Reasoning about Social Interactions. EMNLP,2019. [<a href="https://arxiv.org/abs/1904.09728" target="_blank" rel="noopener">paper</a> / <a href="https://maartensap.github.io/social-iqa/" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, Yejin Choi</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p>[<strong>ANLI</strong>] Abductive Commonsense Reasoning. 2019. [<a href="https://arxiv.org/abs/1908.05739" target="_blank" rel="noopener">paper</a> / <a href="https://leaderboard.allenai.org/anli/submissions/get-started" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Wen-tau Yih, Yejin Choi</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p><strong>Cosmos QA</strong>: Machine Reading Comprehension with Contextual Commonsense Reasoning. EMNLP,2019. [<a href="https://arxiv.org/abs/1909.00277" target="_blank" rel="noopener">paper</a> / <a href="https://wilburone.github.io/cosmos/" target="_blank" rel="noopener">data</a>]</p><p> Authors: <em>Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p><strong>CODAH</strong>: An Adversarially-Authored Question Answering Dataset for Common Sense. ACL,2019,workshop. [<a href="https://www.aclweb.org/anthology/W19-2008" target="_blank" rel="noopener">paper</a> / <a href="https://github.com/Websail-NU/CODAH" target="_blank" rel="noopener">data</a> ]</p><p> Authors: <em>Michael Chen, Mike D’Arcy, Alisa Liu, Jared Fernandez, Doug Downey</em></p><ul><li>Type: Multiple-Choice;</li></ul></li><li><p><strong>CommonGen</strong>: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning. 2019. [ <a href="http://arxiv.org/abs/1911.03705" target="_blank" rel="noopener">paper</a> / <a href="http://inklab.usc.edu/CommonGen/" target="_blank" rel="noopener">data</a> ]</p><p> Authors: <em>Bill Yuchen Lin, Ming Shen, Yu Xing, Pei Zhou, Xiang Ren</em></p><ul><li>Type: Generative;</li></ul></li><li><p><strong>QASC</strong>: A Dataset for Question Answering via Sentence Composition. 2019. [<a href="http://arxiv.org/abs/1910.11473" target="_blank" rel="noopener">paper</a> / <a href="https://leaderboard.allenai.org/qasc" target="_blank" rel="noopener">data</a> ]</p><p> Authors: <em>Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, Ashish Sabharwal</em></p></li><li><p><strong>DROP</strong>: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs. NAACL,2019. [<a href="http://arxiv.org/abs/1903.00161" target="_blank" rel="noopener">paper</a>]</p><p> Authros: <em>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner</em></p></li></ol><p>Note: <em>Only consider the benchmark datasets/tasks which require knowledge to complete.</em></p><h2 id="Other-Paper-List-About-MRC"><a href="#Other-Paper-List-About-MRC" class="headerlink" title="Other Paper List About MRC"></a>Other Paper List About MRC</h2><p><a href="https://github.com/thunlp/RCPapers" target="_blank" rel="noopener">thunlp/RCPapers</a><br><a href="https://github.com/XingLuxi/KMRC-Research-Archive" target="_blank" rel="noopener">xingluxi/KMRC-Research-Archive</a></p><!-- ## PhD Thesis on MRC1. Neural Reading Comprehension and Beyond    *Author*: Danqi Chen (Stanford University)    *Year*: 20181. 机器阅读理解的关键技术研究    *Author*: Bingning Wang (Institute of Automation, Chinese Academy of Sciences)    *Year*: 20181. 机器阅读理解与文本问答技术研究    *Author*: Minghao Hu (Graduate School of National University of Defense Technology)    *Year*: 2019 -->]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> paper </tag>
            
            <tag> mrc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EMNLP2019 | Incorporating External Knowledge into Machine Reading for Generative Question Answering</title>
      <link href="/2019/10/13/paper-emnlp2019-keag/"/>
      <url>/2019/10/13/paper-emnlp2019-keag/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: <a href="http://arxiv.org/abs/1909.02745" target="_blank" rel="noopener">Incorporating External Knowledge into Machine Reading for Generative Question Answering</a><br><em>Authors</em>: Bin Bi, Chen Wu, Ming Yan, Wei Wang, Jiangnan Xia, Chenliang Li<br><em>Org.</em>: Alibaba DAMO<br><em>Published</em>: EMNLP 2019<br><em>code</em>: NULL</p></blockquote><h2 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h2><p>与其他 Knowledge-aware QA 的工作不同，本文的研究重心是：在MRC中使用外部知识来<strong>生成自然语言语句答案</strong>。</p><ol><li><p>与抽取式阅读理解不同，生成式阅读理解所产生的自然语言形式的答案片段不一定在给定的 passage 中出现。</p></li><li><p>目前大多数工作，把给定的 passage 作为<strong>唯一的信息源</strong>，来生成答案。由于缺少一些常识知识或背景知识，例如词汇知识或关系型知识，无法正确理解某些片段或正确回答某些问题（尤其是 non-trivial question）。这些知识虽然未在文本中显式出现，但是却可以在 外部KB 中找到。</p></li></ol><p>基于上述，本文提出了一个 Knowledge-Enriched 答案生成模型（<strong>KEAG</strong>）:</p><ul><li>利用从<strong>4个信息源</strong>聚集得到的证据信息来生成答案，即在生成每个答案词时，从多个信息源中选定某一个作为生成答案词的源；<ul><li>4个信息源分别是：question，passage，vocabulary，knowledge (ConceptNet)</li><li>设计了一个 source selector 用于选择用于生成词的知识源；<ul><li>注：知识在答案的某些部分起着关键作用，而在一些情况下，文本上下文提供的信息应优先于上下文无关的通用知识；</li></ul></li></ul></li><li>KEAG 自适应地决定何时利用符号化知识，以及哪些知识是有用的。使模型能够利用没有显式地在文本中给出的、但是对回答问题有帮助的外部知识。</li><li>通过可微分的采样方法向模型中引入离散隐变量，并进行训练；</li></ul><p>使用的数据集是：MS-MARCO数据集中的 Q&amp;A + Natural Language Generation 任务。</p><h2 id="Model-Knowledge-Enriched-Answer-Generator"><a href="#Model-Knowledge-Enriched-Answer-Generator" class="headerlink" title="Model: Knowledge-Enriched Answer Generator"></a>Model: Knowledge-Enriched Answer Generator</h2><p>KEAG 的整体模型结构如下图所示: <img src="/../images/paper-emnlp2019-keag/model.png" alt="model"></p><p>模型的输入：</p><ul><li>question $q = {w_1^q,w_2^q,…,w_{N_q}^q}$ </li><li>passage $p = {w_1^p,w_2^p,…,w_{N_p}^p}$ </li><li>知识库 $K$, 由一系列三元组组成，每个三元组 $f = (subject, relation, object)$<br>模型的输出：</li><li>answer $r = {w_1^r, …, w_{N_r}^r}$</li></ul><p><strong>4个知识源</strong>：</p><ol><li>question $q$</li><li>passage $p$</li><li>global vocabulary $V$</li><li>knowledge $K$</li></ol><h3 id="1-Seq2Seq"><a href="#1-Seq2Seq" class="headerlink" title="1.Seq2Seq"></a>1.Seq2Seq</h3><p>本文的主体模型是基于 seq-to-seq attention 模型的扩展；</p><p>在编码端，question 和 passage 分别通过两个不同的 encoder 进行编码，每个编码器都是 BiLSTM ;</p><p>编码端的输出为 encoder 的隐藏层状态:</p><ul><li>question $E_q$</li><li>passage $E^p$</li></ul><p>在解码端，使用 unidirectional-LSTM，输出的隐藏层状态为 $s_t^r$;</p><p>分别计算 question 和 passage 的注意力分布：</p><ul><li>$a_t^q = \text(softmax)(g^{q\top} \text{tanh}(W^q E^q + U^qs_t^r + b^q))$</li><li>$a_t^p = \text(softmax)(g^{p\top} \text{tanh}(W^p E^p + U^p s_t^r + V^p c^q+ b^p))$</li><li>$g^q$, $W^q$, $U^q$, $b^q$, $g^p$, $W^p$, $U^p$, $b^p$ 是训练参数；</li><li>注意力分布可以认为是 source words 上的概率分布；</li></ul><p>为了避免生成重复，本文也采用了 coverage 机制：</p><ul><li>$c_t^q = \sum_i a_{ti}^q \cdot e_i^q$</li><li>$c_t^p = \sum_i a_{ti}^p \cdot e_i^p$</li><li>其中 $e_i^q$ 和 $e_i^p$ 分别是 question 和 passage 的 encoder 隐状态；</li></ul><p>计算得到的 上下文向量 $c_t^<em>$, 注意力分布 $a_t^</em>$ 以及 decoder 状态 $s_t^r$ 在后续步骤中用于决定 下个词 的生成来源；</p><h3 id="2-Source-Selector"><a href="#2-Source-Selector" class="headerlink" title="2.Source Selector"></a>2.Source Selector</h3><p>在答案生成过程中，在每个时刻，先使用 source selector 选择一个词源；</p><p>在 第 $t$ 时刻，为了决定在哪个源中进行待生成词概率的计算，引入一个离散隐变量 $y_t \in \{1,2,3,4\}$，作为源指示器：</p><ol><li>如果选择 question 作为 source，$y_t = 1$ ：<ul><li>待生成词将从下面的分布中选取：$a_t^q \in \mathbb{R}^{N_q}$</li><li>$P(w_{t+1}|y_t) = \sum_{i:w_i=w_{t+1}} a_{ti}^q$</li></ul></li><li>如果选择 passage 作为 source，$y_t = 2$ ：<ul><li>待生成词将从下面的分布中选取：$a_t^p \in \mathbb{R}^{N_p}$</li><li>$P(w_{t+1}|y_t) = \sum_{i:w_i=w_{t+1}} a_{ti}^p$</li></ul></li><li>如果选择 vocabulary 作为 source，$y_t = 3$ ：<ul><li>待生成词将从下面的分布中选取：$P_v(w|c_t^q, c_t^p, s_t^r) = \text{softmax}(W^v \cdot [c_t^q, c_t^p, s_t^r] + b^v)$</li><li>$P(w_{t+1}|y_t) = P_v(w|c_t^q, c_t^p, s_t^r)$</li></ul></li><li>如果选择 知识 作为 source，$y_t = 4$：<ul><li>需要使用 fact selector，在下一节中介绍；</li></ul></li></ol><h3 id="3-Knowledge-Integration"><a href="#3-Knowledge-Integration" class="headerlink" title="3.Knowledge Integration"></a>3.Knowledge Integration</h3><p>首先从KB中抽取相关的facts，然后从中选择最相关的fact用于生成答案</p><h4 id="3-1-Related-Fact-Extraction"><a href="#3-1-Related-Fact-Extraction" class="headerlink" title="3.1 Related Fact Extraction"></a>3.1 Related Fact Extraction</h4><p>抽取出与question和passage相关的候选facts集合，对于一个 $(q,p)$ 对，抽取出 facts 中的 subject 或是 object 出现在 question 或 passage 中的三元组，对三元组的打分规则如下：</p><ul><li>score+4 : subject 在 question 中出现，且 object 在 passage 中出现；</li><li>score+2 : subject 和 object 都在 passage 中出现；</li><li>score+1 : subject 在 question 或 passage 中出现；</li></ul><p>最终，选择分数排序 top - $N_f$ 个 facts 用于后续操作；</p><h4 id="3-2-Fact-Selection"><a href="#3-2-Fact-Selection" class="headerlink" title="3.2 Fact Selection"></a>3.2 Fact Selection</h4><p>Fact Selection 模块的结构如下图所示: <img src="/../images/paper-emnlp2019-keag/fact-selector.png" alt="module"></p><ol><li>Fact Embedding：<ul><li>每个 fact 的编码由 subject、relation、object的 Embedding 串联得到</li><li>subject、relation、object的 Embedding 由 Glove 进行初始化</li><li>$f = W^e \cdot [e^s, e^r, e^o] + b^e$</li><li>Facts 集合的表示：$F = \{f_1, f_2, …, f_{N_f}\}$</li></ul></li><li>选择 最相关的 Facts：<ul><li>引入 离散随机隐变量 $z_t \in [1, N_f]$ 用以指示哪条 fact 被选中；</li><li>$z_t$ 通过从 离散分布 $P(z_t | F, s_t^r)$ 中采样得到：<ul><li>$P(z_t | F, s_t^r) = \frac{1}{Z}\cdot \text{exp}(g^{f\top} \text{tanh}(W^f f_{z_t} + U^f s_t^r + b^f))$</li><li>$Z = \sum_{i=1}^{N_f} \text{exp}(g^{f\top} \text{tanh}(W^f f_{i} + U^f s_t^r + b^f))$</li></ul></li><li>在这里使用了 Gumbel-Softmax 技巧；</li><li>$z_t$ 表示 第 $z_t$ 个 fact 在 $t$ 时刻被 decoder 选中，当模型决定从知识源中生成词是，将 选中fact 中的 object 词作为将要生成的答案词；</li></ul></li></ol><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul><li><p>主实验：<img src="/../images/paper-emnlp2019-keag/exp-main.png" alt="exp-main"></p></li><li><p>和加知识模型的对比：<img src="/../images/paper-emnlp2019-keag/exp-main-with-kn.png" alt="exp-main-kn"></p></li><li><p>人工评测：<img src="/../images/paper-emnlp2019-keag/exp-human-eval.png" alt="exp-human"></p></li><li><p>去除实验：<img src="/../images/paper-emnlp2019-keag/exp-ablation.png" alt="exp-ab"></p></li><li><p>可视化例子：<img src="/../images/paper-emnlp2019-keag/exp-vis.png" alt="exp-vis"></p></li></ul><h2 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h2><ul><li>generate natural answers in QA<ul><li>在抽取式模型上面增加一个decoder，使用抽取得到的evidence用于答案生成<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du, Weifeng Lv, and Ming Zhou. 2018. S-net: From answer extraction to answer synthesis for machine reading comprehension. In AAAI.">[1]</span></a></sup>；<ul><li>缺点：依赖于抽取部分，需要抽取位置的标注</li></ul></li><li>使用 seq2seq 模型学习 question 和 passage 之间的对齐，产生丰富的 question 感知 passage 表示，直接用于生成答案<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rajarshee Mitra. 2017. An abstractive approach to question answering. CoRR, abs/1711.06238.">[2]</span></a></sup>；</li><li>natural answer generation 可以被定义为 query-focused 摘要<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Preksha Nema, Mitesh M. Khapra, Anirban Laha, and Balaraman Ravindran. 2017. Diversity driven attention model for query-based abstractive summarization. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1063–1072. Association for Computational Linguistics.">[3]</span></a></sup>；</li></ul></li><li>知识在 QA 任务中的作用：<ul><li>Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge. </li><li>Improving question answering by commonsense-based pre-training.</li><li>World knowledge for reading comprehension: Rare entity prediction with hierarchical lstms using external descriptions.</li></ul></li><li>incorporating knowledge into QA models without passage reading：<ul><li>Neural generative question answering.<ul><li>combines knowledge retrieval and seq2seq learning to produce ﬂuent answers, but it only deals with simple questions containing one single fact.</li></ul></li><li>Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning.<ul><li>extends it with a copy mechanism to learn to copy words from a given question</li></ul></li><li>Natural answer generation with heterogeneous memory.<ul><li>introduced a new attention mechanism that attends across the generated history and memory to explicitly avoid repetition, and incorporated knowledge to enrich generated answers.</li></ul></li></ul></li><li>knowledge-enhanced natural language (NLU) understanding：<ul><li>Dynamic integration of background knowledge in neural NLU systems.<ul><li>dynamically integrates background knowledge in a NLU model in the form of free-text statements, and yields reﬁned word representations to a task-speciﬁc NLU architecture that reprocesses the task inputs with these representations.</li></ul></li><li>Leveraging knowledge bases in LSTMs for improving machine reading.<ul><li>leverages continuous representations of knowledge bases to enhance the learning of recurrent neural networks for machine reading.</li></ul></li><li>Commonsense for generative multi-hop question answering tasks.<ul><li>a QA architecture that ﬁlls in the gaps of inference with commonsense knowledge. The model, however, does not allow an answer word to come directly from knowledge.</li></ul></li></ul></li></ul><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><ul><li>可视化实验中缺少了fact selector的结果；</li></ul><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du, Weifeng Lv, and Ming Zhou. 2018. S-net: From answer extraction to answer synthesis for machine reading comprehension. In AAAI.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rajarshee Mitra. 2017. An abstractive approach to question answering. CoRR, abs/1711.06238.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Preksha Nema, Mitesh M. Khapra, Anirban Laha, and Balaraman Ravindran. 2017. Diversity driven attention model for query-based abstractive summarization. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1063–1072. Association for Computational Linguistics.<a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> tg </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> ml </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EMNLP2019 | What&#39;s Missing? A Knowledge Gap Guided Approach for Multi-hop Question Answering</title>
      <link href="/2019/10/06/paper-emnlp2019-miss-fact/"/>
      <url>/2019/10/06/paper-emnlp2019-miss-fact/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: <a href="http://arxiv.org/abs/1909.09253" target="_blank" rel="noopener">What’s Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering</a><br><em>Authors</em>: Tushar Khot, Ashish Sabharwal, Peter Clark<br><em>Org.</em>: Allen Institute for Artiﬁcial Intelligence<br><em>Published</em>: EMNLP, 2019</p></blockquote><p>Code: <a href="https://github.com/allenai/missing-fact" target="_blank" rel="noopener">https://github.com/allenai/missing-fact</a></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>大多数的RC任务/数据集假设给待测试系统（或模型）提供了回答问题所需要的所有知识。<br>实际上，在处理<strong>multi-hop</strong>类型的问题时，模型只能接触到部分的知识，那么想要正确地回答数据集中的问题，就必须基于问题和提供的知识，抽取出额外的知识/事实。<br>正是这部分额外的知识的存在，造成了 <font color="red"><strong>knowledge gap</strong></font> 问题。</p><p>本文对knowledge gap问题展开研究，目标是确定这样的 gap，并且利用额外的知识源填充这个 knowledge gap。</p><p>本文主要研究的数据集是 OpenBookQA<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Can a suit of armor conduct electricity? A new dataset for open book question answering. 2018. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. In EMNLP.">[1]</span></a></sup>，可以看做是一类 <strong>提供了部分知识的QA任务</strong>。<br>OpenBookQA数据集的主要特点是：</p><ol><li>问题的领域是科学事实</li><li>回答问题时，不仅需要从一个给定的 <strong>small book</strong> 中确定相关的核心科学事实（core science fact），还需要从外部的知识源中抽取出额外的常识知识。<ol><li>这里的core fact就是数据集本身提供的partial knowledge</li><li>额外的外部知识 (broad external knowledge)，用以填充余下的 knowledge gap</li></ol></li></ol><p>OpenBookQA（OBQA） 中的 Knowledge Gap 示例：<img src="/../images/paper-emnlp2019-miss-fact/1-example.png" alt="example"></p><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><p>本文的主要工作：</p><ol><li>在 OBQA 的基础上对 Knowledge Gap 进行了分析和标注，并发布了带标注的数据集；</li><li>提出了一个 two-step 方法，即<strong>先确定再填充</strong>（Knowledge Gap）的方法解决 Multi-hop QA 任务；<ol><li>第一步，确定 knowledge gap：在 core fact 中预测一个 key span；<ul><li>这一步实际上是阅读理解中 span extraction 类型任务，本文中使用了 BIDAF 模型；</li></ul></li><li>第二步，填充 knowledge gap：确定 key span 和候选项之间的关系，然后回答问题；<ul><li>具体分为3个子步骤：(a) 从 ConceptNet/text corpora<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Think you have solved question answering? Try ARC, the AI2 reasoning challenge. 2018. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. CoRR, abs/1803.05457.">[2]</span></a></sup> 中抽取相关知识；(b) 预测 key span 和 候选答案之间的潜在关系；(c) 组合 core fact 和 抽取的外部知识来填充gap；</li><li>这一步中，采取multi-task学习，联合学习 gap 的关系预测 和 最终的答案预测；</li></ul></li></ol></li><li>提出了一个模型，同时学习使用抽取得到的 external knowledge，并与 partial knowledge 组合来填充 knowledge gap；</li></ol><h2 id="Knowledge-Gaps"><a href="#Knowledge-Gaps" class="headerlink" title="Knowledge Gaps"></a>Knowledge Gaps</h2><h3 id="Understanding-Gaps"><a href="#Understanding-Gaps" class="headerlink" title="Understanding Gaps"></a>Understanding Gaps</h3><p>针对 OBQA 任务，可以将 Knowledge Gap 分为三类（见下面的 Figure-2），这种分类也适用于其他的multi-hop QA数据集。</p><ul><li><img src="/../images/paper-emnlp2019-miss-fact/2-gap-def.png" alt="gap-class-all"></li><li>注：OBQA中的核心组成部分：数据集中显示给出的1、Question；2、Answer Choices；3、Fact；以及未在数据集中出现但是对回答问题重要的4、external knowledge；</li></ul><p><font color="cyan"><strong>Question-to-Fact</strong></font> : 存在于问题中的概念（concept）和 core fact 之间；</p><ul><li>例如: <img src="/../images/paper-emnlp2019-miss-fact/gap-q2f.png" alt="gap-1"></li></ul><p><font color="magenta"><strong>Fact-to-Answer</strong></font> : Gap 捕捉的是 core fact 中的概念和候选答案之间的关系；</p><ul><li>例如: <img src="/../images/paper-emnlp2019-miss-fact/gap-a2f.png" alt="gap-2"></li><li>fact 中的概念可能和不正确的选项之间也存在关系</li></ul><p><font color="green"><strong>Question-to-Answer (Fact)</strong></font> : 基于 core fact，问题中的概念和候选答案需要额外的知识（gap）来建立联系；</p><ul><li>例如: <img src="/../images/paper-emnlp2019-miss-fact/gap-q2a.png" alt="gap-3"></li></ul><p>根据抽样统计，在OBQA中 Q2F（44%） 和 <strong>F2A（86%）</strong> 这两种Gap比较常见，Q2A（&lt;20%） 类型比较罕见，一些问题可能存在多种gaps。</p><p>本文只关注 <strong>Fact-to-Answer</strong> gap，并且假设所需的 core fact 已经给定。</p><h3 id="Knowledge-Gap-Dataset-KGD"><a href="#Knowledge-Gap-Dataset-KGD" class="headerlink" title="Knowledge Gap Dataset: KGD"></a>Knowledge Gap Dataset: KGD</h3><p>（文章中介绍了采取众包对数据集中每个样例的knowledge gap进行标注的细节，具体细节请参考文章的 Section 3.2 以及 Appendix A。这里直接介绍标注得到的数据集KGD）</p><p>在KGD中，包含了 Key Span 和 relation label 的标注，用以表示knowledge gap。</p><ul><li>span 是 fact 的子串，relations 是 span 和正确答案之间的关系集合</li><li>KGD示例：<img src="/../images/paper-emnlp2019-miss-fact/example-kgd.png" alt="kgd"></li><li>KGD数据统计：<img src="/../images/paper-emnlp2019-miss-fact/kgd-stats.png" alt="kgd-s"></li></ul><h2 id="Knowledge-Gap-Guided-QA-GapQA"><a href="#Knowledge-Gap-Guided-QA-GapQA" class="headerlink" title="Knowledge-Gap Guided QA: GapQA"></a>Knowledge-Gap Guided QA: GapQA</h2><p>模型介绍中涉及的符合及其含义：</p><ul><li>question $q$；fact $f$；selected span $s$；set of relations between span and correct choice $r$；answer choices $c$；question stem $q_s$<ul><li>其中：$q=q_s c$</li></ul></li><li>tokens in question stem $q_m$；tokens in fact $f_m$；</li><li>predicted span $\hat{s}$；predicted relations $\hat{r}$</li></ul><h3 id="1-Key-Span-Identification-Model"><a href="#1-Key-Span-Identification-Model" class="headerlink" title="1.Key Span Identification Model"></a>1.Key Span Identification Model</h3><p>模型采用 BIDAF；<br>训练时，模型的输入为：fact（对应于passage），question stem（对应于question），模型的输出是 key span 的起止位置；</p><h3 id="2-Knowledge-Retrieval-Module"><a href="#2-Knowledge-Retrieval-Module" class="headerlink" title="2.Knowledge Retrieval Module"></a>2.Knowledge Retrieval Module</h3><p><strong>给定一个预测的span</strong>，从两个知识源中抽取知识，用 $K$ 表示所有抽取到的知识：</p><ol><li>ConceptNet 中的三元组，三元组中的relation与gap需要的relation密切相关；</li><li>ARC corpus 中的句子，ARC corpus 包含 14M 科学相关的句子，用来补充 CN，提高recall；</li></ol><p><strong>Tuple Search</strong>：</p><ol><li>选择tuple时，要求subject中至少有一个token 与 $\hat{s}$ 匹配，object中至少有一个词与 $c_i$ 匹配（反之亦然）；</li><li>通过 Jaccard score 对于tuples进行打分，为每个候选选项 $c_i$ 选择出 top-k 个 tuples（在实验中，k=5）<ul><li>$\text{Jaccard score}(t) = \text{jacc}(\text{tokens}(t), \text{tokens}(\hat{s} + c_i))$</li><li>$\text{jacc}(w_1, w_2) = \frac{w_1 \cap w_2}{w_1 \cup w_2}$</li></ul></li><li>为了知识形式的一致性，将三元组也转化为句子的形式；</li></ol><p><strong>Text Search</strong>：</p><ol><li>使用 ElasticSearch 进行相关句子的检索，查询为：$\hat{s} + c_i$：</li><li>为每个候选选项 $c_i$ 选择出 top-5 个句子</li></ol><h3 id="3-Question-Answering-Module"><a href="#3-Question-Answering-Module" class="headerlink" title="3.Question Answering Module"></a>3.Question Answering Module</h3><p>模型结构图: <img src="/../images/paper-emnlp2019-miss-fact/model.png" alt="kgg-qa-module"></p><p>本模块的输入为：$q_s$, $c$, $f$, $\hat{s}$, $K$；</p><p>计算过程如下：</p><p><strong>Step 1. Embedding</strong>: 使用 300维 的glove 词向量表示输入中的每个词</p><p><strong>Step 2. Encoding</strong>: 使用 BiLSTM 计算上下文编码，隐藏层维度为100，用 $\mathcal{E}_*$ 表示 BiLSTM 的输出</p><p><strong>Step 3. Fact Relevance</strong>:</p><p>出发点：一个相关 fact 可以捕捉到与问题和正确答案对齐的概念间的关系；</p><ol><li>计算 question-weighted 和 answer-weighted fact表示；<ul><li>question-weighted fact repr.<ul><li>$\mathcal{A}_{q_s,f} = \mathcal{E}_{q_s} \cdot \mathcal{E}_f \in \mathbb{R}^{q_m \times f_m}$</li><li>$\mathcal{V}_{q_s}(f) = \text{softmax}_{f_m}(\text{max}_{q_m} \mathcal{A}_{q_s,f}) \in \mathbb{R}^{1 \times f_m} $</li><li>$\mathcal{S}_{q_s}(f) = \mathcal{V}_{q_s}(f) \cdot \mathcal{E}_f \in \mathbb{R}^{1 \times h}$</li></ul></li><li>choice-weighted fact repr. $\mathcal{S}_{c_i}(f)$<ul><li>同样的计算方式，将 $q_s$ 的表示替换为 $c_i$</li></ul></li></ul></li><li>组合计算的 fact-based 表示，确定 fact 对 choice 的支持程度；<ul><li>组合两种 fact 表示：$\mathcal{S}_{q_s c_i}(f) = (\mathcal{S}_{q_s}(f) + \mathcal{S}_{c_i}(f))/2$</li><li>计算候选答案的打分:<ul><li>$\text{score}_f(c_i) = \text{FF}(\bigotimes( \mathcal{S}_{q_s c_i}(f) , \text{avg} \mathcal{E}_f))$</li><li>$\bigotimes(x,y) = [x-y; x \ast y] \in \mathbb{R}^{1 \times 2h}$</li><li>$\text{FF}$ 是 前馈神经网络，输出一个标量值；</li></ul></li></ul></li></ol><p><strong>Step 4. Filling the Gap</strong>:</p><p>实现上是预测 $\hat{s}$ 和 $c_i$ 之间的关系，并将关系和fact进行组合，为choice进行打分</p><ol><li>计算 span weighted 和 choice weighted KB知识句表示 ($\in \mathbb{R}^{1 \times h}$)，反映知识句中的词分别和 span 以及 choice 的相关程度<ul><li>$\mathcal{S}_{\hat{s}}(k_j) = \mathcal{V}_{\hat{s}}(k_j) \cdot \mathcal{E}_{k_j}$</li><li>$\mathcal{S}_{c_i}(k_j) = \mathcal{V}_{c_i}(k_j) \cdot \mathcal{E}_{k_j}$</li></ul></li><li>kb-based relation repr.：<ul><li>$\mathcal{R}_j (\hat{s}, c_i) = \text{FF}(\bigotimes(\mathcal{S}_{\hat{s}}(k_j), \mathcal{S}_{c_i}(k_j)) ) \in \mathbb{R}^{1 \times h}$</li></ul></li><li>聚集所有KB知识句的relation repr.:<ul><li>$\mathcal{R}(\hat{s}, c_i) = \text{avg}_j \mathcal{R}_j (\hat{s}, c_i)$</li></ul></li></ol><p><strong>Step 5. Relation Prediction Score</strong>:</p><ol><li>基于问题，确定/预测与fact信息相符合的relation信息 (通过聚集问题和fact的表示来捕获这个信息)<ul><li>$\mathcal{D}(q_s, f) = \bigotimes(\text{max}_{q_m} \mathcal{E}_{q_s}, \text{max}_{f_m} \mathcal{E}_{f}) \in \mathbb{R}^{1 \times 2h}$</li></ul></li><li>基于fact信息和relation表示，对每个候选项进行打分<ul><li>$\text{score}_r(c_i) = \text{FF}([\mathcal{D}(q_s, f); \mathcal{R}(\hat{s}, c_i)])$</li></ul></li></ol><p><strong>Step 6. Final score for each choice</strong>: $\text{score}(c_i) = \text{score}_f(c_i) + \text{score}_r(c_i)$</p><h3 id="4-Model-Training"><a href="#4-Model-Training" class="headerlink" title="4. Model Training"></a>4. Model Training</h3><p>GapQA 中 Question Answering Module 的总训练 loss 由两部分组成：</p><ol><li>模型预测的答案和正确的答案选项之间的交叉熵损失</li><li>模型预测的relation（span 和 候选项之间）与正确relation之间的损失：<ul><li>为了计算这部分的损失，使 $\mathcal{R}(s,c_i)$ 通过一层的ffn，映射为向量 $\hat{r}_i \in \mathbb{R}^{1\times l}$，$l$ 是 relation 的个数；</li><li>由于$(s,c_i)$对之间可能存在多个有效的relation，创建一个 n-hot 向量表示 $\bar{r} \in \mathbb{R}^{1\times l}$，如果 relation $r_k$ 是有效的，则令 $\bar{r}[k] = 1$；</li><li>对于正确的选项（即问题的目标答案），使用 binary cross-entropy 计算预测的 $\hat{r}_i$ 和 $r$ 之间的损失；</li><li>对于不正确的选项，无法确定 span 和选项之间的relation关系（不存在标注信息），但是针对正确答案选择出的 relations set 不应对错误选项有效。故此，使用 binary cross-entropy 计算预测的 $\hat{r}_i$ 和 $1-r$ 之间的损失，并且不考虑未被选中的 relation，即使用 masked binary cross-entropy；</li><li>（这里选中的 relations 指的是：KGD中对 key span 和 correct choice 之间标注的relation集合）</li></ul></li></ol><p>最终，总体的 $loss = ce(\hat{c}, \bar{c}) + \lambda \cdot (bce(\hat{r}_i, \bar{r}) + \sum_{j\neq i} mbec(\hat{r}_j, 1-\bar{r}, \bar{r}) )$</p><p>为了更清晰的理解模型的整体计算流程，附录中给出了一个可视化的示例：<img src="/../images/paper-emnlp2019-miss-fact/exp-vis.png" alt="eg-vis"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ol><li>Key Span Identification 效果：<img src="/../images/paper-emnlp2019-miss-fact/exp-span.png" alt="exp-span"></li><li>在 OBQA 数据集上的结果：<img src="/../images/paper-emnlp2019-miss-fact/exp-main.png" alt="exp-main"></li><li>输入知识源的影响：<img src="/../images/paper-emnlp2019-miss-fact/exp-k.png" alt="exp-k"></li><li>模型去除实验：<img src="/../images/paper-emnlp2019-miss-fact/exp-ablation.png" alt="exp-a"></li><li>错例分析：<img src="/../images/paper-emnlp2019-miss-fact/exp-error-case.png" alt="exp-error"></li></ol><p>主要存在三种错误类型：<br>(1) Incorrect predicted spans (25%)<br>(2) Incorrect relation scores (55%)<br>(3) Out-of-Scope Gap relations (20%)</p><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><ul><li><p>这篇文章，对于 OBQA 类 partial knowledge multi-hop QA 的任务，引入了一个新颖的问题定义，旨在刻画QA任务中知识缺失的本质问题，并对这个问题进行了细致化的分类。不仅给出了可引入监督信息的标注过程和相应的数据集，同时也提出了有说服力的模型；</p></li><li><p>本文中只对最常见的 Fact-to-Answer Gap 问题进行了研究，针对其余两种 knowledge gap 的解决方案同样值得后续的研究；</p><ul><li>针对MCQ类任务，才会存在涉及到 Answer 的 Gap，那么如果在任务中未给出候选答案的情况下，</li></ul></li><li><p>从解决方法上看，先确定在填充整体上为pipline的形式，在实验部分没有看到使用标注的gold span进行GapQA计算的结果？</p></li></ul><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Can a suit of armor conduct electricity? A new dataset for open book question answering. 2018. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. In EMNLP.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Think you have solved question answering? Try ARC, the AI2 reasoning challenge. 2018. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. CoRR, abs/1803.05457.<a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> dataset </tag>
            
            <tag> multi-hop </tag>
            
            <tag> qa </tag>
            
            <tag> multiple-choice </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 | Exploring ways to incorporate additional knowledge to improve Natural Language Commonsense Question Answering</title>
      <link href="/2019/09/26/paper-kn-mcqa-1909-08855/"/>
      <url>/2019/09/26/paper-kn-mcqa-1909-08855/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: <a href="https://arxiv.org/abs/1909.08855" target="_blank" rel="noopener">Exploring ways to incorporate additional knowledge to improve Natural Language Commonsense Question Answering</a><br><em>Authors</em>: Arindam Mitra, Pratyay Banerjee, Kuntal Kumar Pal, Swaroop Mishra, Chitta Baral<br><em>Org.</em>: Arizona State University<br><em>Published</em>: NULL, 2019</p></blockquote><p>Code: NULL </p><h2 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h2><p>Research Background:</p><ul><li>DARPA 和 Allen AI 提出了一系列数据集来促进 需要 commonsense knowledge (CS-KN) 问答任务的研究；<ul><li>额外的挑战：常识知识通常不容易以文本形式获取；</li></ul></li><li>近期，BERT/GPT 等预训练语言模型在多个 Multiple Choice Question-Answering (MCQ) 数据集上展示出了相当好的效果；<ul><li>已经超越了一些针对特定任务精心设计的模型，甚至可以超过人类水平（i.e., SWAG）；</li><li>成为了大多数新提出的数据集 baseline；</li><li>在需要超越QA对本身知识的question上也表现良好；<ul><li>一些知识可能借由文本的形式，通过在巨大规模的text语料上训练时，封装在了LM中，由此引出的问题：<ul><li>如果进一步给 LM 灌注 the needed KN 是否能带来性能的提升？</li><li>进行 KN infusion 的方法是什么？</li></ul></li></ul></li></ul></li></ul><p>这篇文章的目标是：<strong>设计将外部（常识）知识结合到 LM-based 模型中的方法</strong>，以提高QA模型在<strong>MCQ</strong>任务上的性能。</p><p>本文的具体工作：</p><ul><li>确定外部知识源，展示通过 IR 技术抽取出的 fact 集合，在训练和测试阶段，会为性能带来提升；</li><li>进一步探索如果 用不同的方式提供 task-specific 知识 或是 用不同的策略使用可用的知识，能否进一步带来提升；具体介绍了：<ul><li>3种传递知识 (pass knowledge) 的策略；<ul><li>1\ <strong>revision</strong>: 在KB中拥有与数据集相关的 knowledge statements 上微调LM；</li><li>2\ <strong>open-book</strong>: 为 Q-A 对从KB中选择一定数量的 knowledge statements (textually similar to Q-A pair) 用来回答问题；</li><li>3\ <strong>mixture</strong> (<strong>revision &amp; open-book</strong>): 先进行 revision，在进行 open-book；</li></ul></li><li>5种利用知识 (use knowledge) 的模型；<ul><li><strong>concat</strong> / <strong>max</strong> / <strong>simple sum</strong> / <strong>weighted sum</strong> / <strong>mac</strong> ;</li></ul></li></ul></li><li>提出了一个新的框架，用于处理回答 MCQ 任务中 question 所需要的信息分散在不同的知识句 (knowledge sentences) 中的情况；</li></ul><h2 id="MCQ-Datasets-amp-Knowledge"><a href="#MCQ-Datasets-amp-Knowledge" class="headerlink" title="MCQ Datasets &amp; Knowledge"></a>MCQ Datasets &amp; Knowledge</h2><p>(为了研究如何引入知识，首先要选出需要外部知识的一些数据集作为测试系统的QA任务)</p><p>MCQ Tasks:</p><ol><li>Abductive Natural Language Inference<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Abductive commonsense reasoning. 2019. Bhagavatula, C.; Bras, R. L.; Malaviya, C.; Sakaguchi, K.; Holtzman, A.; Rashkin, H.; Downey, D.; Yih, S. W.-t.; and Choi, Y.. arXiv preprint arXiv:1908.05739.">[1]</span></a></sup></li><li>Physical Interaction QA<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://leaderboard.allenai.org/physicaliqa">[2]</span></a></sup></li><li>Social Interaction QA<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Socialiqa: Commonsense reasoning about social interactions. 2019. Sap, M.; Rashkin, H.; Chen, D.; LeBras, R.; and Choi, Y. arXiv preprint arXiv:1904.09728.">[3]</span></a></sup></li><li>Parent and Family QA<ul><li>这个数据集是本文为了对知识有更好的可控性而合成的数据集；</li><li>一方面来测试NLM的记忆能力，另一方面测试将分散在多个句子中的知识结合起来的能力；</li><li>数据集的来源是DBPedia，抽取其中的亲子关系，构造成多选型数据集；</li></ul></li></ol><p>Knowledge Sources (different KBs for different tasks):</p><ol><li>SCT and RCOStories Corpora<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="A corpus and evaluation framework for deeper understanding of commonsense stories. 2016. Mostafazadeh, N.; Chambers, N.; He, X.; Parikh, D.; Batra, D.; Vanderwende, L.; Kohli, P.; and Allen, J.arXiv preprint arXiv:1604.01696.">[4]</span></a></sup></li><li>Wikihow<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wikihow: A large scale text summarization dataset. 2018. Koupaee, M., and Wang, W. Y. arXiv preprint arXiv:1810.09305.">[5]</span></a></sup></li><li>ATOMIC<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Atomic: an atlas of machine commonsense for if-then reasoning. 2019. Sap, M.; Le Bras, R.; Allaway, E.; Bhagavatula, C.; Lourie, N.; Rashkin, H.; Roof, B.; Smith, N. A.; and Choi, Y.  In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, 3027–3035.">[6]</span></a></sup></li><li>DBPedia</li></ol><p>Relevant Knowledge Extraction:</p><ul><li>使用IR和Re-ranking等方法抽取出相关的 knowledge paragraph</li><li>这里使用的方法参考自ACL 2019的一篇文章<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Careful selection of knowledge to solve open book question answering. 2019. Banerjee, P.; Pal, K. K.; Mitra, A.; and Baral, C. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 6120–6129. Florence, Italy: Association for Computational Linguistics.">[7]</span></a></sup><ul><li>先使用IR模型，然后通过计算 Information Gain 进行 re-ranking</li><li>query 通过简单的 heuristic 方法产生（即，question/answer option/context中出现的非停用词）</li><li>对于每个数据集，选择 top-10 的知识句子。</li></ul></li></ul><p>示例：每个数据集及其从各自KB中抽取出的知识：<br><img src="/../images/paper-kn-mcqa-1909-08855/dataset-with-kn.png" alt="example"></p><h2 id="Modes-of-Knowledge-Infusion"><a href="#Modes-of-Knowledge-Infusion" class="headerlink" title="Modes of Knowledge Infusion"></a>Modes of Knowledge Infusion</h2><p>本文中使用的预训练的 BERT-uncased-whole-word-masked model 作为base模型。</p><p>本节介绍的5种知识结合方法是在open-book策略下进行的。</p><p>模型的输入：</p><ol><li>question $Q$;</li><li>$n$ 个候选答案 $a_1, …, a_n$;</li><li>$n$ 个 premises（每个候选答案对应一个premise）<ol><li>每个 premise包含 $m$ 个 knowledge passages</li><li>使用 $k_{ij}$ 表示 第 $i-th$ 答案选项的第 $j-th$ knowledge passage</li></ol></li></ol><p>模型的输出：为每个候选答案计算一个分数 $score(i)$ , 最终选择分数最高的作为最终的答案</p><h3 id="1-Concat"><a href="#1-Concat" class="headerlink" title="1.Concat"></a>1.Concat</h3><ul><li>将每个选项的所有的（$m$ 个）knowledge passage 连接起来，形成一个 knowledge passage $k_i$ ；</li><li>BERT 的输入为: <code>[CLS] K_i [SEP] Q a_i [SEP]</code> ；</li><li>选择 BERT 最后一层的 <code>[CLS]</code> 的表示；</li><li>计算所有选项，得到 $n$ 个 <code>[CLS]</code> 表示，再经过一个 linear layer 计算得到 $score(i)$ ；</li></ul><h3 id="2-Parallel-Max"><a href="#2-Parallel-Max" class="headerlink" title="2.Parallel-Max"></a>2.Parallel-Max</h3><ul><li>BERT 的输入为: <code>[CLS] K_ij [SEP] Q a_i [SEP]</code> ；</li><li>每条知识和QA对分别编码，对每个QA对中涉及的m条知识，计算m个打分选择最大值作为选项的打分</li></ul><h3 id="3-Simple-Sum"><a href="#3-Simple-Sum" class="headerlink" title="3.Simple Sum"></a>3.Simple Sum</h3><ul><li><strong>simple/weighted sum / MAC 假设信息分散在多个句子中，需要聚集这些分散的信息</strong>；</li><li>BERT 的输入为: <code>[CLS] K_ij [SEP] Q a_i [SEP]</code> ；</li><li>将 $m$ 个向量相加，得到最终的 summary vector；</li><li>计算所有选项，得到 $n$ 个 summary vector，再经过一个 linear layer 计算得到 $score(i)$ ；</li></ul><h3 id="4-Weighted-Sum"><a href="#4-Weighted-Sum" class="headerlink" title="4.Weighted Sum"></a>4.Weighted Sum</h3><ul><li>与simple sum的区别：一些knowledge passage可能更有用;</li><li>为一个候选项对应的 $m$ 个 <code>[CLS]</code> 表示计算一个标量权重 $w_{ij}$ （weight layer）, 然后计算 $m$ 个 <code>[CLS]</code> 表示的权重和;</li><li>最后通过一个线性映射和 复用 weight layer 来计算 候选项$a_i$的 $score(i)$;</li></ul><h3 id="5-MAC"><a href="#5-MAC" class="headerlink" title="5.MAC"></a>5.MAC</h3><p>MAC = Multi-sentence Alighment Classification</p><ul><li>第一步同 weighted sum，先为一个候选项对应的 $m$ 个 <code>[CLS]</code> 表示计算一个标量权重 $w_{ij}$</li><li>reduce the normalized score<ul><li>$ w_{ij}^{\prime} = w_{ij} - ( 1 - w_{ij}) \ast max_{j\neq l \wedge l \in \{1…m\}} \{LinkStrength_{ijl}\} $</li><li>$LinkStrength_{ijl} \in [0,1]$ 表示两个 knowledge passage 之间的连接强度<ul><li>如果两个知识（即 knowledge passage）有更好的 link strength，那么他们可以联合起来推出新的信息</li><li>例如：<ul><li><code>Facebook was launched in Cambridge</code> 和 <code>Cambridge is in MA</code> 的 link strength 更高，可以推出 <code>Facebook was launched in MA</code></li><li><code>Facebook was launched in Cambridge</code> 和 <code>Boston is in MA</code> 的 link strength 较低</li></ul></li></ul></li><li>weight reduction 的含义是：如果一个知识/knowledge passage，不足够和其他知识进行联合，那么它可能在最终的predicttion阶段也没必要被考虑</li></ul></li><li>link strength 的计算方式可以有很多形式，本文中介绍了一种 memory-efficient 的方式<ul><li>从BERT输出的表示中抽取出 knowledge passage 对应的 token 表示：$h_{i,j}^1, …, h_{i,j}^p$</li><li><script type="math/tex; mode=display">LinkStrength_{ijl} = \frac{exp(link_{ij}^T link_{il})}{\sum_{x\neq j}^{x=1...m} exp(link_{ij}^T link_{ix})}</script></li><li>$link_{ij} = \sum_{t=1}^p s_{ij}^t \ast h_{ij}^t$<ul><li>$s_{ij}$ 是标量，由 $h_{ij}$ 通过一个线性层得到，表示 $h_{ij}$ 是否可以被纳入 link description $link_{ij}$</li></ul></li></ul></li></ul><h2 id="Strategies-of-Knowledge-Infusion"><a href="#Strategies-of-Knowledge-Infusion" class="headerlink" title="Strategies of Knowledge Infusion"></a>Strategies of Knowledge Infusion</h2><p>Notation:</p><ul><li>$D$ 表示 MCQ 数据集；$T$ 表示 pre-trained LM；$K_D$ 表示知识库，即抽取出来的 knowledge passages/sentences；K 表示 general knowledge base，即用来预训练 $T$ 的语料；</li><li>$K$ 不一定包含 $K_D$</li></ul><ol><li><strong>Revision：在</strong>$K_D$上使用与MLM和NSP任务微调$T$ ，然后再在 $D$ 上进行微调</li><li><strong>Open-Book</strong>：$D$ 中的每个训练样例都有各自的 $K_D$，在扩充了知识的 $D$ 上微调 $T$</li><li><strong>Revision along with Open-Book</strong>：先进行 Revision，再进行 Open-book</li></ol><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ol><li>三种策略及五种知识融合模型在四个数据集上的效果：<img src="/../images/paper-kn-mcqa-1909-08855/exp-1.png" alt="exp-1"></li><li>知识模型融合性能最好的再测试集上的效果：<img src="/../images/paper-kn-mcqa-1909-08855/exp-2.png" alt="exp-2"></li><li>对比实验：<img src="/../images/paper-kn-mcqa-1909-08855/exp-other.png" alt="exp-other"><ol><li>对于预测正确的情况，分析 (1) Exact appropriate knowledge is present, (2) A related but relevant knowledge is present, (3) Knowledge is present only in the correct option, and (4) No knowledge is present.</li><li>对于预测错误的情况，分析 (1) Is the knowledge insufﬁcient, (2) Is the knowledge present in the wrong answer, (3) Knowledge is appropriate but model fails, and (4) Gold label is questionable.</li></ol></li></ol><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><ul><li><p>遗留问题：5种知识结合方法的计算中，标量权重值通过什么NN得到？标量的范围是否有限制？比如，需要在$[0,1]$之间？</p></li><li><p>这篇文章较为全面的总结了向LM中引入知识的策略以及计算表示的方法。</p></li><li>方法比较启发性但是很有效，在三个commonsense knowledge的数据集上都取得了SOTA的结果。</li><li>虽然已有的KB无法包含所有回答问题所需要的knowledge，但是KB确实能够提供重要的knowledge。</li><li>BERT虽然已经具备了一些knowledge，但是仍然存在 <strong>给定knowledge但是模型无法回答的情况</strong>，或是<strong>使用了 irrelevant knowledge 产生了错误预测的情况</strong>，这些都是值得进一步研究的问题。</li></ul><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Abductive commonsense reasoning. 2019. Bhagavatula, C.; Bras, R. L.; Malaviya, C.; Sakaguchi, K.; Holtzman, A.; Rashkin, H.; Downey, D.; Yih, S. W.-t.; and Choi, Y.. arXiv preprint arXiv:1908.05739.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://leaderboard.allenai.org/physicaliqa<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Socialiqa: Commonsense reasoning about social interactions. 2019. Sap, M.; Rashkin, H.; Chen, D.; LeBras, R.; and Choi, Y. arXiv preprint arXiv:1904.09728.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">A corpus and evaluation framework for deeper understanding of commonsense stories. 2016. Mostafazadeh, N.; Chambers, N.; He, X.; Parikh, D.; Batra, D.; Vanderwende, L.; Kohli, P.; and Allen, J.arXiv preprint arXiv:1604.01696.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wikihow: A large scale text summarization dataset. 2018. Koupaee, M., and Wang, W. Y. arXiv preprint arXiv:1810.09305.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Atomic: an atlas of machine commonsense for if-then reasoning. 2019. Sap, M.; Le Bras, R.; Allaway, E.; Bhagavatula, C.; Lourie, N.; Rashkin, H.; Roof, B.; Smith, N. A.; and Choi, Y.  In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, 3027–3035.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Careful selection of knowledge to solve open book question answering. 2019. Banerjee, P.; Pal, K. K.; Mitra, A.; and Baral, C. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 6120–6129. Florence, Italy: Association for Computational Linguistics.<a href="#fnref:7" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> qa </tag>
            
            <tag> multiple-choice </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACL2019 | Does It Make Sense? And Why? A Pilot Study for Sense Making and Explanation</title>
      <link href="/2019/09/22/paper-acl2019-sense-make/"/>
      <url>/2019/09/22/paper-acl2019-sense-make/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: <a href="https://www.aclweb.org/anthology/P19-1393" target="_blank" rel="noopener">Does It Make Sense? And Why? A Pilot Study for Sense Making and Explanation</a><br><em>Authors</em>: Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, Tian Gao<br><em>Org.</em>: Westlake University<br><em>Published</em>: ACL 2019</p></blockquote><p>Code: <a href="https://github.com/wangcunxiang/Sen-Making-and-Explanation" target="_blank" rel="noopener">https://github.com/wangcunxiang/Sen-Making-and-Explanation</a></p><h2 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h2><p>虽然将常识信息引入到自然语言理解系统中受到了越来越多的关注。<br>但是如何评价系统是否具有 <strong>sense making</strong> 能力，仍是个基础问题。并且现有的 benchmarks 是间接地评估常识知识，而缺乏任何解释。<br>针对这一问题，本文发布了一个 benchmark ，以<strong>直接地</strong>测试系统是否可以区分有意义的和不合理的<strong>自然语言 statement</strong>。</p><p>与人类相比，大多数 End-to-End 训练的模型在 common sense 上的表现很弱。<br>例如：人类可以很直观的理解 <code>someone can put a turkey into a fridge but he can never put an elephant into a fridge.</code>，但是机器很难做出区分（non-trivial）</p><p>此外，常识推理应该在NLU系统中占据中心位置。</p><p>因此，评测模型在 sense making 上的能力是很重要的一件事。</p><p>现有数据集的问题：</p><ul><li>没有给出直接的评价指标来量化理解的能力；</li><li>没有给出理解过程中需要的显示、关键因素；</li></ul><p>本文提出的数据集示例：<br><img src="/../images/paper-acl2019-sense-make/example.png" alt="example"></p><p>本文提出的 benchmark 包含两个子任务：每个样例由5个句子组成 $\{s_1, s_2, r_1, r_2, r_3\}$</p><ul><li>subtask1 - <strong>Sen-Making</strong>: choose from two natural language statements ($s_1$, $s_2$) with similar wordings which one makes sense and which one does not make sense；</li><li>subtask2 - <strong>Explanation</strong>: find the key reason (choose from $\{r_1, r_2, r_3\}$) why a given statement does <strong>not</strong> make sense；</li></ul><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><p>本文的工作将常识理解任务分解为了两个更具体的子任务来评价模型的常识理解能力。有点在复杂任务过程中打断点的感觉。</p>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> dataset </tag>
            
            <tag> reasoning </tag>
            
            <tag> commonsense </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 | Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering</title>
      <link href="/2019/09/17/paper-csqa-1909-05311/"/>
      <url>/2019/09/17/paper-csqa-1909-05311/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: <a href="https://arxiv.org/abs/1909.05311" target="_blank" rel="noopener">Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering</a><br><em>Author</em>: Shangwen Lv, Daya Guo , Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Songlin Hu<br><em>Org.</em>: CAS, Sun Yat-sen University, PKU, Microsoft<br><em>Published</em>: AAAI,2020<br><em>Code</em>: NULL</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>常识问答任务(Commonsense Question Answering, CSQA) 要求回答需要背景知识的，且背景知识未在问题中提及的一些问题</p><p>CSQA 的主要挑战是：<font color="blue"><strong>如何从外部的知识中抽取证据信息，并给予证据做出预测</strong></font>。</p><p>近期的工作主要集中在两个方面：</p><ol><li>根据带有人工标注出证据的数据集，学习生成证据 <sup><a href="#fn_cose" id="reffn_cose">cose</a></sup>.<ul><li>问题是：标注代价昂贵</li></ul></li><li>仅从结构化知识库或是非结构化知识库，即同构知识源，中抽取证据信息，并没有<strong>同时利用</strong>不同来源的知识<ul><li><strong>为什么要同时利用结构化和非结构化的知识库?</strong><ul><li><strong>结构化</strong>知识 (Structured Knowledge Source): 包含大量的三元组信息（概念 及其之间的关系），利于推理，但是存在覆盖度低的问题</li><li><strong>非结构化</strong>知识 (Unstructured Knowledge Source): 即 Plain-Text，包含大量冗余的、覆盖范围广的信息，可以辅助/补充结构化知识</li></ul></li></ul></li></ol><p>融合异构知识源必要性的例证：<img src="/../images/paper-csqa-1909-05311/example.png" alt="example"></p><ul><li>根据结构化知识库ConceptNet，可以挑选出候选 A 和 C</li><li>根据Wikipedia文本，可以挑选出候选 C 和 E</li><li>结合两类来源的证据，即可得到最终的正确答案 C</li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><p>本文针对的数据集是：<strong>CommonsenseQA</strong></p><p>本文的主要工作：</p><ul><li>自动地从<strong>异构知识源</strong>中抽取证据，即同时从ConceptNet和Wikipedia文章中抽取知识<ul><li>为每个知识源构建图，来获取证据间的关系结构</li></ul></li><li>提出了一个基于图的模型，由两个模块组成:<ul><li>基于图的上下文词表示学习模块<ul><li>为每个知识源都构建图结构，CN中使用自身的三元组，Wiki中通过语义角色标注来抽取出句子的三元组（谓词，论元）</li><li>utilize graph structural information to re-define the distance between words for learning better contextual word contextual word representations<ul><li>(利用图结构化信息来 重新定义词之间的距离以学习更好的上下文词表示)</li></ul></li></ul></li><li>基于图的推理模块<ul><li>使用GCN编码图信息</li><li>使用图注意力机制聚集证据信息</li></ul></li></ul></li><li>在CommonsenseQA上取得了当前最好的效果</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>CommonsenseQA 的任务形式：<br>输入：问题 $Q = {q_1, …, q_m}$ 和包含五个选项的候选答案集合 $A = {a_1, a_2, …, a_5}$<br>目标：从候选集合中选出正确答案<br>评价指标：准确率</p><p>本文提出的框架：<img src="/../images/paper-csqa-1909-05311/overview.png" alt="overview"></p><h3 id="Knowledge-Extraction"><a href="#Knowledge-Extraction" class="headerlink" title="Knowledge Extraction"></a>Knowledge Extraction</h3><p>知识抽取，主要是对数据进行预处理的阶段</p><h4 id="Knowledge-Extraction-from-ConceptNet"><a href="#Knowledge-Extraction-from-ConceptNet" class="headerlink" title="Knowledge Extraction from ConceptNet"></a>Knowledge Extraction from ConceptNet</h4><p>构建 Concept-Graph :</p><p>1、对于每个问题和选项，在CN中确定其中出现的实体；</p><p>2、构建从问题中的实体到候选中的实体的路径：</p><ul><li>小于 3 hops</li></ul><p>3、对于从CN中抽取出来的路径拆分为三元组，将<strong>每个三元组看做一个节点</strong>，融合到图中：</p><ul><li>如果两个三元组中包含相同的实体，就在图中相应的两点之间增加一条边</li><li>问题：为什么将三元组对应为图中的一个点？而不是之间利用CN原生的节点-关系结构</li></ul><p>4、为了获取CN中节点的上下文词表示，将三元组根据<strong>关系模板</strong>转化为<strong>自然语言语句</strong></p><h4 id="Knowledge-Extraction-from-Wikipedia"><a href="#Knowledge-Extraction-from-Wikipedia" class="headerlink" title="Knowledge Extraction from Wikipedia"></a>Knowledge Extraction from Wikipedia</h4><p>使用的 Wikipedia 版本信息：version enwiki-20190301</p><p>Wikipedia 文本处理:</p><p>1、使用 Spacy 从中抽取出 107M 个句子， 并用 Elastic Search 工具构建句子索引<br>2、对于每个训练样例，去除问句和候选中的停用词，然后将所有词串联，作为检索查询<br>3、使用 Elastic 搜索引擎 在检索查询和所有句子之间进行排序，选择出 top-K 个句子作为 Wikipedia 提供的证据信息，在实验中 K=10</p><p>构建 Wiki-Graph :</p><p>1、使用SRL抽取出 句子中每个谓词的论元 (主语和宾语)<br>2、将论元和谓词都作为图中的节点，谓词和论元之间的关系作为边<br>3、为了增强图的连通性，基于下述两条规则来在 节点 a 和 节点 b 之间加入边 (首先去除停用词)：</p><ul><li>b 中包含 a 且 a 的词数大于3</li><li>a 与 b 仅有一个不同的词，并且 a 和 b 包含的词数都大于3</li></ul><h3 id="Graph-Based-Reasoning"><a href="#Graph-Based-Reasoning" class="headerlink" title="Graph-Based Reasoning"></a>Graph-Based Reasoning</h3><p>基于图的推理模块：<img src="/../images/paper-csqa-1909-05311/module.png" alt="gr"></p><p>由两部分组成，分别是 1. 基于图的上下文表示学习模块 和 2. 基于图的推理模块</p><h4 id="1-Graph-based-Contextual-Representation-Learning-Module"><a href="#1-Graph-based-Contextual-Representation-Learning-Module" class="headerlink" title="1.Graph-based Contextual Representation Learning Module"></a>1.Graph-based Contextual Representation Learning Module</h4><p>本文使用 <strong>XLNet</strong> 作为基本编码器。</p><p>将所有证据信息进行串联，作为 raw input 输入给 XLNet，获得每个词的表示。</p><p>这种方式构成的编码器输入存在一个问题：</p><ul><li>使在不同的知识源中提及的词的距离变远，即便是语义相关的。</li><li>（个人理解，这里是想说同一个词，在不同的证据句中出现的时候，由于仅仅将证据句进行简单串联，而且编码中依然存在长期依赖的问题，所以会造成，在多个证据句中出现的相同词的编码表示是存在较大差异的）</li></ul><p>针对这个问题，提出根据 graph 结构，来re-define证据词之间的相对位置。</p><ul><li>使语义相关的词的相对位置更近一些；</li><li>并用证据的内部关系结构获得更好的 CWR (contextual word representation)；</li><li>采用的方法是：利用排序算法，根据知识抽取部分得到的图结构，对证据句的顺序进行re-order；</li></ul><p>对于 Wikipedia Sentences :</p><ul><li>构建一个句子图（sentence-graph），证据句是图中的节点</li><li>当满足以下条件时，在两个句子 $s_i$ 和 $s_j$ 间建立边：<ul><li>如果在 Wiki-Graph 中的 节点 $p$ 和 $q$ 之间存在一条边，且 $p$ 和 $q$ 分别在句子 $s_i$ 和 $s_j$中。</li></ul></li><li>然后根据 下图（<strong>算法1</strong>） 对证据句进行重排序<ul><li><img src="/../images/paper-csqa-1909-05311/sort-algo.png" alt="algo"></li></ul></li></ul><p>对于结构化知识CN :</p><ul><li>根据关系模板，将 CN 中的三元组转化为自然语句，作为 CN 提供的证据句：<ul><li>E.g.: <code>(mammals, HasA, hair)</code> —&gt; <code>mammals has hair</code></li></ul></li><li>也是根据上图的 <strong>算法1</strong> 对证据句进行重排序</li></ul><p>注：从两个知识源中抽取出的证据句分别排序</p><p>（针对算法1的一些看法：深度优先搜索，构建的句子图是无向图，1、在DFS方法定义中，递归调用DFS时没有传递sorted_sequence参数；2、在每次访问完一个节点之后，都将其插入到sorted_sequence的第0位上；3、排序之后，选取多少个句子？）</p><p>最终，XLNet 的输入格式为：</p><ul><li>CN Evidence sentences $S^{\prime}_T$ ; Wiki Evidence sentences ; question $q$ ; choice $c$</li><li>四个部分的串接，用 <code>[SEP]</code> 进行分隔</li><li>在实验部分，对choice还增加了一个头部：<code>The answer is</code></li><li>最大长度设为 <code>256</code></li><li>得到 word-level clue</li></ul><h4 id="2-Graph-based-Inference-Module"><a href="#2-Graph-based-Inference-Module" class="headerlink" title="2.Graph-based Inference Module"></a>2.Graph-based Inference Module</h4><p>主要目的：在图级别上聚集、传播证据信息，并在图上进行推理以预测最终的答案</p><p>1、将 两个证据图 CN-graph 和 Wiki-Graph 看做一个图，使用 GCN 进行编码，获得节点的表示</p><ul><li>在使用 GCN 的时候，将图看为无向图进行设置</li><li>节点表示：$i$-th node<ul><li>$h_i^0$ 由证据句的 hidden state 的平均得到<ul><li>$h_i^0 = \sigma(W \sum_{w_j \in s_i} \frac{1}{|s_i|} h_{w_j})$</li><li>$s_j = \{w_0, …, w_t\}$</li><li>$W \in \mathbb{R}^{d \times k}$</li></ul></li></ul></li></ul><p>2、证据传播过程，分为两步：聚集 和 组合<br>（1）从邻居节点聚集信息</p><ul><li>$z_i^l = \sum_{j\in N_i} \frac{1}{|N_i|} V^l h_j^l$<br>（2）组合，更新节点表示</li><li>$h_i^{l+1} = \sigma(W^l h_i^l + z_i^l)$</li></ul><p>（$l$ 表示层数，$L$ 表示最终层）</p><p>3、利用图注意力，聚集图级别表示，进行最终的预测</p><ul><li>使用 multiplicative attention，$h^c$ 表示 XLNet 中对应的 <code>[CLS]</code> 位置的表示</li><li>第 $i$ 个节点的重要程度：<ul><li>$\alpha_i = \frac{h^c \sigma(W_1 h_i^L)}{\sum_{j\in N h^c \sigma(W_1 h_j^L)}}$</li></ul></li><li>最终的图表示<ul><li>$h^g = \sum_{h\in N} \alpha_j^L h_j^L$</li></ul></li></ul><p>4、最终的预测打分</p><ul><li>$score(q,a) = \text{MLP}(h^g)$</li><li>$p(q,a) = \frac{e^{score(q,a)}}{\sum_{a^{\prime}\in A} e^{score(q,a^{\prime})}}$</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>实验设置：</p><ul><li>batch size = 4</li><li>学习率 = 5e-6</li><li>训练轮数 = 1 epoch (2800 steps)</li></ul><p>主实验结果：<br><img src="/../images/paper-csqa-1909-05311/exp-all.png" alt="exp-all"></p><ul><li>给leaderboard上公布出的模型划分了四个组，对比的很全面</li></ul><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>1、对模型组成结构的验证</p><ul><li><img src="/../images/paper-csqa-1909-05311/exp-ablation.png" alt="exp-module"></li></ul><p>2、对使用知识源的验证，证实了加入异构来源的知识对最终的性能提升有很大的帮助</p><ul><li><img src="/../images/paper-csqa-1909-05311/exp-kn.png" alt="exp-kn"></li></ul><h3 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h3><p>在验证集中随机选择了50个错误样例，错误类型大致分为三种：</p><ul><li>lack of evidence</li><li>similar evidence</li><li>dataset noise</li></ul><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><ul><li>论文中涉及到了多个图，在叙述上有些混乱。</li><li>对于 topology 排序算法的验证，还需要加入一些例子来证明这部分性能提升的缘由，排序之前和排序之后，对于evidence的表示，产生了什么样的影响。</li></ul><p>疑问：</p><ol><li>在CN知识抽取部分，为什么将三元组对应为图中的一个点？而不是之间利用CN原生的节点-关系结构？<ul><li>以句子作为节点，可能会获得更长的context信息，用于表示节点信息？</li></ul></li></ol><blockquote id="fn_cose"><sup>cose</sup>. Explain Yourself! Leveraging Language Models for Commonsense Reasoning. ACL,2019. <a href="/2019/07/10/paper-acl2019-cos-e/" title="note">note</a><a href="#reffn_cose" title="Jump back to footnote [cose] in the text."> &#8617;</a></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> commonsense </tag>
            
            <tag> graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EMNLP2019 | KagNet - Knowledge-Aware Graph Networks for Commonsense Reasoning</title>
      <link href="/2019/09/09/paper-emnlp2019-kagnet/"/>
      <url>/2019/09/09/paper-emnlp2019-kagnet/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: <a href="https://arxiv.org/abs/1909.02151" target="_blank" rel="noopener">KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning</a><br><em>Author</em>: Bill Yuchen Lin, Xinyue Chen, Jamin Chen, Xiang Ren<br><em>Org.</em>: University of Southern California, Shanghai Jiao Tong University<br><em>Published</em>: EMNLP,2019<br><em>Code</em>: <a href="https://github.com/INK-USC/KagNet" target="_blank" rel="noopener">https://github.com/INK-USC/KagNet</a></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>本文针对的数据集是：<strong>CommonsenseQA</strong></p><p>目标：</p><ul><li><strong>empowering</strong> machines with the ability to perform commonsense reasoning/inferences<ul><li>关于推理的定义：<ul><li>reasoning is the process of combining facts and beliefs to make new decisions <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Philip N Johnson-Laird. 1980. Mental models in cognitive science. Cognitive science, 4(1):71–115.">[1]</span></a></sup>.</li><li>reasoning is the ability to manipulate knowledge to draw inferences <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Drew A. Hudson and Christopher D. Manning. 2018. Compositional attention networks for machine reasoning. In Proc. of ICLR.">[2]</span></a></sup>.</li></ul></li><li>关于常识推理的定义：<ul><li>commonsense reasoning utilizes the basic knowledge that reflects our natural understanding of the world and human behaviors, which is common to all humans.</li></ul></li><li>Naive <strong>Physics</strong>: Humans’ natural understanding of the physical world</li><li>Folk <strong>Psychology</strong>: Humans’ innate ability to reason about people’s behavior and intentions</li></ul></li><li>gap between baselines and human performance</li><li>lack of transparency and interpretability<ul><li>how the machines manage to answer commonsense questions and make their inferences.</li></ul></li><li>why exploit commonsense knowledge bases<ul><li>knowledge-aware models can explicitly incorporate external knowledge as <strong>relational inductive biases</strong><ul><li>enhance reasoning capacity</li><li>increase the transparency of model behaviors for more interpretable results</li></ul></li><li>challenges<ul><li>How can we ﬁnd the most relevant paths in KG? ( noisy )</li><li>What if the best path is not existent in the KG? ( incomplete )</li></ul></li></ul></li></ul><h3 id="This-work"><a href="#This-work" class="headerlink" title="This work"></a>This work</h3><p>Knowledge-aware reasoning framework, two major steps:</p><ul><li>schema graph grounding (see figure below)</li><li>graph modeling for inference</li><li><img src="/../images/paper-emnlp2019-kagnet/1-graph.png" alt="k-g"></li></ul><p>Knowledge-aware graph network module: <code>KAGNET</code></p><ul><li><p><code>GCN-LSTM-HPA</code> 结构：</p><ul><li>由GCN, LSTM, 和一个 <strong>hierarchical path-based attention</strong> mechanism组成</li><li>用于 <strong>path-based relational graph representation</strong></li></ul></li><li><p>overall workflow</p><ul><li><p><img src="/../images/paper-emnlp2019-kagnet/2-overview.png" alt="overview"></p></li><li><p>首先，分别识别出 $q$ 和 $a$ 中提及的 concept ，根据这些 concept ，找到他们之间的路径，构建出 (ground) schema graph；</p></li><li>使用 LM encoder 编码 QA 对，产生 statement vector，作为 <code>GCN-LSTM-HPA</code> 的输入，来计算 graph vector；</li><li>最后使用 graph vector 计算最终的QA对的打分</li></ul></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>question $q$ with $N$ candidate answers $\{a_i\}$</p><p>schema graph $g=(V, E)$</p><h3 id="Schema-Graph-Grounding"><a href="#Schema-Graph-Grounding" class="headerlink" title="Schema Graph Grounding"></a>Schema Graph Grounding</h3><h4 id="Concept-Recognition"><a href="#Concept-Recognition" class="headerlink" title="Concept Recognition"></a>Concept Recognition</h4><ul><li>n-gram 匹配：句子中的 token 和 ConceptNet 的顶点集合进行 n-gram 匹配</li><li>Note：从有噪声的知识源中有效地提取上下文相关的知识仍是一个开放问题</li></ul><h4 id="Schema-Graph-Construction"><a href="#Schema-Graph-Construction" class="headerlink" title="Schema Graph Construction"></a>Schema Graph Construction</h4><p>sub-graph matching via path finding</p><ul><li>采取一种直接有效的方法：直接在Q和A中提及的Concept ($\mathcal{C}_q \cup \mathcal{C}_a$) 之间查找路径<ul><li>对于问题中的一个 Concept $\mathcal{c}_i \in \mathcal{C}_q$ 和候选项中的一个 Concept $\mathcal{c}_j \in \mathcal{C}_a$ ，查找他们之间路径长度小于 $k$ 的path，添加到图中<ul><li>本文中，设置 $k=4$，即3-hop paths</li></ul></li></ul></li></ul><h4 id="Path-Pruning-via-KG-Embedding"><a href="#Path-Pruning-via-KG-Embedding" class="headerlink" title="Path Pruning via KG Embedding"></a>Path Pruning via KG Embedding</h4><p>为了从潜在噪声的schema graph中删除无关的path</p><ul><li>使用KGE方法，如TransE等，预训练Concept Embedding和Relation Type Embedding（同时可用于KAGNET的初始化）</li><li>评价路径的质量<ul><li>将路径分解为三元组集合，一条路径的打分为每一组三元组的乘积，通过设置一个阈值进行过滤。</li><li>三元组的打分通过KGE中的打分函数进行计算（例如，the confidence of triple classification）</li></ul></li></ul><h3 id="Knowledge-Aware-Graph-Network"><a href="#Knowledge-Aware-Graph-Network" class="headerlink" title="Knowledge-Aware Graph Network"></a>Knowledge-Aware Graph Network</h3><ul><li><p>overview: <img src="/../images/paper-emnlp2019-kagnet/3-module.png" alt="overviwe"></p></li><li><p>首先，使用GCN对图进行编码</p></li><li>然后，使用LSTM对 $\mathcal{C}_q$ 和 $\mathcal{C}_a$ 之间的路径进行编码，捕捉multi-hop relational Information</li><li>最后，使用 hierarchical path-based attention 计算 relational schema graph 和 QA 之间路径的关系</li></ul><h4 id="Graph-Convolution-Networks"><a href="#Graph-Convolution-Networks" class="headerlink" title="Graph Convolution Networks"></a>Graph Convolution Networks</h4><p>使用GCN的目的：</p><ul><li>contextually refine the concept vector<ul><li>这里的 context 指节点在图中的上下文，即邻接关系</li><li>使用邻居来对预训练的Concept Embedding进行消歧</li></ul></li><li>capture structural patterns of schema graphs for generalization</li><li>schema graph 的模式为推理提供了潜在的有价值的信息<ul><li>QA对Concept之间的 更短、更稠密的连接 可能意味着更大的可能性，在特定的上下中。</li><li>评价 候选答案 的可能性</li></ul></li><li>$h_i^{(l+1)} = \sigma(W_{self}^{(l)} h_i^{(l)} + \sum_{j \in N_i} \frac{1}{|N_i|} W^{(l)} h_j^{l})$</li></ul><h4 id="Relational-Path-Encoding"><a href="#Relational-Path-Encoding" class="headerlink" title="Relational Path Encoding"></a>Relational Path Encoding</h4><p>定义问题中的第 $i$个 concept $c_i^{(q)}$ 和候选答案中的第 $j$ 个 concept $c_j^{(a)}$ 之间的第$k$ 条路径为 $P_{i,j}[k]$ ，路径是三元组序列：</p><ul><li>$P_{i,j}[k]=[(c_i^{(q)}, r_0, t_0), …, (t_{n-1}, r_n, c_j^{(a)})]$<ul><li>relation vector 由 KGE 预训练得到</li><li>concept vector 是 上一环节 GCN 的顶层输出</li></ul></li><li>每个三元组表示为 三个向量的串联，得到 triple vector</li><li>使用LSTM编码三元组向量序列，得到 path vector<ul><li>$R_{i,j} = \frac{1}{|P_{i,j}|} \sum_k LSTM(P_{i,j}[k])$</li></ul></li><li>$R_{i,j}$ 可以视为问题中的concept 和 候选项中的concept 之间的潜在的关系</li></ul><p>聚集所有路径的表示，得到最终的 graph vector $g$</p><ul><li>这里使用了 <code>Relation Network</code> 的方法：<ul><li>$T_{i,j} = MLP([s;c_q^{i};c_a^{(j)}])$</li><li>statement vector $s$ 为 LM encoder <code>[CLS]</code>  的表示</li></ul></li><li>然后通过mean-pooling：称这种计算方式为 <code>GCN-LSTM-mean</code><ul><li><script type="math/tex; mode=display">g=\frac{\sum_{i,j}[R_{i,j};T_{i,j}]}{|\mathcal{C}_q| \times |\mathcal{C}_a|}</script></li></ul></li><li>通过这种简单的方式将分别从 <code>symbolic space</code>  和 <code>semantic space</code> 中计算的relational representation 进行融合</li></ul><p>候选项的 plausibility 打分：</p><ul><li>$\text{score}(q,a)=sigmod(MLP(g))$</li></ul><h4 id="Hierarchical-Attention-Mechanism"><a href="#Hierarchical-Attention-Mechanism" class="headerlink" title="Hierarchical Attention Mechanism"></a>Hierarchical Attention Mechanism</h4><p>考虑到不同的路径对推理的重要程度不同，采用 mean-pooling 不是一种可取的方式。</p><p>基于此，本文提出了 hierarchical path-based attention 机制，有选择地聚集重要的path vector以及更重要的QA concept 对。</p><ul><li>使用 path-level 和 concept-pair-level attention 来学习 根据上下文建模图表示</li><li>path-level<ul><li>$\alpha_{(i,j,k)} = T_{i,j} W_1 LSTM(P_{i,j}[k])$</li><li>$\hat{a}_{(i,j,\cdot)} = softmax(\alpha_{(i,j,k)})$</li><li>$\hat{R}_{i,j} = \sum_k \hat{a}_{(i,j,k)} \cdot LSTM(P_{i,j}[k])$</li></ul></li><li>concept-pair level<ul><li>$\beta_{(i,j)} = s W_2 T_{i,j}$</li><li>$\hat{\beta}_{(\cdot, \cdot)} = softmax(\beta_{(\cdot, \cdot)})$</li><li>$\hat{g} = \sum_{i,j} \hat{\beta}_{(i,j)} [\hat{R}_{(i,j)}; T_{i,j}]$ </li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Transferability"><a href="#Transferability" class="headerlink" title="Transferability"></a>Transferability</h3><h3 id="Case-Study-on-Interpretibility"><a href="#Case-Study-on-Interpretibility" class="headerlink" title="Case Study on Interpretibility"></a>Case Study on Interpretibility</h3><p><img src="/../images/paper-emnlp2019-kagnet/4-showcase.png" alt="exp"></p><h3 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h3><ul><li>negative reasoning<ul><li>graph grounding 对否定词不敏感</li></ul></li><li>comparative reasoning strategy<ul><li>没有进行答案之间的比较</li></ul></li><li>subjective reasoning<ul><li>有些答案是根据带有主观性的推理得到的</li></ul></li></ul><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><ul><li>kAGNET 可以看做是 knowledge-augmented Relation Network (RN) module</li></ul><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Philip N Johnson-Laird. 1980. Mental models in cognitive science. Cognitive science, 4(1):71–115.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Drew A. Hudson and Christopher D. Manning. 2018. Compositional attention networks for machine reasoning. In Proc. of ICLR.<a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> commonsense </tag>
            
            <tag> graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACL2019 | Modeling Semantic Compositionality with Sememe Knowledge</title>
      <link href="/2019/08/27/paper-acl2019-sc-with-sememe/"/>
      <url>/2019/08/27/paper-acl2019-sc-with-sememe/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Title: Modeling Semantic Compositionality with Sememe Knowledge<br>Authors: Fanchao Qi, Junjie Huang, Chenghao Yang, Zhiyuan Liu, Xiao Chen, Qun Liu, Maosong Sun<br>Org: Tsinghua University, Beihang University, Huawei Noah’s Ark Lab<br>Published: ACL 2018</p></blockquote><p>official code: <a href="https://github.com/thunlp/Sememe-SC" target="_blank" rel="noopener">https://github.com/thunlp/Sememe-SC</a></p><h2 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h2><ul><li>Semantic Compositionality (SC, 语义组合):<ul><li>is deﬁned as the linguistic phenomenon that the meaning of a syntactically complex unit is a function of meanings of the complex unit’s constituents and their combination rule <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Francis Jeffry Pelletier. 1994. The Principle of Semantic Compositionality. Topoi, 13(1):11–24.">[1]</span></a></sup></li></ul></li><li>Multiword Expressions (MWE, 多字/词表达):<ul><li>大多数关于 SC 的工作都是集中在 基于向量的分布式语义模型来学习 多字/词构成的 短语、成分的表示</li><li>基本框架（公式1），以两个词为例：<ul><li><script type="math/tex; mode=display">\mathbf{p} = f(\mathbf{w}_1, \mathbf{w}_2, R, K)</script></li><li>其中: <ul><li>$f$ 是组合函数</li><li>$\mathbf{p}$ 是MWE的embedding</li><li>$\mathbf{w}_1$ 和 $\mathbf{w}_2$ 表示 MWE 成分的embedding，也就是 组成 MWE 的词的表示</li><li>$R$ 是 <strong>组合规则（Combination Rule）</strong></li><li>$K$ 指在构建MWE语义时需要的额外的知识</li></ul></li></ul></li></ul></li><li>Sememe (义原)：<ul><li>the minimum Semantic units of human language</li><li><strong>all the words can be composed of a limited set of sememes</strong>, which is similar to the idea of <em>semantic primes</em> (语义启动)<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Anna Wierzbicka. 1996. Semantics: Primes and Universals: Primes and Universals. Oxford University Press, UK.">[2]</span></a></sup></li><li>HowNet：义原知识库</li></ul></li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>之前的工作：利用复杂的组合函数，而很少考虑外部知识</li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><ul><li>1、设计了一个简单的 Semantic Compositionality Degree (SCD，语义组合程度) 测量实验：<ul><li>通过实验发现 MWE 的 SCD 可以通过简单的基于义原的公式计算，并且和人类的判断高度相关</li><li>义原可以很好的刻画 MWE 的含义和 MWE 的组成成分，并且可以捕捉这两者之间的语义关联</li><li>证实：义原适用于建模 SC，并且可以提高 SC 相关任务的效果，如 MWE 表示学习</li></ul></li><li>2、提出了两个结合义原的SC模型同于学习 MWE embedding，同时将公式1中的组合规则也结合到了两个模型中:<ul><li>模型1：Semantic Compositionality with Aggregated Sememe model (SCAS)</li><li>模型2：Semantic Compositionality with Mutual Sememe Attention model (SCMSA)</li></ul></li><li>Note: 在这篇文章中，主要关注<strong>两个词组成的中文 MWE 的语义组合建模</strong></li></ul><h2 id="Measuring-SC-Degree-with-Sememes"><a href="#Measuring-SC-Degree-with-Sememes" class="headerlink" title="Measuring SC Degree with Sememes"></a>Measuring SC Degree with Sememes</h2><p>这一部分的主要工作是通过一个SCD验证实验来证明义原适用于建模语义组合</p><h3 id="Sememe-based-SCD-Computation-Formulae"><a href="#Sememe-based-SCD-Computation-Formulae" class="headerlink" title="Sememe-based SCD Computation Formulae"></a>Sememe-based SCD Computation Formulae</h3><p>基于义原的SCD计算规则</p><p>基本原则：</p><ul><li>不同的MWE具有不同的 SC degrees</li><li>一个词的所有义原可以准确的描述一个词的意思</li></ul><p>启发式的设计了SCD的计算规则集合，见下图：</p><ul><li><img src="/images/paper-acl2019-sc-with-sememe/scd.png" alt="scd"></li><li>数字越大表示SCD值越高</li><li>$S_p$, $S_{w_1}$, $S_{w_2}$ 分别表示 MWE 的义原集合 和 MWE 两个组成成分(词)的义原集合</li><li>SCD = 3:<ul><li>MWE 的义原集合为两个组成成分义原集合的的并集（union）相同</li><li>也就是说，MWE 的含义正好是两个成分含义的组合</li><li>the MWE is <strong>fully semantically compositional</strong></li></ul></li><li>SCD = 2:<ul><li>MWE 的义原集合为两个组成成分义原集合的的并集（union）的一个适合的子集</li><li>组成成分的语义有覆盖 MWE 语义的部分，但是不能准确的推出 MWE 的语义</li></ul></li><li>SCD = 1:<ul><li>MWE 与 组成成分共享一部分义原，但是他们分别具有各自私有的义原</li></ul></li><li>SCD = 0:<ul><li>MWE 的含义与其组成成分的含义完全不同</li><li>无法从组成成分的含义中推出 MWE 的语义</li><li>the MWE is <strong>completely</strong> <strong>non-compositional</strong></li></ul></li></ul><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>构建了一个人工标注的SCD数据集</p><p>评测人工标注的与规则预测的SCD之间的相关系数 (Pearson 和 Spearman)</p><h2 id="Sememe-incorporated-SC-Models"><a href="#Sememe-incorporated-SC-Models" class="headerlink" title="Sememe-incorporated SC Models"></a>Sememe-incorporated SC Models</h2><h3 id="Incorporating-Sememes-Only"><a href="#Incorporating-Sememes-Only" class="headerlink" title="Incorporating Sememes Only"></a>Incorporating Sememes Only</h3><p>仅结合义原计算 MWE 的情况对应于公式1的一个简化：</p><ul><li>$\mathbf{p} = f(w_1, w_2, K)$</li><li>使用 $S$ 表示所有的义原集合，词w的义原集合为 $S_w = \{s_1, …, s_{|S_w|}\} \subset S$</li><li>$w$ 和 $\mathbf{s}$ 的 embedding 维度都是 $\mathbb{R}^d$</li></ul><h4 id="Semantic-Compositionality-with-Aggregated-Sememe-model-SCAS"><a href="#Semantic-Compositionality-with-Aggregated-Sememe-model-SCAS" class="headerlink" title="Semantic Compositionality with Aggregated Sememe model (SCAS)"></a>Semantic Compositionality with Aggregated Sememe model (SCAS)</h4><p>SCAS 的结构如下图所示：</p><ul><li><img src="/images/paper-acl2019-sc-with-sememe/m1-scas.png" alt="scad"></li><li>SCAS 模型仅仅串联了MWE组成成分和他们的义原的embedding</li><li>$\mathbf{p} = \text{tanh} (W_c [w_1 + w_2; w_1^\prime + w_2^\prime] + b_c)$<ul><li>$w_1^\prime = \sum_{s_i \in S_{w_1}} \mathbf{s}_i$</li><li>$w_2^\prime = \sum_{s_j \in S_{w_2}} \mathbf{s}_j$</li></ul></li></ul><h4 id="Semantic-Compositionality-with-Mutual-Sememe-Attention-model-SCMSA"><a href="#Semantic-Compositionality-with-Mutual-Sememe-Attention-model-SCMSA" class="headerlink" title="Semantic Compositionality with Mutual Sememe Attention model (SCMSA)"></a>Semantic Compositionality with Mutual Sememe Attention model (SCMSA)</h4><p>SCMSA 的结构如下图所示：</p><ul><li><img src="/images/paper-acl2019-sc-with-sememe/m2-scmsa.png" alt="scmsa"></li><li>SCMSA 模型 计算了 一个组成成分的义原的 Mutual Attention 以及 与其他成分的义原集合的 Mutual Attention<ul><li>动机：组成成分之间的义原互不相同，在进行义原组合的时候，MWE 的义原应对组成成分的义原应该分配不同的权重</li></ul></li><li>$\mathbf{p}$ 的计算与SCAS相同，不同的是 $w^\prime$的计算：<ul><li>$\mathbf{e}_1 = \text{tanh} (W_a w_1 +b_a)$</li><li>$a_{2,i} = \frac{\text{exp}(\mathbf{s}_i \cdot \mathbf{e}_1)}{\sum_{s_j \in S_{w_2}} \text{exp}(\mathbf{s}_j \cdot \mathbf{e}_1) }$</li><li>$w_2^\prime = \sum_{s_i \in S_{w_2}} a_{2,i} \mathbf{s}_i$</li></ul></li></ul><h3 id="Integrating-Combination-Rules"><a href="#Integrating-Combination-Rules" class="headerlink" title="Integrating Combination Rules"></a>Integrating Combination Rules</h3><p>根据原始的公式1：</p><ul><li>使用不同的MWE组合矩阵来表示不同的组合规则，即：<ul><li>$W_c = W_c^r, r\in R_s$<ul><li>$W_c^r \in \mathbb{R}^{d \times 2d}$</li></ul></li><li>$R_s$ refers to combination rule set containing syntax rules of MWE<ul><li>e.g., adjective-noun, noun-noun</li></ul></li></ul></li><li>考虑到组合矩阵的稀疏性，以及组合矩阵之间会存在通用的组合信息，将上述矩阵进行拆分：<ul><li>$W_c = U^rV^r + W_c^c$<ul><li>$U^r \in \mathbb{R}^{d \times h_r}$</li><li>$V^r \in \mathbb{R}^{d \times h_r}$</li><li>$h_r \in \mathbb{N}_+$ 为超参数，根据组合规则变化</li><li>$W_c^c \in \mathbb{R}^{d\times 2d}$</li></ul></li></ul></li></ul><h3 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h3><ul><li><p>training for MWE similarity computation</p><ul><li>squared Euclidean distance</li></ul></li><li><p>training for MWE sememe prediction</p><ul><li>weighted cross-entropy loss</li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Francis Jeffry Pelletier. 1994. The Principle of Semantic Compositionality. Topoi, 13(1):11–24.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Anna Wierzbicka. 1996. Semantics: Primes and Universals: Primes and Universals. Oxford University Press, UK.<a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> chinese </tag>
            
            <tag> sememe </tag>
            
            <tag> hownet </tag>
            
            <tag> semantic-compositionality </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 | Improving Question Answering with External Knowledge</title>
      <link href="/2019/08/26/paper-2019-edl-md/"/>
      <url>/2019/08/26/paper-2019-edl-md/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Title: <a href="https://arxiv.org/pdf/1902.00993v1.pdf" target="_blank" rel="noopener">Improving Question Answering with External Knowledge</a><br>Authors: Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, Dong Yu<br>Org: Rensselaer Polytechnic Institute/ CMU/ Tencent AI Lab<br>Published: unpublished</p></blockquote><p>之前在组内汇报过的一篇文章，最近翻出来了，直接把之前做的slide分享一下</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li><img src="/images/paper-2019-edl-md/EDL-4.png" alt="4"></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul><li><img src="/images/paper-2019-edl-md/EDL-6.png" alt="6"></li><li><img src="/images/paper-2019-edl-md/EDL-7.png" alt="7"></li><li><img src="/images/paper-2019-edl-md/EDL-8.png" alt="8"></li><li><img src="/images/paper-2019-edl-md/EDL-9.png" alt="9"></li><li><img src="/images/paper-2019-edl-md/EDL-10.png" alt="10"></li><li><img src="/images/paper-2019-edl-md/EDL-11.png" alt="11"></li><li><img src="/images/paper-2019-edl-md/EDL-12.png" alt="12"></li><li><img src="/images/paper-2019-edl-md/EDL-13.png" alt="13"></li></ul><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul><li><img src="/images/paper-2019-edl-md/EDL-15.png" alt="15"></li></ul><h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> qa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 | SenseBERT - Driving Some Sense into BERT</title>
      <link href="/2019/08/18/paper-2019-sense-bert/"/>
      <url>/2019/08/18/paper-2019-sense-bert/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Title: <a href="https://arxiv.org/abs/1908.05646" target="_blank" rel="noopener">SenseBERT: Driving Some Sense into BERT</a><br>Authors: Yoav Levine, Barak Lenz, Or Dagan, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, Yoav Shoham<br>Org: AI21 Labs, Tel Aviv, Israel<br>Published: unpublished</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>现有的工作通过应用自监督学习，使神经网络语言模型在NLU上取得了很大的进步</li><li>但是，目前自监督技术主要在 <strong>word-form level</strong> 上进行操作（即在word-form level上提取自监督信号指导模型进行学习）<ul><li>这种 <strong>word-form</strong> 级别的监督只是本质的语义内容信号的一种替代。</li><li>从词汇语义 (lexical semantic) 的角度来看，word-form 可以看做是词汇的 surface-level 的表现形式</li></ul></li><li>一词多义是自然语言处理中的一个常见现象，一个词具有多种不同的含义（文中举的例子是 ‘bass’，既可以指一种鲈鱼，低音吉他，还可以指低音歌唱家）<ul><li>一个词，其本身的形式，仅仅是在给定上下文/特定语境中的实际意义的一个替代。</li><li>一词多义现象所带来的一个重要挑战就是自然语言理解中的歧义问题</li></ul></li><li>BERT中的MLM只是对word-form进行的mask，无法捕捉word-sense信息，即缺乏对 lexical semantic 的建模</li></ul><h2 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h2><p>这篇文章对于BERT的改进，正如其题目所说，drive some sense into BERT：</p><ul><li>除了基础的预测 masked word 任务，还引入了一类 <strong>explicit word-sense</strong> 作为 BERT的 <strong>semantic-level</strong> 的自监督信号<ul><li><strong>explicit word-sense</strong> 信息指的是 每个词在 WordNet 中所对应的 <strong>supersense</strong> （共有45个supersense分类，具体参见论文Appendix）</li><li>相应的增加了一个预测 masked word-sense 任务，即预测被mask的词所对应的supersense</li></ul></li><li>外部语言学知识的引入，还可以提高模型对于词汇语义（lexical semantics）的归纳偏置。</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>本文通过两种实验来证明所提出的SenseBERT的有效性: <strong>Lexical Semantics 实验</strong>和通用的<strong>GLUE</strong>评测</p><p>Lexical Semantics实验数据集为：SemEval WSD 和 WiC(Word in Context)</p><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><ul><li>一词多义是自然语言处理中普遍存在的现象之一</li><li>而传统的词向量训练方法得到的 Word Embedding 都是静态的向量表示，无法准确的表示一个 word-form 的多种不同 <strong>词义</strong></li><li>这两年以来，GPT、ELMO、BERT等一系列预训练语言模型，通过在大规模的无监督语料上进行预训练，从而使PLM产生 dynamic contextual word/token representation ，可以认为间接的缓解了 一词多义 问题。<ul><li>对于下游任务来说，使用PLM产生了更符合当前语境或上下文的词表示，使词表示更加准确</li><li>但还是从 word surface level 进行词义的学习，缺乏直接针对 lexical semantic 的监督信号</li></ul></li><li>相比于过去一些工作，使用 WordNet 中的 lexical semantic 信息作为词级别的特征输入，SenseBERT 使用 lexical semantic 信息作为监督信号参与到PLM中的训练中，还可以使模型具有区分 lexical semantic 信息的能力，增加了PLM对 word-sense 的建模能力。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> note </tag>
            
            <tag> pre-trained-lm </tag>
            
            <tag> wordnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACL2019 | Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension</title>
      <link href="/2019/07/29/paper-acl2019-kt-net/"/>
      <url>/2019/07/29/paper-acl2019-kt-net/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Title: Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension<br>Author: An Yang et al.<br>Org.: MOE(PKU), Baidu<br>Published: ACL,2019</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>预训练语言模型在机器阅读理解任务上取得了突破性进展。通过在海量无标注文本数据上对足够deep的网络结构进行预训练而得到的LM，能够捕捉复杂的语言现象，更好地理解语言。</li><li>然而，真正意义上的阅读理解不仅要求机器具备语言理解的能力，还要求机器具备知识以支撑复杂的推理。</li><li>例如：<img src="/images/paper-acl2019-kt-net/kt-net-example.png" alt="example"><ul><li>这个例子需要用到下面的知识<ul><li>world knowledge：<em>Trump is the person who leads US</em></li><li>word knowledge：<em>sanctions has a common hypernym with ban</em></li></ul></li><li>BERT在不知道额外知识的情况下无法正确地判断出结果</li></ul></li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><ul><li>为了同时利用强大的PLM捕获的语言规律和外部高质量的知识/事实，提出了语言表示与知识表示的深度融合模型 <strong>KT-NET（Knowledge and Text Fusion Net）</strong></li><li>外部知识使用的是 包含词汇知识的WordNet 以及 包含实体信息的NELL<ul><li>没有采取符号化的知识表示，而是使用了KB embedding，文中给出了两点原因：</li><li>1、KB embedding 带有整个KB的全局信息</li><li>2、易于同时融合多个KBs，而不需要过多的task-specific设计</li></ul></li><li>在ReCoRD（<a href="https://sheng-z.github.io/ReCoRD-explorer/）数据集上取得了SOTA" target="_blank" rel="noopener">https://sheng-z.github.io/ReCoRD-explorer/）数据集上取得了SOTA</a></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>模型的整体结构如图所示，主要包含4个模块：</p><ul><li><img src="/images/paper-acl2019-kt-net/kt-net-model.png" alt="model"></li></ul><ol><li>BERT encoding layer</li><li>knowledge integration layer: select desired KB embeddings and integrate KB representation with BERT representation</li><li>self-matching layer: fuse BERT and KB representations</li><li>output layer: predict the final answer</li></ol><p>使用 BERT 进行编码时，将question和passage连在一起输给BERT，question 作为 sentence1，passage 作为 sentence2。<br>使用 BERT 最后一层的输出 $h_i^L \in \mathbb{R}^{d_1}$ 作为 passage 和 question 中每个token的上下文编码。</p><p>下面主要介绍一下KT-NET中最重要的两个模块和KB编码与抽取过程：</p><h3 id="Knowledge-Integration-Layer"><a href="#Knowledge-Integration-Layer" class="headerlink" title="Knowledge Integration Layer"></a>Knowledge Integration Layer</h3><p>对于每个token $s_i$，我们可以得到上下文编码 $h_i^L$，以及检索到的相关KB concepts集合 $C(s_i)$</p><ul><li>$C(s_i)$中的每个 $c_j \in \mathbb{R}^{d_2}$ 是预训练好的KB embedding</li></ul><p>使用Attention机制自适应地选择最相关的KB Concepts，同时引入了一个sentinel向量$\bar{c} \in \mathbb{R}^{d_2}$，来控制检索出来的KB Concepts中没有相关KB的情况：</p><ul><li>concept 相关度：$\alpha_{ij} \propto \text{exp}(c_j^\top W h_i^L)$<ul><li>$W\in \mathbb{R}^{d_2 \times d_1}$</li></ul></li><li>sentinel 相关度：$\beta_i \propto \text{exp}(\bar{c}^\top W h_i^L)$</li></ul><p>经过Attention之后得到该token的 Knowledge State 表示向量：</p><ul><li>$\mathbf{k}_i = \sum_j \alpha_{ij}c_j + \beta_i \bar{c}$</li><li>由于sentinel向量的加入，需要对attention score进行约束：$\sum_j \alpha_{ij} + \beta_{i} = 1$</li></ul><p>最后，将得到是knowledge state向量与上下文编码表示进行<strong>串联</strong>，得到本模块的输出：</p><ul><li>knowledge-enriched 表示：$u_i = [h_i^L,k_i] \in \mathbb{R}^{d_1 + d_2}$</li></ul><h3 id="Self-Matching-Layer"><a href="#Self-Matching-Layer" class="headerlink" title="Self-Matching Layer"></a>Self-Matching Layer</h3><p>获取了knowledge-enriched表示之后，通过self-attention机制使上下文表示和知识表示进行交互，本文中设计了直接交互和间接交互两种self-attention计算方式</p><p>直接（direct）交互：采用的是BIDAF中的trilinear公式计算attention，计算token $s_j$ 和 token $s_i$ :</p><ul><li>$r_{ij} = w^\top [u_i, u_j, u_i \odot u_j]$<ul><li>$w \in \mathbb{R}^{3d_1 + 3d_2}$</li></ul></li><li>row-wise softmax: $a_{ij}=\frac{\text{exp}(r_{ij})}{\sum_j \text{exp} (r_{ij})}$</li><li>attended vector: $\mathbf{v}_i  = \sum_j a_{ij} u_j$</li><li>$\mathbf{v}_i$ 表示 每个token j与token i的直接交互程度</li></ul><p>间接（indirect）交互：</p><ul><li>间接交互指的是：token i和token j可以通过一个中间token k产生间接的关联，计算间接交互的方式很简单</li><li>attention matrix $\bar{\mathbf{A}} = \mathbf{A}^2$</li><li>$\bar{\mathbf{v}}_i = \sum_j \bar{a}_{ij} u_j$</li></ul><p>最后，将两种交互得到的attended向量表示进行串接，作为本模块的输出：</p><ul><li>$\mathbf{o}_i = [u_i, v_i, u_i - v_i, u_i \cdot v_i, \bar{v}_i, u_i - \bar{v}_i] \in \mathbb{R}^{6d_1 + 6d_2}$</li></ul><h3 id="Knowledge-Embedding-and-Retrieval"><a href="#Knowledge-Embedding-and-Retrieval" class="headerlink" title="Knowledge Embedding and Retrieval"></a>Knowledge Embedding and Retrieval</h3><p>KB Embedding：</p><ul><li>采用的是 Yang et al. (2015)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Embedding entities and relations for learning and inference in knowledge bases. ICLR,2015.">[1]</span></a></sup> 提出的 BILINEAR 算法</li></ul><p>KB Concepts Retrieval:</p><ul><li>WordNet：给定一个词，返回其在WordNet中的额同义词集（synsets）作为候选的KB Concepts</li><li>NELL：首先对Passage和Question进行NER，通过字符串匹配在NELL找到实体对应的mention，抽取对应的KB作为候选的KB Concepts</li><li>每个KB Concept 其实是在KB中对应的尾实体？</li></ul><p>在这一部分最后，文中还列举了自身的几点Advantages：</p><ul><li>之前的一些结合knowledge的MRC工作，都可以算作是一种 <strong>retrieve-then-encode</strong> 范式，这部分工作只对抽取出来的相关知识进行编码（参考另一篇blog<a href="/2019/01/09/paper-knreader/" title="note">note</a>）与整合，获得的是局部的、与文本相关的知识信息<ul><li>本文的工作首先是在整个KB上预训练出kb embedding，可以捕获全局的信息</li><li>基于使用的KB Embedding方法来说，是全局的表示</li></ul></li><li><strong>易于扩展至融合多个KB的信息</strong><ul><li>对于每个token可以从不同的KB中抽取出不同的KB Concepts集合 $C^j(s_i)$</li><li>通过不同的KB Concepts集合 $C^j(s_i)$ 计算出不同的 Knowledge State 向量 $k_i^j$</li><li>直接将多个 $k^j_i$ 与 $h_i^L$ 串联获得融合多个KB的knowledge-enriched表示：$u_i = [h_i^L, k_i^1, k_i^2,…]$</li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul><li>KT-NET中的encoder使用的是BERT-Large-Cased版本，输入最大长度限制为 384，学习率 3e-05，batch大小为24</li><li>KB embedding是预训练好的，在训练过程中固定，不进行微调</li><li>实验数据集为：ReCoRD 和 SQuAD</li><li>case study<ul><li>从例子的热力图中，可以明显看出与BERT相比，KT-Net可以使不同的question词对passage中的词赋予不同的相关性，并且具有KB信息的支持，而BERT中，不同的question词对同一个passage中的词的相关性是相近的，也就是说，question中的每个词对于passage中的词的相关性相等，</li><li><img src="/../images/paper-acl2019-kt-net/kt-net-case.png" alt="case"></li></ul></li></ul><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><ul><li>KT-NET 仅在 ReCoRD 数据集上取得了SOTA的成绩，印象中在leaderboard上放了将近半年也没有其他模型可以超越它的成绩，足以证明这个模型的性能</li><li>如何选择预训练KB embedding的方法？</li><li>从在SQuAD数据集上的实验可以看出，相比$KT-NET_{BOTH}$和$KT-NET_{NELL}$，$KT-NET_{wordnet}$取得了最好的结果，可以间接的说明，SQuAD数据集仅需要文本的信息就可以很好的回答问题，并不依赖外部的世界知识（实体信息）</li></ul><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Embedding entities and relations for learning and inference in knowledge bases. ICLR,2015.<a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> pre-trained-lm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACL2019 | Explicit Utilization of General Knowledge in Machine Reading Comprehension</title>
      <link href="/2019/07/17/paper-acl2019-kar/"/>
      <url>/2019/07/17/paper-acl2019-kar/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Title: Explicit Utilization of General Knowledge in Machine Reading Comprehension<br>Author: Chao Wang, Hui Jiang<br>Org.: York University<br>Published: ACL,2019<br>Old Version: Exploring Machine Reading Comprehension with Explicit Knowledge. arXiv:1809.03449</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>MRC模型和人类之间的差距有两方面<ul><li>1、MRC模型需要大量的训练样例来学习</li><li>2、MRC模型对于有意加入噪声数据不鲁棒</li></ul></li><li>造成差距的原因在于目前MRC模型仅利用了给定passage-question对中的信息，而没有像人类一样利用一些general knowledge</li><li>如何利用抽取的知识<ul><li>目前的做法都是隐式地将抽取到的知识的编码用于增强相应词的lexical/contextual表示</li><li>缺点：缺乏解释性和控制性</li></ul></li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><ul><li>本文中提出，词之间的语义联系（<strong>inter-word semantic connections</strong>）可以作为一种general knowledge<ul><li>例如：<img src="/images/paper-acl2019-kar/data-example.png" alt="example"></li></ul></li></ul><p>此外，本文</p><ul><li>提出了一种 <strong>data enrichment</strong> 方法，利用 <strong>WordNet</strong> 作为知识源，为passage-question对抽取 inter-word semantic connections</li><li>提出了一个 <strong>Knowledge Aided Reader（KAR）</strong> 模型，用于显示地将抽取到的 general knowledge 引入到模型中，并辅助 attention 机制</li></ul><h2 id="Data-Enrichment-Method"><a href="#Data-Enrichment-Method" class="headerlink" title="Data Enrichment Method"></a>Data Enrichment Method</h2><p>本文介绍的基于 WordNet 的数据富集的方法是一个可控的抽取过程</p><h3 id="Semantic-Relation-Chain"><a href="#Semantic-Relation-Chain" class="headerlink" title="Semantic Relation Chain"></a>Semantic Relation Chain</h3><p>WordNet 中的几个要素：</p><ul><li><strong>synset</strong>：同义集合（a set of words expressing the same sense）<ul><li>一个词可以有多个不同的senses，可以属于多个synset</li></ul></li><li><strong>semantic relations</strong>：synset 之间的语义关系<ul><li>在NLTK中，共有16种<ul><li>hypernyms, hyponyms, holonyms, meronyms, attributes, etc.</li></ul></li></ul></li></ul><p>本文根据 synset 和 semantic relation 定义了一个新的概念：<font color="blue"><strong>Semantic Relation Chain</strong></font></p><ul><li>连接一个 synset 与另一个 synset 的一系列 semantic relation（a concatenated sequence of semantic relations）</li><li>semantic relation chain 中的每个 semantic relation 定义为一跳（a hop）</li></ul><h3 id="Inter-word-Semantic-Connection"><a href="#Inter-word-Semantic-Connection" class="headerlink" title="Inter-word Semantic Connection"></a>Inter-word Semantic Connection</h3><p>Data Enrichment 方法的核心问题就是确定两个词之间是否存在 <strong>语义的联系（semantically connections）</strong><br>为了解决这个问题，定义了一个新的概念：<strong>扩展同义词集（the extended synsets of a word）</strong></p><ul><li>定义：通过 semantic relation chain 可以到达的 synset</li><li>定义符号：$S_w$ 表示 synset，$S_w^*$ 表示扩展同义词集</li><li>理论上来看，如果不加以限制的话，WordNet 中所有的 synset 都将属于 $S_w^*$<ul><li>故此，引入了一个超参数 $k \in \mathbb{N}$，表示 semantic relation chain 的最大跳数</li><li>即，只有小于 $k$ 的 chains 才用于构建 $S_w^*$：<ul><li>$ S_w^{*}(k) $</li><li>$\text{ if } k=0, \text{ we will have }S_w^{*}(0)=S_w$</li></ul></li></ul></li></ul><p>构建 inter-word semantic connections，定义：</p><ul><li>当且仅当 $S_{w_1}^* (k) \cap S_{w_2} \neq \emptyset$ 时，$w_1$ 和 $w_2$ 具有语义关联</li></ul><h3 id="General-Knowledge-Extraction"><a href="#General-Knowledge-Extraction" class="headerlink" title="General Knowledge Extraction"></a>General Knowledge Extraction</h3><p>遵循上述的定义，为给定的 passage-question 对抽取任意词与 passage 中的词的 inter-word semantic relation：</p><ul><li>只抽取 positional information</li><li>对词 $w$，抽取一个集合 $E_w$，包含所有 passage 中与 $w$ 有语义关联的词的位置，如果 $w$ 本身是 passage 中的词，去除其本身在 passage 中的位置</li><li>通过上一节定义的超参数 $k$ 就可以控制抽取的 扩展同义词集 的大小，即抽取出来的 general Knowledge 的数量<ul><li>超参数 $k$ 通过在验证集上的效果确定</li></ul></li></ul><h2 id="Knowledge-Aided-Reader"><a href="#Knowledge-Aided-Reader" class="headerlink" title="Knowledge Aided Reader"></a>Knowledge Aided Reader</h2><p>KAR 模型的结构如下图所示：</p><ul><li><img src="/images/paper-acl2019-kar/model-kar.png" alt="model-kar"></li><li>与现有的一些模型相比，主要的改进集中在右侧部分的输入和引入 general Knowledge 之后， attention 计算的改进</li></ul><p>Notation:</p><ul><li>$P = \{p_1, … , p_n\}$</li><li>$Q = \{q_1, … , q_m\}$</li></ul><p>Knowledge Aided Mutual Attention</p><ul><li>利用提前抽取好的 general Knowledge 为每个词 $w$ 构建 增强的上下文表示 $c_w^*$<ul><li>通过 $E_w$ 和 原始上下文向量 $C_p$ 的对应关系 得到 matching context embeddings $Z \in \mathbb{R}^{d\times |E_w|}$</li><li>计算 matching vector $c_w^+$<ul><li>$t_i = v_c^{\top} tanh(W_c x_i + U_c c_w) \in \mathbb{R}$</li><li>$c_w^+ = Z \text{ softmax} ({t_1,…,t_{|E_w|}}) \in \mathbb{R}^{d}$</li></ul></li><li>$c_w^* = \text{ReLU}( \text{MLP}([c_w ; c_w^+]) )$ </li></ul></li><li>attention 的计算方式 同 BIDAF</li></ul><p>Knowledge Aided Self Attention</p><ul><li>方法同上</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul><li>超参数 $k$ 的选择<ul><li><img src="/images/paper-acl2019-kar/exp-k.png" alt="exp-k"></li></ul></li></ul><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><ul><li>抽取出来的 general knowledge 相当于为 passage-question 对中的词构建了一个隐式的关联矩阵，通过这个关联矩阵，在两次 attention 中抽取出对应的 contextual representation 和 coarse memory （result of mutal attention）来辅助增强 attention 的输入</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> wordnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACL2019 | Explain Yourself! Leveraging Language Models for Commonsense Reasoning</title>
      <link href="/2019/07/10/paper-acl2019-cos-e/"/>
      <url>/2019/07/10/paper-acl2019-cos-e/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Title: Explain Yourself! Leveraging Language Models for Commonsense Reasoning<br>Authors: Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, Richard Socher<br>Org.: Salesforce Research<br>Published: ACL 2019</p></blockquote><p>official code: <a href="https://github.com/salesforce/cos-e" target="_blank" rel="noopener">https://github.com/salesforce/cos-e</a></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote><p>Commonsense reasoning that draws upon world knowledge derived from spatial and temporal relations, laws of physics, causes and effects, and social conventions is a feature of human intelligence.<br>However, it is difficult to instill such commonsense reasoning abilities into artificial intelligence implemented by deep neural networks. While neural networks effectively learn from a large number of examples, commonsense reasoning for humans precisely hits upon the kind of reasoning that is in less need of exemplification.<br>Rather, humans pick up the kind of knowledge required to do commonsense reasoning simply by living in the world and doing everyday things.<br>AI models have limited access to the kind of world knowledge that is necessary for commonsense reasoning.</p><p>from official blog: <a href="https://blog.einstein.ai/leveraging-language-models-for-commonsense/" target="_blank" rel="noopener">https://blog.einstein.ai/leveraging-language-models-for-commonsense/</a></p></blockquote><ul><li>DL 模型在需要常识推理的任务上表现不佳，原因可能是往往需要某种形式的世界知识，或是推理的信息未在输入中直接显示出来</li><li>虽然已经有了一些用于检验模型Commonsense推理能力的数据集，但是 it is still unclear how these models perform reasoning and to what extent that reasoning is based on world knowledge.</li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><ul><li>构建了一个Common Sense Explanations（CoS-E）数据集：收集了人类对于常识推理的解释（模拟人类的常识推理过程）以及highlight annotation（针对问句的）</li><li>提出了一个常识自动生成解释（Commonsense Auto-Generate Explanation，CAGE）框架：使用CoS-E数据集来训练语言模型，是LM可以自动的产生解释（这个解释可以在训练和推理过程中使用），证明可以有效地利用语言模型来进行Commonsense Reasoning</li><li>在任务数据集（CommonsenseQA）上取得了10%的提升（Version 1.0）</li><li>还进行了跨领域迁移的实验</li></ul><h3 id="Background-and-Related-Work"><a href="#Background-and-Related-Work" class="headerlink" title="Background and Related Work"></a>Background and Related Work</h3><ul><li>Commonsense reasoning</li><li>Natural language explanations</li><li>Knowledge Transfer in NLP</li></ul><h2 id="CoS-E-Corpus"><a href="#CoS-E-Corpus" class="headerlink" title="CoS-E Corpus"></a>CoS-E Corpus</h2><p>Common Sense Explanations 数据集是在基于CommonsenseQA任务数据集上构建的，包含两种形式的人类解释：</p><ul><li>1、open-ended 自然语言解释 （<em>CoS-E-open-ended</em>）</li><li>2、对于问句，标注了 highlighted span annotations，标注的是对于预测正确答案起到重要作用的词 （<em>CoS-E-selected</em>）</li><li>示例：<ul><li><img src="/images/paper-acl2019-cos-e/cos-e-example.png" alt="cos-e"></li></ul></li></ul><h2 id="Algorithm-and-Model"><a href="#Algorithm-and-Model" class="headerlink" title="Algorithm and Model"></a>Algorithm and Model</h2><p>（在这篇文章中，进行实验时使用的是CommonsenseQA Version 1.0 的数据集，所以只有三个候选答案，在 Version 1.1 中，每个问题对应5个候选答案）</p><p>框架图：</p><ul><li><img src="/images/paper-acl2019-cos-e/framework-overview.png" alt="overview"></li></ul><h3 id="Commonsense-Auto-Generated-Explanations-CAGE"><a href="#Commonsense-Auto-Generated-Explanations-CAGE" class="headerlink" title="Commonsense Auto-Generated Explanations (CAGE)"></a>Commonsense Auto-Generated Explanations (CAGE)</h3><p>在CAGE框架中，使用GPT进行微调。微调部分的框架如上图中的图（a）所示。</p><p>本文提出了两种生成解释（即微调GPT-LM）的方法：</p><ul><li>1、explain-and-then-predict（reasoning）</li><li>2、predict-and-then-explain（rationalization）</li></ul><p>方法一：<strong>Reasoning</strong></p><ul><li>输入：$C_{RE} = q,c_0, c_1, c_2 \text{ ?}\text{ commonsense says} $</li><li>目标：条件式生成 explanations $e$<ul><li>$\sum_i \text{log} P(e_i|e_{i-k},…,e_{i-1}, C_{RE}; \Theta)$</li><li>其中，$k$ 是语言模型生成句子的窗口大小，在本文中，设为大于explanation的长度</li></ul></li><li>称为reasoning的原因：<ul><li>在推理阶段也可以自动的生成explanation，为常识问答提供额外的上下文</li><li>Note：在生成explanation的过程中不知道正确答案的标签</li></ul></li></ul><p>方法二：<strong>Rationalization</strong></p><ul><li>输入：$C_{RA} = q,c_0, c_1, c_2 \text{ ? } a \text{ because} $</li><li>目标：同上</li><li>在这个算法中，在训练时，生成explanation的过程中是已知正确答案的标签的<ul><li>相当于为正确的 q-a 对 产生一个合理的解释</li><li>而不是真正的常识推理</li></ul></li></ul><p>CAGE的参数设置：</p><ul><li>最大序列长度：20</li><li>batch size：36</li><li>train epochs：10</li><li>learning rate：$1e-6$</li><li>warmup linearly with proportion $0.002$</li><li>weight decay：0.01</li><li>模型选择指标：在验证集上的BLEU和perplexity</li></ul><h3 id="Commonsense-Predictions-with-Explanations"><a href="#Commonsense-Predictions-with-Explanations" class="headerlink" title="Commonsense Predictions with Explanations"></a>Commonsense Predictions with Explanations</h3><p>这部分关注的是，给定一个人类生成的解释或是LM推理生成的解释，使模型在CQA任务上进行预测。见上图图（b）</p><p>任务模型采用BERT with multiple choice</p><ul><li>BERT输入：question [SEP] explanation [SEP] candidate answer choice</li><li>微调BERT的参数：<ul><li>batch size：24</li><li>test batch size：12</li><li>train epochs：10</li><li>maximum sequence length：175（baseline=50）</li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>这篇文章的实验部分很充分、很完整，包含以下几个方面：</p><ul><li>使用CoS-E进行CQA任务，在dev上的效果：Table-2<ul><li><em>仅在训练时应用</em> CoS-E-open-ended</li><li>BERT baseline：63.8%</li><li>在BERT中加入CoS-E-open-ended，提升了2个点（65.5%）</li><li>CAGE-reasoning：72.6%</li><li>CQA test set 上的效果见 Table-3</li></ul></li><li>Table-4：在训练和验证时同时应用CoS-E</li><li>Table-5：在CQA v1.1 测试集上的结果</li><li>迁移到域外数据集</li></ul><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><p>论文中有意思的几点分析</p><ul><li>explanations可以帮助阐明更长更复合的问题</li><li>CAGE-reasoning 生成的explanations中，有43%的可能性包含答案选项</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> note </tag>
            
            <tag> commonsense </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EMNLP2018 | Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text</title>
      <link href="/2019/07/08/paper-emnlp2018-graft-net/"/>
      <url>/2019/07/08/paper-emnlp2018-graft-net/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Title</em>: Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text<br><em>Authors</em>: Haitian Sun, Bhuwan Dhingra et al.<br><em>Org.</em>: CMU<br><em>Published</em>: EMNLP 2018</p></blockquote><p>official code: <a href="https://github.com/OceanskySun/GraftNet" target="_blank" rel="noopener">https://github.com/OceanskySun/GraftNet</a></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>大多数 open domain QA 任务都是使用单信息源（要么是text from encyclopedia，或者是 single KB）来回答问题</li><li>判断一个信息源的适用性（suitability），取决于<strong>信息源的覆盖度（coverage）</strong>和从其中<strong>抽取答案的难度（difficulty of extracting answers from it）</strong><ul><li>非结构化，large text corpus作为信息源：具有很高的覆盖度，但是信息被不同的text pattern表示（这里可以理解为，不同领域/体裁的文本有不同的表现形式），模型还需要学习这些text pattern，导致模型难以泛化到其他领域以及新的推理类型</li><li>结构化，KB作为信息源：覆盖度低（由于不可避免的不完全性和有限的模式），但是更易于抽取答案</li></ul></li><li>由于有两种信息源存在，有些问题可被text回答，有些问题更适合用KB回答，但是只使用一种信息源不足以回答问题，一个很自然的问题就是<strong>如何有效地结合多种类型的信息</strong>，有以下两种方式：<ul><li><font color="blue"><strong>late fusion</strong></font>: <ul><li>为每种信息源设计SOTA的QA模型，得到他们的预测结果之后，再用一些启发式的方法将得到的答案进行聚合</li><li>问题：sub-optimal的解决方法，模型受限于从不同的信息源中聚集证据信息</li></ul></li><li><font color="green"><strong>early fusion</strong></font>：本文所采取的方式<ul><li>只利用一个模型，训练其从一个问题子图中抽取答案</li><li>问题子图，既包含相关的 KB fact 又包含 text</li><li>可以灵活地结合多个信息源的知识</li></ul></li></ul></li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><p>这篇文章进行的是开放域的KBQA任务（incomplete KB），结合图表示学习，提出了一个GRAFT-Net模型，可以从同时包含文本、KB实体与关系的 Question-specific 子图中抽取答案</p><ul><li>为了实现early fusion，提出了一个 Graphs of Relations Among Facts and Text Network（GRAFT-Net）</li><li>基于图卷积神经网络模型，可以在由KB facts和text sentences组成的异构图上进行运算<ul><li>提出了 <strong>heterogeneous update rules</strong> 来处理KB节点；<strong>LSTM-based update rules</strong>来更新text节点</li><li>提出了 <strong>directed propagation method</strong>， 启发自 Personalized PageRank 算法，用于限制图中 embedding 在基于从seed节点链接到question的路径上进行传播</li></ul></li><li>实验数据集：WikiMovies，WebQuestionSP</li></ul><h2 id="Task-Setup"><a href="#Task-Setup" class="headerlink" title="Task Setup"></a>Task Setup</h2><h3 id="Task-Description"><a href="#Task-Description" class="headerlink" title="Task Description"></a>Task Description</h3><h3 id="Question-Subgraph-Retrieval"><a href="#Question-Subgraph-Retrieval" class="headerlink" title="Question Subgraph Retrieval"></a>Question Subgraph Retrieval</h3><h2 id="GRAFT-Net"><a href="#GRAFT-Net" class="headerlink" title="GRAFT-Net"></a>GRAFT-Net</h2><h3 id="1-Node-Initialization"><a href="#1-Node-Initialization" class="headerlink" title="1.Node Initialization"></a>1.Node Initialization</h3><h3 id="2-Heterogeneous-Updates"><a href="#2-Heterogeneous-Updates" class="headerlink" title="2.Heterogeneous Updates"></a>2.Heterogeneous Updates</h3><p>Entities<br>Documents</p><h3 id="3-Conditioning-on-the-Question"><a href="#3-Conditioning-on-the-Question" class="headerlink" title="3.Conditioning on the Question"></a>3.Conditioning on the Question</h3><p>Attention over Relations<br>Directed Propagation</p><h3 id="4-Answer-Selection"><a href="#4-Answer-Selection" class="headerlink" title="4.Answer Selection"></a>4.Answer Selection</h3><h3 id="5-Regularization-via-Fact-Dropout"><a href="#5-Regularization-via-Fact-Dropout" class="headerlink" title="5.Regularization via Fact Dropout"></a>5.Regularization via Fact Dropout</h3><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> note </tag>
            
            <tag> reasoning </tag>
            
            <tag> kbqa </tag>
            
            <tag> graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 | Incorporating Sememes into Chinese Definition Modeling</title>
      <link href="/2019/07/08/paper-2019-cdm-with-sememe/"/>
      <url>/2019/07/08/paper-2019-cdm-with-sememe/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Incorporating Sememes into Chinese Definition Modeling<br>2019<br>Linear Yang et al.</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Chinese Definition Modeling(CDM) 任务：为给定的中文词产生词典式的中文定义</p><h3 id="This-work"><a href="#This-work" class="headerlink" title="This work"></a>This work</h3><ul><li>为了解决CDM任务，构建了一个CDM数据集，每个example由 <code>&lt;word, sememes, definition&gt;</code> 三元组组成</li><li>两个新模型 <ul><li>1、Adaptive-Attention Model（AAM）：利用adaptive注意力机制结合sememes（义原）信息生成 Definition</li><li>2、Self- and Adaptive-Attention Model（SAAM ）：进一步使用self-attention替代AAM中的recurrent connection，减少word，sememes，definition之间的路径长度</li></ul></li></ul><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>Chinese Definition Modeling Corpus：</p><ul><li>包含 104,517 个条目</li><li>三元组：<code>&lt;a word, the sememes of a speciﬁc word sense, and the deﬁnition in Chinese of the same word sense&gt;</code></li><li>Sememes：义原，是描述词义的最小的语义单位<ul><li>具体信息请参考：HowNet</li><li>为什么要使用义原：可以为生成定义提供额外的语义信息</li></ul></li><li>例子：<img src="/images/paper-2019-cdm-with-sememe/fig-1-corpus.png" alt="cdm-corpus-example"></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>RNN-based Seq-to-Seq Model</p><h3 id="Adaptive-Attention-Model"><a href="#Adaptive-Attention-Model" class="headerlink" title="Adaptive-Attention Model"></a>Adaptive-Attention Model</h3><p>引入 Adaptive-Attention 的原因：</p><ul><li>vanilla attention 在每一步都会关于sememes</li><li>在生成definition的过程中，不是所有词都与sememes有关</li></ul><p>Adaptive-Attention</p><ul><li>利用 time-varying sememes 信息作为 sememe context</li><li>LM的信息作为 LM context<ul><li>首先，由 decoder 的 hidden state 和 上一时刻生成的definition词 通过线性映射和sigmoid运算得到 一个 gate 向量</li><li>再，对上一时刻的hidden state 进行tanh激活运算，通过 gated unit，得到 LM context</li></ul></li><li>再根据 context，引入一个新的attention，决定在生成当前时刻词是依赖 sememe context 还是 LM context</li></ul><h3 id="Self-and-Adaptive-Attention-Model"><a href="#Self-and-Adaptive-Attention-Model" class="headerlink" title="Self- and Adaptive-Attention Model"></a>Self- and Adaptive-Attention Model</h3><p>将 AAM 中的 RNN，换成 transformer</p><!-- ## Experiments --><!-- ## Summary & Analysis -->]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> tg </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AAAI2019 | Improving Question Answering by Commonsense-Based Pre-Training</title>
      <link href="/2019/07/06/paper-aaai2019-cs-based-pre-train/"/>
      <url>/2019/07/06/paper-aaai2019-cs-based-pre-train/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>AAAI,2019<br>中山大学，微软亚研</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>现有的神经网络模型不能很好的<strong>回答常识问题</strong>是由于<strong>缺乏concepts之间的常识联系</strong></li><li>回答有些问题需要模型有能力在常识知识上进行推理</li><li>回答这类的问题既需要词本身的知识又需要世界知识</li></ul><h2 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h2><ul><li>利用外部常识知识（ConceptNet）提高QA系统的常识推理能力</li><li>根据外部关于世界的常识知识预训练一个模型，对concepts之间的<strong>直接关系和间接关系</strong>进行<strong>预训练</strong><ul><li>预训练的functions可以轻松地加到神经网络中</li></ul></li><li>concept之间的关系可以分为<strong>直接</strong>和<strong>间接</strong><ul><li>可以学习<strong>两个</strong>度量每对concept之间直接和间接关系的functions</li></ul></li><li>好处：<ol><li>模型具有很大的concept/entity覆盖度</li><li>模型常识推理的能力不受限于训练实例的数量和不需要覆盖所有终端任务中需要的推理类型</li><li>易于扩展</li></ol></li><li>实验数据集：ARC / MCScripts</li></ul><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><ul><li>候选答案的打分函数由两部分组成<ul><li>$f(a_i) = \alpha f_{doc}(a_i) + \beta f_{cs}(a_i)$</li><li>分别是 document-based model 给出的打分 和 commonsense-based model 给出的打分</li><li>document-based model 同 yuanfudao 的 tri-attention</li></ul></li></ul><h3 id="Commonsense-based-Model"><a href="#Commonsense-based-Model" class="headerlink" title="Commonsense-based Model"></a>Commonsense-based Model</h3><ul><li>预训练知识表示的参数：</li><li><p>关系表示：</p><ul><li>两个concept之间的关系表示：$f_{cs}(c_1,c_2)=Enc(c_1) \odot Enc(c_2)$</li><li>concept encoder $Enc(\cdot)$ 的计算：<ul><li>$h^w(c) = BiLSTM(Emb(c))$</li><li>考虑到邻居节点：$h^n(c) = \sum_{c^\prime\in NBR(c)}(W^{r(c,c^\prime)} h^w(c^\prime) + b^{r(c,c^\prime)})$</li><li>$Enc(c) = [h^w(c);h^n(c)]$</li></ul></li><li>基于排序的损失函数：<ul><li>$l(c_1,c_2,c^\prime) = max(0, f_{cs}(c_1, c^\prime) - f_{cs}(c_1,c_2) + mgn)$</li><li>$c_1$和$c_2$是正例</li><li>$c_1$和$c^\prime$是负例</li><li>根据不同的策略对负例进行采样<ul><li>直接关系：直接根据kg中的邻接图进行采样</li><li>间接关系：拥有共同邻节点的作为正例，没有one-hop或two-hop关系的作为负例</li></ul></li></ul></li></ul></li><li><p>$f_{cs}(a_i)$函数是commonsense-based model 的打分函数：</p><ul><li>$f_{cs}(a_i) = \frac{1}{|E_1|} \sum_{x\in E_1} max_{y\in E_2}(f_{cs}(x,y))$<ul><li>max 表示选择$E_1$中最相关的concept</li></ul></li><li>其中<strong>$E_1$和$E_2$分别表示从问题句子Q和候选答案抽取出来的常识事实</strong></li><li>同样还可以计算从文章和候选答案抽取处理的常识事实<ul><li>对于P-Q对，为了保证和候选答案的相关性，去除不在候选答案抽取出来的知识集中的concept</li></ul></li><li>每个$E$是从知识库中抽取出的三元组</li></ul></li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>Topic：结合外部知识库或结构知识库的相关工作<br>这方面的工作可以分为两类，大部分属于第一类</p><ul><li>enhance each basic computational unit (word or noun phrase)<ol><li>Leveraging knowledge bases in lstms for improving machine reading</li><li>Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge.</li></ol></li><li>support external signals at the top layer before the model makes the final decision</li></ul><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><ul><li>cons:<ul><li>对文本和问题的建模和对知识的建模是分开的，通过最终的打分函数进行关联</li></ul></li><li>pros:<ul><li>通过区分直接关系和间接关系的采样来训练得到的知识表示向量效果更好</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cognition - Primary Research</title>
      <link href="/2019/06/11/mrc-research-cognitive/"/>
      <url>/2019/06/11/mrc-research-cognitive/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>To Be Update</p>]]></content>
      
      
      <categories>
          
          <category> MRC-Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> cognition </tag>
            
            <tag> research </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Recent Advances on Multi-Hop RC - WikiHop</title>
      <link href="/2019/06/06/paper-wikihop-1/"/>
      <url>/2019/06/06/paper-wikihop-1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>WikiHop in QAngaroo benchmark is a dataset for Multi-hop Reading Comprehension <strong>Across Documents</strong>.<br>Style: Multiple-Choice</p><p>Reference papers on WikiHop task:</p><blockquote><ol><li>Neural models for reasoning over multiple mentions using coreference. NAACL,2018.</li><li>Exploring graph-structured passage representation for multihop reading comprehension with graph neural networks. 2018.</li><li>Exploiting explicit paths for multihop reading comprehension. 2018.</li><li>BAG: Bi-directional Attention Entity Graph Convolutional Network for Multi-hop Reasoning Question Answering. NAACL,2019.</li><li>Question Answering by Reasoning Across Documents with Graph Convolutional Networks. NAACL,2019.</li><li>Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs. ACL,2019.</li><li>Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question Answering. ICLR,2019.</li><li>Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension. ACL,2019.</li></ol></blockquote><h2 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h2><p>Input: $&lt; q, P, C_q &gt;$</p><ul><li>query: $q$ in the form of triple without tail entity $&lt; h_e, r, ? &gt;$</li><li>a set of supporting documents: $P = \{P_1, …, P_M\}$</li><li>a set of candidate answers (all of which are entities mentioned in $P$): $C_q = \{C_1,…,C_N\}$</li></ul><p>Goal: </p><ul><li>select $a^{\star} \in C_q$, which is the entity that correctly answers the question</li><li>need to aggregate information from multiple evidences across documents</li></ul><h2 id="Coref-GRU"><a href="#Coref-GRU" class="headerlink" title="Coref-GRU"></a>Coref-GRU</h2><blockquote><p> <em>Neural models for reasoning over multiple mentions using coreference.</em><br> NAACL,2018.<br> Bhuwan Dhingra. (William W. Cohen)<br> CMU.</p></blockquote><h3 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>existing RNN layer are biased towards short-term dependencies</li></ul><p>This work:</p><ul><li>adapt a standard RNN layer by introducing a bias towards <strong>coreferent recency</strong></li></ul><h3 id="2-Model-Details"><a href="#2-Model-Details" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><ul><li>coreference relationships between words (Directed acyclic graph(DAG) style graph)</li><li>introduce a term in the update equations for GRU which depends on the hidden state of the coreferent antecedent of the current token/word<ul><li>hidden states are propagated along coreference chains and the original sequence in parallel</li></ul></li></ul><h2 id="MHQA-GCN-GRN"><a href="#MHQA-GCN-GRN" class="headerlink" title="MHQA-GCN/GRN"></a>MHQA-GCN/GRN</h2><blockquote><p><em>Exploring graph-structured passage representation for multihop reading comprehension with graph neural networks.</em><br>2018.<br>Linfeng Song. (Yue Zhang)<br>University of Rochester. (Westlake University)</p></blockquote><h3 id="1-Motivation-1"><a href="#1-Motivation-1" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>local coreference information is limited in providing information for rich inference and global evidence</li></ul><p>This work:</p><ul><li>form more complex graphs to better connecting global evidence</li><li>considering two more types of edges in addition to coreference<ul><li>same entity mentions (<strong>cross-document</strong>)</li><li>window-typed (<strong>within-document</strong>)<ul><li>two mentions of different entities within a context window</li></ul></li></ul></li></ul><h3 id="2-Model-Details-1"><a href="#2-Model-Details-1" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><ul><li>Encoding: evidence integration with graph network<ul><li>graph recurren network</li><li>graph convolutional network</li></ul></li><li>Prediction: summing the probabilities over all occurrences of the same entity mention<ul><li><script type="math/tex; mode=display">Pr_{\epsilon}=\frac{\sum_{k\in{C_q}}\alpha_k}{\sum_{k^{\prime} \in {C_q}} \alpha_{k^{\prime}}}</script></li><li><script type="math/tex; mode=display">\alpha_k = \frac{exp(e^k)}{\sum_{k^{\prime} \in C_q} exp(e^{k^{\prime}})}</script></li><li>$e^k$ is the representation of entity mention of $\epsilon_k$</li></ul></li></ul><h2 id="Path-based-Kundu-2018"><a href="#Path-based-Kundu-2018" class="headerlink" title="Path-based(Kundu.2018.)"></a>Path-based(Kundu.2018.)</h2><blockquote><p><em>Exploiting Explicit Paths for Multi-hop Reading Comprehension</em><br>2018</p></blockquote><h3 id="1-Motivation-2"><a href="#1-Motivation-2" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>graph-based models only <strong>implicitly</strong> <strong>combine</strong> knowledge from all the passages<ul><li>unable to provide explicit reasoning paths for the selected answer.</li></ul></li></ul><p>This work</p><ul><li>present a path-based reasoning approach for textual reading comprehension<ul><li>generating potential paths across multiple passages</li><li>extracting implicit relations along this path</li><li>composing relations to encode each path</li></ul></li></ul><h3 id="2-Model-Details-2"><a href="#2-Model-Details-2" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><ul><li>Path Define:<ul><li>only consider <strong>two-hop</strong> path: eg: $path_{kj}=h_e \rightarrow e_1 \rightarrow c_k$</li><li>$e_1$: intermediate entity and can be extended to multi-hop</li><li>an extracted path is <strong>a set of entity sequences</strong></li></ul></li><li>Path Extraction: for each candidate<ul><li>step #1: find a passage $P_1$ contains $h_e$ of the query</li><li>step #2: find intermediate entities: all Named Entities and Noun Phrases that appear in the same sentence with $h_e$ or in the subsequent sentence</li><li>step #3: find another passage $P_2$ containes any of the intermediate entities found in step#3<ul><li>for distinguishing the $e_1$ in different passage，use $e_1^{\prime}$ to stand for the same mention in the second passage</li></ul></li><li>step #4: check if passage $P_2$ contains any of the candidate answer choices</li></ul></li><li>Path Encoding:<ul><li>context-based path encoding<ul><li>use the concatenation of the boundary vectors of the passage encoding as the location encoding vector of entity<ul><li>$g_{e} = [s_{p_1,i_1};s_{p_1,i_2}]$</li></ul></li><li>extract implicit relation with a feed forward layer<ul><li>$r_{h_e,e_1}=FFL(g_{h_e}, g_{e_1})$</li><li>as well as $r_{e_1^{\prime}, c_k}$</li></ul></li><li>compose implicit relation vector with a feed forward layer<ul><li>$x_{ctx}=FFL(r_{h_e,e_1}, r_{e_1^{\prime},c_k})$</li></ul></li><li>feed forward layer $FFL(a,b)=tanh(aW_a + bW_b + bias)$</li></ul></li><li>passage-based path encoding<ul><li>question-weighted passage representation<ul><li>query-aware passage representation: $S_p^1$ and $S_p^2$</li></ul></li><li>aggregate passage representation: get single passage vector<ul><li>self-attention</li><li>$\tilde{s}_{p_1}$ and $\tilde{s}_{p_2}$ </li></ul></li><li>$x_{psg} = FFL(\tilde{s}_{p_1}, \tilde{s}_{p_2})$</li></ul></li></ul></li><li>Path Scorer:<ul><li>context-based path scoring<ul><li>$\tilde{q}=([q_0;q_L])W_q$</li><li>$y_{ctx,q}=FFL(x_{ctx},\tilde{q})$</li><li>$z_{ctx}=y_{ctx,q}W_{ctx}^T$</li></ul></li><li>passage-based path scoring<ul><li>self attention get single candidate answer choice vector $\tilde{c}_k$</li><li>$z_{psg}=\tilde{c}_k x_{psg}^T$</li></ul></li><li>unormalized score $z = z_{ctx} + z_{psg}$<ul><li>softmax over all the paths and candidates get $score(path_{kj})$</li></ul></li></ul></li><li>Prediction<ul><li>$prob(c_k)=\sum_j score(path_{kj})$</li></ul></li></ul><h2 id="BAG"><a href="#BAG" class="headerlink" title="BAG"></a>BAG</h2><blockquote><p><em>BAG: Bi-directional Attention Entity Graph Convolutional Network for Multi-hop Reasoning Question Answering.</em><br>NAACL,2019.</p></blockquote><h3 id="1-Motivation-3"><a href="#1-Motivation-3" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>comprehend the relationships of entities across documents before answering questions</li></ul><h3 id="2-Model-Details-3"><a href="#2-Model-Details-3" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><ul><li>Graph Construction<ul><li>node: all mentions of candidates</li><li>edge: undirected<ul><li>1)cross-document edge</li><li>2)within-document edge</li></ul></li></ul></li><li>Multi-Level Features:<ul><li>input node embedding</li><li>concatenation of GLoVe/ELMo(+linear)/NER/POS</li></ul></li><li>Query Encoding: BiLSTM + linear</li><li>Relational Graph Convolutional Network, same with Entity-GCN.</li><li>Bi-directional Attention Between a Graph and a Query<ul><li>similarity matrix $S = pooling_{mean}(f_a ([h_n; f_q; h_n \circ f_q] ))$<ul><li>$f_q$: query representation</li><li>$h_n$: all node representation</li></ul></li><li>directional computation is the same with BiDAF</li></ul></li><li>Prediction: the probability of each node becoming answer.<ul><li>the probability of each candidate is the sum of all corresponding nodes.</li></ul></li></ul><h2 id="Entity-GCN"><a href="#Entity-GCN" class="headerlink" title="Entity-GCN"></a>Entity-GCN</h2><blockquote><p><em>Question Answering by Reasoning Across Documents with Graph Convolutional Networks.</em><br>NAACL,2019.</p></blockquote><h3 id="1-Motivation-4"><a href="#1-Motivation-4" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><p>This work</p><ul><li>frame question answering as an inference problem on a graph representing the document collection.</li></ul><h3 id="2-Model-Details-4"><a href="#2-Model-Details-4" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><ul><li>Graph Construct<ul><li>node: mentions of candidate choices and head entity in the query</li><li>edge: <ul><li>co-occurrence in the same document</li><li>mentions that exactly match across document</li></ul></li></ul></li><li>Encoding<ul><li>ELMo: a concatenation of three 1024-dimensional vectors resulting in 3072-dimensional input vectors</li></ul></li><li>Graph Encoding: Relational GCN to model message passing process<ul><li>at layer $l$:</li><li>aggregation: aggregate information from neighbors of each node<ul><li><script type="math/tex; mode=display">z_i^l = \frac{1}{|N_i|} \sum_{j\in N_i} \sum_{r \in R_{ij}} f_r(h_j^l)</script></li><li>$N_i$ is the set of indices of nodes neighbouring $i$-th node</li><li>$R_{ij}$ is the set of edge annotations between $i$ and $j$</li></ul></li><li>combination<ul><li><script type="math/tex; mode=display">u_i^l = f_s(h_i^l) + z_i^l</script></li></ul></li><li>updating: how much of the update message propagates to the next step<ul><li><script type="math/tex; mode=display">g_i^l = sigmoid(f_g ( [z_i^l; h_i^l] ))</script></li><li><script type="math/tex; mode=display">h_i^{l+1} = tanh(u_i^l) \odot g_i^l + h_i^l \odot (1-g_i^l)</script></li></ul></li></ul></li><li>Prediction<ul><li>$Prob(c|q,C_q,P) \varpropto exp(max_{i\in M_c} f_o( [q, h_i^L] ) )$<ul><li>$M_c$ is the set of node indicate that $i \in M_c$ only if node $i$ is a mention of candidate choice $c$</li></ul></li></ul></li></ul><h2 id="Heterogeneous-Document-Entity"><a href="#Heterogeneous-Document-Entity" class="headerlink" title="Heterogeneous Document-Entity"></a>Heterogeneous Document-Entity</h2><blockquote><p><em>Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs.</em><br>ACL,2019.</p></blockquote><h3 id="1-Motivation-5"><a href="#1-Motivation-5" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>mainly compared with Entity-GCN</li><li>contains different granularity levels of information including candidates, documents and entities in speciﬁc document contexts</li></ul><p>This work</p><ul><li>design the Heterogeneous graph contains<ul><li>three kinds of nodes</li><li>seven types of edges</li></ul></li><li>include nodes corresponding to candidates, documents and entities.</li></ul><h3 id="2-Model-Details-5"><a href="#2-Model-Details-5" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><ul><li>Graph Construction<ul><li>node: <ul><li>1) candidate entity nodes</li><li>2) entity nodes extracted from documents</li><li>3) document nodes</li></ul></li><li>edge<ul><li>between (doc, entity)<ul><li>1) if the candidate appear in the document at least one time</li></ul></li><li>between (doc, entity)<ul><li>2) if the entity is extracted from the document</li></ul></li><li>between (entity, candidate)<ul><li>3) if the entity is a mention of the candidate</li></ul></li><li>between (entity, entity)<ul><li>4) if they are extracted from the same document</li><li>5) if they are mentions of the same candidate or query subject and they are extracted from different documents</li><li>7) entity nodes that do not meet previous conditions are connected</li></ul></li><li>between (cnadidate, candidate)<ul><li>6) all candidate nodes connect with each other</li></ul></li></ul></li></ul></li><li>Graph Encoding<ul><li>Relational GCN, the same with Entity-GCN</li></ul></li><li>Prediction<ul><li>$a = f_C(H^C) + ACC_{max}(f_E(H^E)$</li><li>$H^C$ : node representations of all candidate nodes </li><li>$H^E$: node representations of all entity nodes that correspond to candidates</li><li>$ACC_{max}$: max pooling of entites belong to the same candidate</li><li>$f(\cdot)$: two-layers MLP with tanh</li></ul></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul><li>related works can be categorized as follows:<ul><li>graph-based<ul><li>coreference</li><li>co-occurrence</li><li>heterogeneous</li></ul></li><li>path-based</li><li>neural network based</li></ul></li><li>official leaderboard: <a href="http://qangaroo.cs.ucl.ac.uk/leaderboard.html" target="_blank" rel="noopener">http://qangaroo.cs.ucl.ac.uk/leaderboard.html</a></li></ul><div class="table-container"><table><thead><tr><th>Models</th><th>UnMask<br>Dev</th><th>UnMaks<br>Test</th><th>Mask<br>Dev</th><th>Maks<br>Test</th></tr></thead><tbody><tr><td>BiDAF</td><td>49.7</td><td>42.9</td><td>59.8</td><td>-</td></tr><tr><td>Coref-GRU</td><td>56.0</td><td>59.3</td><td>-</td><td>-</td></tr><tr><td>MHQA-GCN</td><td>62.6</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MHQA-GRN</td><td>62.8</td><td>65.4</td><td>-</td><td>-</td></tr><tr><td>Entity-GCN</td><td>64.8</td><td>67.6</td><td>-</td><td>-</td></tr><tr><td>CFC</td><td>66.4</td><td>70.6</td><td>-</td><td>-</td></tr><tr><td>Kundu.2018</td><td>67.1</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BAG</td><td>66.5</td><td>69</td><td>70.9</td><td>68.9</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> multi-hop </tag>
            
            <tag> gnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Recent Advances on Multi-Hop RC - HotpotQA</title>
      <link href="/2019/05/30/mrc-paper-hotpotqa/"/>
      <url>/2019/05/30/mrc-paper-hotpotqa/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><strong>HotpotQA</strong> is a dataset for <strong>Diverse</strong>, <strong>Explainable</strong> <strong>Multi-hop</strong> Question Answering.<br>Style: Extractive-based</p><p>HotpotQA has 4 features, which can also be seen as challenges:</p><ul><li>questions require finding and reasoning over multiple supporting documents to answer</li><li>questions are diverse and not constrained to any pre-existing knowledge</li><li>contains two types of questions: <ul><li>1.<strong>bridge</strong></li><li>2.<strong>comparison</strong> (factoid comparision questions)</li></ul></li><li>provides sentence-level supporting facts required for reasoning<ul><li>introduces the <strong>strong supervision</strong> for reasoning and make the final predictions <strong>explainable</strong></li><li>(can form a reasoning chain through the support facts)</li></ul></li></ul><p>Reference papers on HotpotQA task:</p><blockquote><ol><li>Cognitive Graph for Multi-Hop Reading Comprehension at Scale. ACL, 2019.</li><li>Dynamically Fused Graph Network for Multi-Hop Reasoning. ACL, 2019.</li><li>Answering while Summarizing: Multi-task Learning for Multi-Hop QA with Evidence Extraction. ACL, 2019.</li><li>Compositional Questions Do Not Necessitate Multi-hop Reasoning. ACL,2019. (short)</li><li>Multi-hop Reading Comprehension through Question Decomposition and Rescoring. ACL,2019.</li></ol></blockquote><h2 id="Task-Defination"><a href="#Task-Defination" class="headerlink" title="Task Defination"></a>Task Defination</h2><p>Input:</p><ul><li>query</li><li>$N_p = 10$ paragraphs</li></ul><p>Output</p><ul><li>answer span</li><li>supprot facts (sentence), which can be regarded as the supervised signal during training</li></ul><h2 id="Cognitive-Graph-QA"><a href="#Cognitive-Graph-QA" class="headerlink" title="Cognitive Graph QA"></a>Cognitive Graph QA</h2><blockquote><p>Cognitive Graph for Multi-Hop Reading Comprehension at Scale.<br>ACL, 2019.<br>Ming Ding (Chang Zhou)<br>THU (DAMO, Alibaba)</p></blockquote><h3 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><p>Dual Process Theory from the cognitive process of humans:</p><ul><li>System1 first retrieve relevant information following attention via an <strong>implicit, unconscious, intuitive</strong> process<ul><li>efficiently provides resources according to requests</li></ul></li><li>System2 conduct <strong>explicit, conscious, controllable</strong> reasoning process based on the result of System1<ul><li>enable diving deeper into relational information by performing sequential thinking in the working memory</li><li>slower but human-unique rationality</li></ul></li><li>For complex reasoning, two systems are coordinated to perform fast and slow thinking iteratively</li></ul><p>This work:</p><ul><li>System1 extracts question-relevant entities and answer candidates from paragraphs and encodes their semantic information<ul><li>Extracted entities are organized as a cognitive graph —&gt; working memory</li></ul></li><li>System2 conducts the reasoning procedure over the graph, and collects clues to guide System 1 to better extract next-hop entities.</li><li>iterate until all possible answers are found, and then the final answer is chosen based on reasoning results from System 2.</li></ul><h3 id="2-Model-Details"><a href="#2-Model-Details" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><p>Algorithm of Cognitive Graph QA</p><ul><li><img src="/images/mrc-paper-hotpotqa/Cog-algo.png" alt="alog"></li></ul><p>Model Overview</p><ul><li><img src="/images/mrc-paper-hotpotqa/Cog-model.png" alt="model"></li></ul><h3 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3.Experiments"></a>3.Experiments</h3><p>case study</p><ul><li><img src="/images/mrc-paper-hotpotqa/Cog-case.png" alt="case-study"></li></ul><h2 id="Dynamically-Fused-Graph-Network"><a href="#Dynamically-Fused-Graph-Network" class="headerlink" title="Dynamically Fused Graph Network"></a>Dynamically Fused Graph Network</h2><blockquote><p>Dynamically Fused Graph Network for Multi-Hop Reasoning.<br>ACL,2019.<br>Yunxuan Xiao<br>Shanghai Jiao Tong University. (ByteDance AT Lab)</p></blockquote><h3 id="1-Motivation-1"><a href="#1-Motivation-1" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><p><i class="fa fa-thumbtack"></i> Challenges:</p><ul><li>1) filtering out noises from multiple paragraphs and extracting useful information<ul><li>previous work: build entity graph from input paragraphs and apply GNN to aggregate the information</li><li>shortcomings: static global entity graph of each QA pair —&gt; <strong>implicit reasoning, lack of explainability</strong></li></ul></li><li>2) aggregate document information to an entity graph and answers are then <strong>directly</strong> selected on entities of the entity graph<ul><li>shortcomings: answers may not reside in entities of the extracted entity graph</li></ul></li></ul><p><i class="fa fa-highlighter"></i> This work:</p><ul><li>intuition: mimic human reasoning process in multi-hop QA<ul><li>start from an entity of interest in the query</li><li>focus on the words surrounding the start entities</li><li>connect to some related entity in the neighborhood guided by the question</li><li>repeat the step to form a reasoning chain, and lands on some entity or snippets likely to be answer.</li></ul></li><li>consturcts <strong>dynamic</strong> entity graph</li><li>propagating information on dynamic entity graph under soft-mask constraint</li><li><strong>bidirectional fusion</strong>:<ul><li>aggregate information from document to the entity graph and entity graph to document</li></ul></li></ul><h3 id="2-Model-Details-1"><a href="#2-Model-Details-1" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><h3 id="2-1-Graph-Constructing"><a href="#2-1-Graph-Constructing" class="headerlink" title="2.1 Graph Constructing"></a>2.1 Graph Constructing</h3><p>node: POL (<code>Persion</code>, <code>Organization</code>, <code>Location</code>) entities<br>edge:</p><ul><li>1) sentence-level: co-occurence</li><li>2) context-level: coreference</li><li>3) paragraph-level: link with central entities extracted from title sentence of each paragraph</li></ul><p><img src="/images/mrc-paper-hotpotqa/DFGN-graph.png" alt="dfgn-graph"></p><h3 id="2-2-Model"><a href="#2-2-Model" class="headerlink" title="2.2 Model"></a>2.2 Model</h3><p><img src="/images/mrc-paper-hotpotqa/DFGN-model.png" alt="dfgn-model"></p><p>Paragraph Selector</p><ul><li>use BERT sentence classification: [Q , Pi]</li><li>concat all selected Pi —&gt; C</li></ul><p>Encoder</p><ul><li>use BERT encoding concatenating of query and context performs better</li><li>output:<ul><li>query $Q_0 \in \mathbb{R}^{L\times d_2}$</li><li>context $C_0 \in \mathbb{R}^{M\times d_2}$</li></ul></li></ul><p><strong>Fusion Block</strong></p><ul><li>document to graph<ul><li>binary Matrix $M \in \mathbb{R}^{M \times N}$ : get text span associated with an entity</li><li>entity embedding: mean-max pooling $E_{t-1} = [e_{t-1,1},…,e_{t-1,N}] \in \mathbb{R}^{2d_2 \times N}$</li></ul></li><li>dynamic graph attention<ul><li>soft-mask: signify the start entities in the t-th reasoning step<ul><li>query vector $\tilde{q}^{t-1} = MeanPooling(Q^{t-1})$</li><li>masked entity $\tilde{E}^{t-1} = [m_1^t e_1^{t-1},…]$<ul><li><script type="math/tex; mode=display">\gamma_i^t = \tilde{q}^{t-1} V^t e_t^{t-1} / \sqrt{d_2}</script></li><li><script type="math/tex; mode=display">m^t = \sigma([\gamma_1^t,...,\gamma_N^t])</script></li></ul></li></ul></li><li>propagate information in graph by GAT (the more relevant to the query, the neighbor nodes receive more information from nearby)<ul><li>$e^t_i = ReLu( \sum_{j\in B_i} \alpha_{j,i}^t h_j^t)$<ul><li><script type="math/tex; mode=display">h_i^t = U_t \tilde{e}_i^{t-1} + b_t</script></li><li><script type="math/tex; mode=display">\beta_{i,j}^t = LeakyReLu(W_t^T [h_i^t, h_j^t])</script></li><li><script type="math/tex; mode=display">\alpha_{i,j}^t = \frac{exp(\beta_{i,j}^t)}{\sum_k exp(\beta_{i,k}^t)}</script></li></ul></li><li>$E_t = [e_1^t,…,e_N^t]$</li></ul></li></ul></li><li>updating query<ul><li>$Q^t = BiAttention(Q^{t-1}, E^t)$</li></ul></li><li>graph to document<ul><li>issue: the unrestricted answer still cannot be back traced</li><li>keep information flowing from entity back to tokens in the context<ul><li>the same matrix $M$</li><li>update the context embedding</li></ul></li><li>$C^t = LSTM( [C^{t-1}, {ME^t}^T] ) \in \mathbb{R}^{M\times d_2}$</li></ul></li></ul><p>Prediction Layer</p><ul><li>4 targets<ul><li>supporting sentences</li><li>start position of answer span</li><li>end position of answer span</li><li>answer type</li></ul></li><li>use four LSTMs to get final representation<ul><li>$O_{sup} = F_0 (C^t)$</li><li>$O_{start} = F_1([C^t, O_{sup}])$</li><li>$O_{end} = F_2([C^t, O_{sup}, O_{start}])$</li><li>$O_{type} = F_3([C^t, O_{sup}, O_{end}])$</li><li>$L = L_{start} + L_{end} + \lambda_s L_{sup} + \lambda_t L_{type}$</li></ul></li></ul><h3 id="3-Experiments-1"><a href="#3-Experiments-1" class="headerlink" title="3.Experiments"></a>3.Experiments</h3><h2 id="Query-Focused-Extractor"><a href="#Query-Focused-Extractor" class="headerlink" title="Query Focused Extractor"></a>Query Focused Extractor</h2><blockquote><p>Answering while Summarizaing: Multi-task Learning for Multi-hop QA with Evidence Extraction<br>ACL,2019.<br>Kosuke Nishida.<br>NTT Media Intelligence Laboratories.</p></blockquote><h3 id="1-Motivation-2"><a href="#1-Motivation-2" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><h3 id="2-Model-Details-2"><a href="#2-Model-Details-2" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><h3 id="3-Experiments-2"><a href="#3-Experiments-2" class="headerlink" title="3.Experiments"></a>3.Experiments</h3><h2 id="DecompRC"><a href="#DecompRC" class="headerlink" title="DecompRC"></a>DecompRC</h2><blockquote><p>Multi-hop Reading Comprehension through Question Decomposition and Rescoring<br>ACL,2019.<br>Sewon Min.<br>University of Washington. AI2.</p></blockquote><h3 id="1-Motivation-3"><a href="#1-Motivation-3" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models<ul><li>inspired by the idea of compositionality from semantic parsing</li></ul></li></ul><h3 id="2-Model-Details-3"><a href="#2-Model-Details-3" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><p>3 step process:</p><ul><li>decomposes the original, multi-hop question into several single-hop sub-questions according to a few reasoning types in parallel, based on span predictions.</li><li>for every reasoning types D ECOMP RC leverages a single-hop reading comprehension model to answer each sub-question, and combines the answers according to the reasoning type.</li><li>leverages a decomposition scorer to judge which decomposition is the most suitable, and outputs the answer from that decomposition as the ﬁnal answer.</li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> multi-hop </tag>
            
            <tag> gnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Commonsense Reasoning for Natural Language Understanding - A Survey of Benchmarks, Resources, and Approachs</title>
      <link href="/2019/04/18/mrc-cs-reasoning-for-nlu-survey/"/>
      <url>/2019/04/18/mrc-cs-reasoning-for-nlu-survey/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Authors</em>: Shane Storks, Qianzi Gao, Joyce Y. Chai<br><em>Org.</em>: Department of Computers Science and Engineering, Michigan State University<br><em>Year</em>: 2019<br><em>Paper Link</em>: <a href="https://arxiv.org/abs/1904.01172" target="_blank" rel="noopener">https://arxiv.org/abs/1904.01172</a></p></blockquote><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Commonsense Knowledge (CS Know.) 和 Commonsense Reasoning 是机器智能的两大重要瓶颈。</p><p>现有的NLP研究中，已经提出了一些需要常识推理的benchmarks和tasks，旨在评估机器获得和学习常识知识的能力。</p><p>这篇文章的主要目的是针对NLU的常识推理，提供关于以下四个方面的一个综述：</p><ul><li>现有的任务和benchmarks</li><li>Knowledge Resources</li><li><p>Learning and Inference Approachs</p></li><li><p>关于本篇文章的思维导图：</p><ul><li><img src="/images/mrc-cs-reasoning-for-nlu-survey/mind.png" alt="mind-note"></li></ul></li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>Davis and Marcus (2015)<sup><a href="#fn_davis" id="reffn_davis">davis</a></sup> 指出常识推理的挑战：spans from difficulties in <strong>understanding and formulating commonsense knowledge for specific or general domains</strong> to complexities in <strong>various forms of reasoning and their integration for problem solving</strong>.</p><p>现有的研究主要关注如下图所示的几个方面：(在这篇文章中主要关注文本数据源)</p><p><img src="/images/mrc-cs-reasoning-for-nlu-survey/overview.png" alt="image-overview"></p><h2 id="2-Benchmarks-and-Tasks"><a href="#2-Benchmarks-and-Tasks" class="headerlink" title="2.Benchmarks and Tasks"></a>2.Benchmarks and Tasks</h2><p>这章主要介绍一些需要常识推理的benchmarks，以及对构建这类benchmarks的重要要求进行一个总结。</p><ul><li>benchmarks数据集的发展：<ul><li><img src="/images/mrc-cs-reasoning-for-nlu-survey/benchmark-trend.png" alt="benchmark-trend"></li></ul></li></ul><h3 id="2-1-Overivew-of-ExistingBenchmarks"><a href="#2-1-Overivew-of-ExistingBenchmarks" class="headerlink" title="2.1 Overivew of ExistingBenchmarks"></a>2.1 Overivew of ExistingBenchmarks</h3><p>很多常识benchmark数据集都是基于classic language processing问题建立起的，从focused task (共指消解、命名实体识别) 到更理解性的任务和应用。</p><p>Benchmarks不应该局限于需要 language processing 能力提升性能的类型，应该更有针对性，更关注某类的常识知识和推理 (或是某几类的混合) 。</p><p>将Benchmarks分为6类，分别展开介绍</p><h4 id="2-1-1-Coreference-Resolution"><a href="#2-1-1-Coreference-Resolution" class="headerlink" title="2.1.1 Coreference Resolution"></a>2.1.1 Coreference Resolution</h4><p>共指消解是NLU中的一个基本任务，在句子中出现<strong>多个代名词或明显复杂的过程</strong>时，需要常识知识确定决策。</p><ul><li>代表数据集：Winograd Schema Challenge (<a href="http://commonsensereasoning.org/winograd.html" target="_blank" rel="noopener">link</a>)</li></ul><h4 id="2-1-2-Question-Answering"><a href="#2-1-2-Question-Answering" class="headerlink" title="2.1.2 Question Answering"></a>2.1.2 Question Answering</h4><p>相比于只关注某些特定的语言处理或是推理的任务，QA提供了一种在单个任务中更全面地混合语言处理和推理技巧的benchmark。</p><blockquote><p>contain questions requiring commonsense knowledge alongside question requiring comprehension of a given text.</p></blockquote><ul><li>代表数据集：ARC、MCScript、ProPara、MultiRC、SQuADv2、CoQA、OpenBookQA、CommonsenseQA<ul><li>ProPara：面向过程性文本，旨在学习目标的追踪和状态变化</li></ul></li></ul><h4 id="2-1-3-Textual-Entailment"><a href="#2-1-3-Textual-Entailment" class="headerlink" title="2.1.3 Textual Entailment"></a>2.1.3 Textual Entailment</h4><p>文本推理任务旨在推理两个句子之间的关系，需要多种语言处理能力（paraphrase）以及object tracking、causal reasoning和常识知识。</p><ul><li>代表数据集：RTE challenges、SICK、SNLI、SciTail</li><li>RTE knowledge resources: <a href="https://aclweb.org/aclwiki/RTE_Knowledge_Resources" target="_blank" rel="noopener">https://aclweb.org/aclwiki/RTE_Knowledge_Resources</a></li></ul><h4 id="2-1-4-Plausible-Inference"><a href="#2-1-4-Plausible-Inference" class="headerlink" title="2.1.4 Plausible Inference"></a>2.1.4 Plausible Inference</h4><p>似然推理：require hypothetical, intermediate or uncertain conclusions defined as plausible inference.</p><p>这类数据集关注的是everyday events和interactions，包含各种的实际的常识关系。</p><ul><li>代表数据集：COPA、CBT、ROCStories、JOCI、CLOTH、SWAG、ReCoRD</li></ul><h4 id="2-1-5-Psychological-Reasoning"><a href="#2-1-5-Psychological-Reasoning" class="headerlink" title="2.1.5 Psychological Reasoning"></a>2.1.5 Psychological Reasoning</h4><p>心理推理：关于情绪（情感）和意图的推理，需要社会心理学的常识知识</p><ul><li>代表数据集：Triangle-COPA、Story Commonsense、Event2Mind<ul><li>StoryCommonsense<sup><a href="#fn_rashkin-2018a" id="reffn_rashkin-2018a">rashkin-2018a</a></sup>：要求预测Motivation和Emotions，以及Maslow (human need)、Reiss (human motives)、Plutchik (emotions)<ul><li>link：<a href="http://uwnlp.github.io/storycommonsense" target="_blank" rel="noopener">http://uwnlp.github.io/storycommonsense</a></li><li>Theories of Motivation (Maslow and Reiss) and Emotional Reaction (Plutchik): <img src="/images/mrc-cs-reasoning-for-nlu-survey/MaslowReissPlutchik.png" alt="png"></li></ul></li><li>Event2Mind<sup><a href="#fn_rashkin-2018b" id="reffn_rashkin-2018b">rashkin-2018b</a></sup>：推理关于事件的intentions和reactions，每个事件都有1到2个参与者，三个任务：预测主要参与者的意图和反应，并预测其他人的反应<ul><li>link：<a href="http://uwnlp.github.io/event2mind" target="_blank" rel="noopener">http://uwnlp.github.io/event2mind</a></li></ul></li></ul></li></ul><h4 id="2-1-6-Multiple-Tasks"><a href="#2-1-6-Multiple-Tasks" class="headerlink" title="2.1.6 Multiple Tasks"></a>2.1.6 Multiple Tasks</h4><p>consist of several focused language processing or reasoning tasks so that reading comprehension skills can be learned one by one in a consistent format</p><ul><li>代表数据集：bAbI、IIE、GLUE、DNC<ul><li>IIE：Inference is Everything，RTE的形式</li><li>DNC <sup><a href="#fn_poliak" id="reffn_poliak">poliak</a></sup>：Diverse Natural Language Inference Collection，包含9个NLI任务需要7中不同类型的推理<ul><li>recast from：<ul><li>Event Factuality, recast from UW (Lee, Artzi, Choi, &amp; Zettlemoyer, 2015), MEANTIME (Minard, Speranza, Urizar, Altuna, van Erp, Schoen, &amp; van Son, 2016), and (Rudinger, White, &amp; Van Durme, 2018b)</li><li>Named Entity Recognition, recast from the Groningen Meaning Bank (Bos, Basile, Evang, Venhuizen, &amp; Bjerva, 2017) and the ConLL-2003 shared task (Tjong Kim Sang &amp; De Meulder, 2003)</li><li>Gendered Anaphora Resolution, recast from the Winogender dataset (Rudinger et al., 2018a)</li><li>Lexicosyntactic Inference, recast from MegaVeridicality (White &amp; Rawlins, 2018), VerbNet (Schuler, 2005), and VerbCorner (Hartshorne, Bonial, &amp; Palmer, 2013)</li><li>Figurative Language, recast from puns by Yang, Lavie, Dyer, and Hovy (2015) and Miller, Hempelmann, and Gurevych (2017)</li><li>Relation Extraction, partially from FACC1 (Gabrilovich, Ringgaard, &amp; Subramanya, 2013)</li><li>Subjectivity, recast from Kotzias, Denil, De Freitas, and Smyth (2015)</li></ul></li><li>link：<a href="http://github.com/decompositional-semantics-initiative/DNC" target="_blank" rel="noopener">http://github.com/decompositional-semantics-initiative/DNC</a></li></ul></li></ul></li></ul><h3 id="2-2-Criteria-and-Consideration-for-Creating-Benchmarks"><a href="#2-2-Criteria-and-Consideration-for-Creating-Benchmarks" class="headerlink" title="2.2 Criteria and Consideration for Creating Benchmarks"></a>2.2 Criteria and Consideration for Creating Benchmarks</h3><h4 id="2-2-1-Task-Format"><a href="#2-2-1-Task-Format" class="headerlink" title="2.2.1 Task Format"></a>2.2.1 Task Format</h4><p>决定任务形式对于Benchmarks的创建是重要的一步，现有的任务形式有三类：</p><ul><li>Classification Task：有三种形式<ul><li>Textual Entailment Task</li><li>Cloze Task</li><li>Traditional Multiple-Choice Task</li></ul></li><li>Open-ended Task：开放式任务<ul><li>Span</li><li>Subset of category labels：Story Commonsense</li><li>Purely open-ended：Event2Mind、bAbI</li><li><del>Generative</del></li></ul></li></ul><h4 id="2-2-2-Evaluation-Schemes"><a href="#2-2-2-Evaluation-Schemes" class="headerlink" title="2.2.2 Evaluation Schemes"></a>2.2.2 Evaluation Schemes</h4><p>评测形式</p><p>现有的评测结果都是直接给出是否通过(pass or fail grade)，没有任何反馈</p><p>理想的评测形式应该考虑有信息的指标，可以比较不同的方法，比较机器和人之间性能表现的差异</p><ul><li>Evaluation Metrics：Precision、Recall、F-Measure、Exact-Match、Recall@k、BLEU、ROUGE</li><li>Comparison of Approaches</li><li>Human Performance Measurement</li></ul><h4 id="2-2-3-Data-Biases"><a href="#2-2-3-Data-Biases" class="headerlink" title="2.2.3 Data Biases"></a>2.2.3 Data Biases</h4><p>数据分布的平衡</p><ul><li>Label Distribution Bias</li><li>Question Type Bias</li><li>Superficial Correlation Bias：gender bias</li></ul><h4 id="2-2-4-Collection-Methods"><a href="#2-2-4-Collection-Methods" class="headerlink" title="2.2.4 Collection Methods"></a>2.2.4 Collection Methods</h4><p>[Not Focus]</p><ul><li>Manual versus Automatic Generation</li><li>Automatic Generation versus Text Mining</li><li>Crowsourcing Considerations</li></ul><h2 id="3-Knowledge-Resources"><a href="#3-Knowledge-Resources" class="headerlink" title="3.Knowledge Resources"></a>3.Knowledge Resources</h2><h3 id="3-1-Overview-of-Knowledge-Resources-for-NLU"><a href="#3-1-Overview-of-Knowledge-Resources-for-NLU" class="headerlink" title="3.1 Overview of Knowledge Resources for NLU"></a>3.1 Overview of Knowledge Resources for NLU</h3><p>为了理解自然语言，通常需要语言学知识来却确定文本的句法、语义结构，再进一步使用通识、常识知识来增强对结构的理解，以达到更全面的理解</p><h4 id="3-1-1-Linguistic-Knowledge-Resources"><a href="#3-1-1-Linguistic-Knowledge-Resources" class="headerlink" title="3.1.1 Linguistic Knowledge Resources"></a>3.1.1 Linguistic Knowledge Resources</h4><p>带标记的句法、语义、篇章结构资源</p><ul><li>Annotated Linguistic Corpora<ul><li>Penn TreeBank：POS tags &amp; syntactic structures based on context-free grammar</li><li>PropBank：predicate-argument structures</li><li>Penn Discourse TreeBank</li><li>Abstract Meaning Representation (AMR)</li></ul></li><li>Lexical Resources<ul><li>WordNet</li><li>VerbNet：hierarchical English Verb lexicon</li><li>FrameNet：frame semantics for a set of verbs</li></ul></li></ul><h4 id="3-1-2-Common-Knowledge-Resources"><a href="#3-1-2-Common-Knowledge-Resources" class="headerlink" title="3.1.2 Common Knowledge Resources"></a>3.1.2 Common Knowledge Resources</h4><blockquote><p><strong><font color="blue">Common knowledge refers to speciﬁc facts about the world that are often explicitly stated.</font></strong></p></blockquote><p>与Commonsense Knowledge的不同是<sup><a href="#fn_cambria-2011" id="reffn_cambria-2011">cambria-2011</a></sup>：CS Know. required to achieve a deep understanding of both the low- and high-level concepts found in language.</p><ul><li>Yet Another Great Ontology (YAGO): with common knowledge facts extracted from Wikipedia, converting WordNet from a primarily linguistic resource to a common knowledge base.</li><li>DBpedia: Wikipedia-based knowledge base originally consisting of structured knowledge from more than 1.95 million Wikipedia articles.</li><li>WikiTaxonomy: consists of about 105,000 well-evaluated semantic links between categories in Wikipedia articles. Categories and relationships are labeled using the connectivity of the conceptual network formed by the categories.</li><li>Freebase</li><li>NELL</li><li>Probase</li></ul><h4 id="3-1-3-Commonsense-Knowledge-Resources"><a href="#3-1-3-Commonsense-Knowledge-Resources" class="headerlink" title="3.1.3 Commonsense Knowledge Resources"></a>3.1.3 Commonsense Knowledge Resources</h4><blockquote><p><strong><font color="blue">Commonsense knowledge, on the other hand, is considered obvious to most humans, and not so likely to be explicitly stated</font></strong></p><p>Cambria, E., Song, Y., Wang, H., &amp; Hussain, A. (2011). Isanette: A Common and Common Sense Knowledge Base for Opinion Mining. In 2011 IEEE 11th International Conference on Data Mining Workshops, pp. 315–322, Vancouver, BC, Canada. IEEE.</p></blockquote><ul><li>Cyc</li><li>ConceptNet</li><li>AnalogySpace<ul><li>is an algorithm for reducing the dimensionality of commonsense knowledge so that knowledge bases can be more efﬁciently and accurately reasoned over.</li></ul></li><li>SenticNet: intended for sentiment analysis</li><li>IsaCore: a set of “is a” relationships and conﬁdences.<ul><li><a href="http://sentic.net/downloads" target="_blank" rel="noopener">http://sentic.net/downloads</a></li></ul></li><li>COGBASE</li><li>WebChild</li><li>LocatedNear</li><li>Atlas of Machine Commonsense (ATOMIC)<ul><li>about 300,000 nodes corresponding to short textual descriptions of events, and about 877,000 “if-event-then” triples representing if-then relationships between everyday events.</li></ul></li></ul><h3 id="3-2-Approaches-to-Creating-Knowledge-Resources"><a href="#3-2-Approaches-to-Creating-Knowledge-Resources" class="headerlink" title="3.2 Approaches to Creating Knowledge Resources"></a>3.2 Approaches to Creating Knowledge Resources</h3><blockquote><p>The goal is to create general knowledge bases to <strong>provide inductive bias for a variety of learning and reasoning tasks</strong></p></blockquote><ul><li>Manual Encoding</li><li>Text Mining</li><li>Crowdsourcing</li></ul><h2 id="4-Learning-and-Inference-Approaches"><a href="#4-Learning-and-Inference-Approaches" class="headerlink" title="4.Learning and Inference Approaches"></a>4.Learning and Inference Approaches</h2><h3 id="4-1-Symbolic-and-Statistical-Approaches"><a href="#4-1-Symbolic-and-Statistical-Approaches" class="headerlink" title="4.1 Symbolic and Statistical Approaches"></a>4.1 Symbolic and Statistical Approaches</h3><h3 id="4-2-Neural-Approaches"><a href="#4-2-Neural-Approaches" class="headerlink" title="4.2 Neural Approaches"></a>4.2 Neural Approaches</h3><ul><li>common components in neural models: <ul><li><img src="/images/mrc-cs-reasoning-for-nlu-survey/neural-approachs.png" alt="neural"></li></ul></li></ul><h4 id="4-2-1-Memory-Augmentation"><a href="#4-2-1-Memory-Augmentation" class="headerlink" title="4.2.1 Memory Augmentation"></a>4.2.1 Memory Augmentation</h4><p>针对需要理解状态变化或是具有多个支撑事实来进行文本理解的任务</p><h5 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h5><ul><li>add a long-term memory component to track the world state and context</li><li>MemNet can efficiently leverage a wider context in making inferences<ul><li>outperform primarily RNN and LSTM based models</li></ul></li></ul><h5 id="Recurrent-Entity-Networks-ENTENT"><a href="#Recurrent-Entity-Networks-ENTENT" class="headerlink" title="Recurrent Entity Networks (ENTENT)"></a>Recurrent Entity Networks (ENTENT)</h5><blockquote><p>Henaff, M., Weston, J., Szlam, A., Bordes, A., &amp; LeCun, Y. (2017).<br>Tracking the World State with Recurrent Entity Networks. In Proceedings of the 5th International Conference on Learning Representations.<br>ICLR,2017.</p></blockquote><ul><li>composed of several dynamic memory cell<ul><li>each cell learns to represent the <strong>state</strong> or <strong>properties</strong> concerning entities mentioned in the input.</li><li>each cell is a Gated-RNN, only updates its content when new information relevant to the particular entity is received</li><li>run in parallel, allow multiple locations of memory to be updated at the same time.</li></ul></li><li>unlike MemNet:<ul><li>MemNet only preform reasoning when the entire supporting text and the question are processed and loaded to the memory.</li><li>when given a supporting text with multiple questions:<ul><li>ENTENT do not need to process the input text multiple times to answer these question.</li><li>MemNet need to re-process the whole input for each question.</li></ul></li></ul></li><li>drawbacks<ul><li>perform well in bAbI, but not in ProPara</li><li>maintain memory registers for entities, it has no separate embedding for individual states of entities over time</li><li>do not explicitly update coreferences in memory</li></ul></li></ul><h5 id="KG-MRC"><a href="#KG-MRC" class="headerlink" title="KG-MRC"></a>KG-MRC</h5><blockquote><p>Das, R., Munkhdalai, T., Yuan, X., Trischler, A., &amp; McCallum, A.<br>Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension.<br>ICLR, 2019</p></blockquote><p>Knowledge Graph Machine Reading Comprehension</p><ul><li>maintain a dynamic memory<ul><li>memory is in the form of knowledge graphs generated after every sentence of procedural text.</li></ul></li><li>generated knowledge graphs:<ul><li>are bipartite, connecting entities in the paragraph with their locations (currently, only capture the location relation)</li><li>connections between entities and locations are updated to generate a new graph after each sentence</li></ul></li><li>KG-MRC learns some commonsense constraints automatically.<ul><li>recurrent graph representations help.</li></ul></li></ul><h4 id="4-2-2-Attention-Mechanism"><a href="#4-2-2-Attention-Mechanism" class="headerlink" title="4.2.2 Attention Mechanism"></a>4.2.2 Attention Mechanism</h4><ul><li>automatically provides an alignment between inputs and outputs</li><li>have limitations when the alignment between inputs and outputs is not straightforward.</li><li>sequentail attention</li><li>self-attention</li><li>multi-head</li><li>comparison score function</li></ul><h4 id="4-2-3-Pre-Trained-Models-and-Representations"><a href="#4-2-3-Pre-Trained-Models-and-Representations" class="headerlink" title="4.2.3 Pre-Trained Models and Representations"></a>4.2.3 Pre-Trained Models and Representations</h4><ul><li>ELMO</li><li>GPT</li><li>BERT<ul><li>still far from human: <del>SciTail</del>、ReCoRD、OpenBookQA</li></ul></li><li>When to fine-tune<ul><li>sentence pair tasks</li></ul></li></ul><h3 id="4-3-Incorporating-External-Knowledge"><a href="#4-3-Incorporating-External-Knowledge" class="headerlink" title="4.3 Incorporating External Knowledge"></a>4.3 Incorporating External Knowledge</h3><ul><li>WordNet in Textual Entailment</li><li>ConceptNet in Commonsense Task</li><li>Main Problems：<ul><li>how to incorporate external knowledge in modern neural approaches</li><li>how to acquire relevant external knowledge</li></ul></li></ul><h2 id="5-Other-Related-Benchmarks"><a href="#5-Other-Related-Benchmarks" class="headerlink" title="5.Other Related Benchmarks"></a>5.Other Related Benchmarks</h2><ul><li>language-related tasks</li><li>visual benchmarks<ul><li>perception</li></ul></li></ul><h2 id="6-Discussion-and-Conclusion"><a href="#6-Discussion-and-Conclusion" class="headerlink" title="6.Discussion and Conclusion"></a>6.Discussion and Conclusion</h2><ul><li><p>two types of commonsense knowledge are considered fundamental for human reasoning and decision making:</p><ul><li>intuitive psychology：心理</li><li>intuitive physics：物理</li></ul></li><li><p>Challenges</p><ul><li>relation with humans: understanding <strong>how much Commonsense Knowledge is developed and acquire in humans</strong> and how they related to human Language Production and Comprehension may shed light on computational models for NLP</li><li>difficult to identify and formalize Commonsense Knowledge</li><li>disconnect between Commonsense Knowledge resources and approaches to tackle these benchmarks<ul><li>One likely reason is that these knowledge bases do not cover the kind of knowledge that is required to solve those tasks<ul><li>To address this problem, several methods have been proposed for <strong>leveraging incomplete knowledge bases</strong></li><li>Eg1 <strong>AnalogySpace</strong>：uses principle component analysis to make analogies to smooth missing commonsense axioms</li><li>Eg2 <strong>Memory Comparison Networks</strong>：allow machines to generalize over existing temporal relations in Knowledge Sources in order to acquire new relations</li></ul></li><li>jointly develop benchmark tasks and construct knowledge bases<ul><li>Event2Mind &amp; ATOMIC</li><li>CommonsenseQA &amp; ConceptNet</li></ul></li></ul></li><li>only learning superficial artifacts from the dataset<ul><li>obscure statistical biases — high preformance, but not actual reasoning</li></ul></li></ul></li></ul><blockquote id="fn_davis"><sup>davis</sup>. Commonsense reasoning and commonsense knowledge in artiﬁcial intelligence. Commun. ACM, 58(9), 92–103.<a href="#reffn_davis" title="Jump back to footnote [davis] in the text."> &#8617;</a></blockquote><blockquote id="fn_rashkin-2018a"><sup>rashkin-2018a</sup>. Modeling Naive Psychology of Characters in Simple Commonsense Stories. ACL,2018.<a href="#reffn_rashkin-2018a" title="Jump back to footnote [rashkin-2018a] in the text."> &#8617;</a></blockquote><blockquote id="fn_rashkin-2018b"><sup>rashkin-2018b</sup>. Event2Mind: Commonsense Inference on Events, Intents, and Reactions. ACL,2018.<a href="#reffn_rashkin-2018b" title="Jump back to footnote [rashkin-2018b] in the text."> &#8617;</a></blockquote><blockquote id="fn_poliak"><sup>poliak</sup>. Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation. EMNLP, 2018.<a href="#reffn_poliak" title="Jump back to footnote [poliak] in the text."> &#8617;</a></blockquote><blockquote id="fn_cambria-2011"><sup>cambria-2011</sup>. Isanette: A Common and Common Sense Knowledge Base for Opinion Mining. In 2011 IEEE 11th International Conference on Data Mining Workshops, pp. 315–322, Vancouver, BC, Canada. IEEE.<a href="#reffn_cambria-2011" title="Jump back to footnote [cambria-2011] in the text."> &#8617;</a></blockquote>]]></content>
      
      
      <categories>
          
          <category> MRC-Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> survey </tag>
            
            <tag> reasoning </tag>
            
            <tag> commonsense </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Memory Networks</title>
      <link href="/2019/04/17/paper-memory-network/"/>
      <url>/2019/04/17/paper-memory-network/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Memory Networks 是一种框架，在这个框架内部的每个module都可以根据特定任务的需要用不同的方式来实现（task-specific）。</p><p>本篇主要以 《End-to-End Memory Networks》 (2015, MemN2N) 对 Memory Network 的框架进行介绍。</p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><ul><li>记忆网络的核心是记忆模块，可以看做是一个知识存储器。</li><li>在学习的过程中，首先需要对这个存储器的内容进行插入或更新，然后在测试的时候依靠这个存储器中的信息对于答案进行推理判断，具体包含以下四个主要模块：<ul><li>I：输入特征映射<ul><li>将输入转换为内部特征的表示？</li><li>将输入映射到特征空间</li></ul></li><li>G：泛化<ul><li>得到新的输入时，对过去的记忆进行更新；</li><li>称为泛化的原因是：在整个过程中网络能够根据未来的某些特定需要压缩、泛化本身的记忆；</li></ul></li><li>O：输出特征映射<ul><li>根据当前的输入和记忆状态得到输出，输出的是内部特征表示的形式（以内部特征表示作为输出）</li></ul></li><li>R：响应<ul><li>将上一步中的输出转换为指定的响应格式；</li></ul></li></ul></li><li>具体过程：对于一个特定的输入：one-hop<ul><li>1、将转换为内部特征表示的形式；</li><li>2、根据输入更新记忆；</li><li>3、根据输入和记忆计算输出特征；</li><li>4、解码得到响应的结果；</li></ul></li><li>MemN2N 的模型结构图，左侧是单层结构，右侧是多层（3层）结构<ul><li><img src="/images/paper-memory-network/memn2n-model.png" alt="MemN2N-Architecture"></li><li>Multi-hop的计算过程：<ul><li><img src="/images/paper-memory-network/memn2n-arch-flow.png" alt="MemN2N-Architecture2"></li></ul></li><li>Memory Module:<ul><li><img src="/images/paper-memory-network/memn2n-memory-module.png" alt="MemN2N-Memory-Module"></li></ul></li></ul></li></ul><h2 id="Approach-Details"><a href="#Approach-Details" class="headerlink" title="Approach Details"></a>Approach Details</h2><ul><li>模型的输入输出：<ul><li>输入：inputs $x_1,…,x_n$ (会被存储到memory中，$x_i$是一个句子) 和 query $q$，词典大小为 $|V|$</li><li>输出：answer $a$</li></ul></li></ul><h3 id="Single-Layer"><a href="#Single-Layer" class="headerlink" title="Single Layer"></a>Single Layer</h3><ol><li>input memory representation，将输入映射到特征/memory空间<ul><li>将 $x_i$ 通过 input embedding $A \in \mathbb{R}^{d\times |V|}$ 映射为 memory vector $m_i$</li><li>将 $q$ 通过 question embedding $B \in \mathbb{R}^{d\times |V|}$ 映射为 internal state $u$</li></ul></li><li>计算每个 memory 和 query 之间的attention，得到匹配程度：<ul><li>$p_i = softmax(u^T m_i)$</li><li>有多少个 memory vector 就有多少个 $p$</li></ul></li><li>output memroy representation：<ul><li>再将 $x_i$ 通过 output embedding $C \in \mathbb{R}^{d\times |V|}$ 映射为相应的 output vector $c_i$</li><li>计算 response context vector $o$:<ul><li>$o = \sum_i p_i c_i$</li></ul></li></ul></li><li>generating final prediction:<ul><li>使用 $o$ 和 $u$ 一起预测答案标签（可以是一个词）</li><li>$\hat{a} = softmax(W(o+u))$</li></ul></li></ol><ul><li>模型中的主要训练参数为：$A$、$B$、$C$、$W$</li></ul><h3 id="Multiple-Layers"><a href="#Multiple-Layers" class="headerlink" title="Multiple Layers"></a>Multiple Layers</h3><p>多层的结构就是对memory进行多次寻址（addressing/attention），每次关注不同的memory，主要的几点不同是：</p><ul><li>第一层之后的每层/每个hop的 query vector 是前一层的 response context vector 和 query vector 的结合，可以用不同的结合方式计算：<ul><li>$u^{k+1} = u^k + o^k$</li></ul></li><li>每层之间的embedding矩阵$A^k$和$C^k$不是共享的，具体有两种 权重初始化方式，参考下面的weight typing。</li></ul><h4 id="Weight-Typing"><a href="#Weight-Typing" class="headerlink" title="Weight Typing"></a>Weight Typing</h4><p>每个embedding A 和 embedding C 都是与词典大小相等的词向量矩阵，在multiple layers的结构中引入这两个参数矩阵会带来很大的参数开销</p><ul><li>Adjacent方式<ul><li>使第$k+1$层的input embedding $A^{k+1}$ 等于 第$k$ 层的output embedding $C^{k}$：$A{k+1} = C^k$</li><li>还是增加其他的约束：<ul><li>(a) 用最后一层的output embedding $C^{K}$ 去对 answer prediction中的参数矩阵 $W$ 进行赋值：$W^T = C^K$</li><li>(b) 使 question embedding 等于 第一层的input embedding $A^1$：$B = A^1$</li></ul></li></ul></li><li>Layer-wise（RNN-like）方式<ul><li>不同的层之间使用相同的embedding参数，在层间加入一个线性映射 $H$ 来更新 $u$：$u^{k+1} = H u^k + o^k$</li><li>在这种方式下，整体模型可以看成一个传统的rnn，将rnn的输出分为 internal 和 external 两类，$u$ 是rnn的hidden state</li></ul></li></ul><h2 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h2><ul><li>Memory Networks</li><li>Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> note </tag>
            
            <tag> network </tag>
            
            <tag> memory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ICLR2019 | Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question Answering</title>
      <link href="/2019/04/11/paper-iclr2019-cfc/"/>
      <url>/2019/04/11/paper-iclr2019-cfc/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question Answering<br>ICLR 2019<br>Victor Zhong, Caiming Xiong, Nitish Shirish Keskar, Richard Socher<br>University of Washington, Salesforce Research<br>Datasets: Qangaroo-WikiHop</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Analysis of Multi-Passage RC Task</title>
      <link href="/2019/04/11/mrc-analysis-multi-passage/"/>
      <url>/2019/04/11/mrc-analysis-multi-passage/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Works of multi-passage MRC task<br>Target Datasets: MS MARCO, Dureader</p><p>Reference papers on this track:</p><blockquote><ol><li>Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification. ACL,2018.</li><li>A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension. EMNLP,2018.</li><li>A Deep Cascade Model for Multi-Document Reading Comprehension. AAAI,2019.</li><li>Multi-Mention Learning for Reading Comprehension with Neural Cascades. ICLR,2018.</li><li>Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question Answering. ICLR,2019.</li><li>Multi-style Generative Reading Comprehension. 2019.</li></ol></blockquote><p>TO BE Updated</p>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> research </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Analysis of Multi-Choice RC Task</title>
      <link href="/2019/03/28/mrc-analysis-multichoice/"/>
      <url>/2019/03/28/mrc-analysis-multichoice/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><ul><li>focus on the strategy of <strong>matching processing</strong> between <code>(P, Q, Ans)</code></li><li>target datasets: <strong>RACE, MCScripts</strong></li></ul><p>Reference papers on multi-choice MRC task, especially toward matching processing.</p><blockquote><ol><li>Hierarchical Attention Flow for Multiple-Choice Reading Comprehension. AAAI,2018.</li><li>Dynamic Fusion Networks for Machine Reading Comprehension. 2017.</li><li><strong>A Co-Matching Model for Multi-choice Reading Comprehension</strong>. ACL,2018.</li><li><strong>Dual Co-Matching Network for Multi-choice Reading Comprehension</strong>. 2019.</li><li>Convolutional Spatial Attention Model for Reading Comprehension with Multiple-Choice Questions. AAAI,2019.</li><li><strong>Option Comparison Network for Multiple-choice Reading Comprehension</strong>. 2019</li><li>Yuanfudao at SemEval-2018 Task 11: Three-way Attention and Relational Knowledge for Commonsense Machine Comprehension. 2018.</li><li>HFL-RC System at SemEval-2018 Task 11: Hybrid Multi-Aspects Model for Commonsense Reading Comprehension. 2018.</li></ol></blockquote><h2 id="Co-Match-Network-HCM"><a href="#Co-Match-Network-HCM" class="headerlink" title="Co-Match Network (HCM)"></a>Co-Match Network (HCM)</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>previous works: 之前的MRC的工作通常是基于句对的序列匹配（Pair-Wise Sequence Matching)，有如下情况：<ul><li>passage 与 question 和 candidate answer 的串联进行比较；</li><li>passage 先与 question 进行比较，计算出 matching 结果，再使结果与 candidate answer 进行比较；</li></ul></li><li><p>这样的计算方式不适用于多选型RC任务，具体存在以下几点问题：</p><ul><li>1、仅将 passage 和 question 进行匹配，得到的结果可能没有意义并且会导致原始 passage 的信息丢失；<ul><li>例如：问题 Which statement of the following is true？</li></ul></li><li>若将 question 和 candidate answer 串联成为一个序列，损失了 question 和 candidate answer 的交互信息；</li></ul></li><li><p>基于此，多选RC任务需要解决<strong>匹配序列三元组 (matching sequence triplets)</strong>的问题；</p></li><li>本文的方法：<ul><li>match a question-answer pair to a given passage；<ul><li>explicitly treat the question and the candidate answer as two sequences and jointly match them to the given passage；</li></ul></li><li>对P中的每个位置，都计算两个attention权重，构成两个匹配表示，形成一个co-match状态（同时计算P和Q/A的匹配），然后再用一个层次LSTM框架（2个LSTM）对passage进行编码；<ul><li>层次汇聚信息：<ul><li>在passage中的每个句子内部，信息从word-level汇聚在sentence-level</li><li>在passage中的句子序列维度上，再从sentence-level汇聚到document-level；</li></ul></li><li>可以更好的处理，问题需要的信息分散在passage中不同句子，的情况</li></ul></li></ul></li></ul><h3 id="Model-Details"><a href="#Model-Details" class="headerlink" title="Model Details"></a>Model Details</h3><p>Notation:<br>&nbsp;&nbsp;&nbsp;&nbsp;(one sentence in) Passage: $P\in \mathbb{R}^{d\times P}$<br>&nbsp;&nbsp;&nbsp;&nbsp;Question: $Q \in \mathbb{R}^{d\times Q}$<br>&nbsp;&nbsp;&nbsp;&nbsp;(one candidate answer in) Answer: $A \in \mathbb{R}^{d\times A}$</p><ul><li><p>architecture</p><ul><li><img src="/images/mrc-analysis-multichoice/co-match.png" alt="co-match"></li></ul></li><li><p>co-matching</p><ul><li>encoding: the same BiLSTM<ul><li>$H^p\in \mathbb{R}^{l\times P}$, 每个句子分别计算</li><li>$H^q\in \mathbb{R}^{l\times Q}$,</li><li>$H^a\in \mathbb{R}^{l\times A}$, 每个候选分别计算</li></ul></li><li>attention:<ul><li>$G^q = softmax( (W^gH^q + b^g \otimes e_Q)^T H^p ) \in \mathbb{R}^{Q\times P}$</li><li>$G^a = softmax( (W^gH^a + b^g \otimes e_Q)^T H^p ) \in \mathbb{R}^{A\times P}$</li></ul></li><li>aggregation: attentive passage representation<ul><li>$\bar{H}^q = H^q G^q \in \mathbb{R}^{l \times P}$</li><li>$\bar{H}^a = H^q G^a \in \mathbb{R}^{l \times P}$</li></ul></li><li>co-match passage state: concurrently matches a passage state with both the question and the candidate answer. It represent how each P state can be matched with the Q and Candidate A.<ul><li>$M^q = ReLU(W^m[\bar{H}^q \ominus H^p; \bar{H}^q \otimes H^p]) + b^m \in \mathbb{R}^{l\times P}$</li><li>$M^a = ReLU(W^m[\bar{H}^a \ominus H^p; \bar{H}^a \otimes H^p]) + b^m \in \mathbb{R}^{l\times P}$</li><li>$W^m \in \mathbb{R}^{l\times 2l}$</li><li>$C = [M^q; M^a] \in \mathbb{R}^{2l \times P}$</li></ul></li></ul></li><li>hierarchical aggregation<ul><li>for each triplet $\{P_n, Q, A\}, n\in [1,N]$, get $C_n$ through co-match</li><li>sentence-level aggregation of the co-matching states:<ul><li>sentence sequence representation merge into a single vector</li><li>$h_n^s = MaxPooling(BiLSTM(C_n)) \in \mathbb{R}^l$</li><li>$MaxPooling$： row-wise max pooling</li></ul></li><li>final triplet matching representation:<ul><li>$H^s=[h_1^s, h_2^s,…,h_N^s]$</li><li>$h^t = MaxPooling (BiLSTM (H^s)) \in \mathbb{R}^{l}$</li></ul></li></ul></li><li>Output Layer<ul><li>for each candidate answer $A_i$, get $h_i^t \in \mathbb{R}^{l} $</li><li>$L(A_i|P,Q) = -log \frac{exp(w^Th_i^t)}{\sum_{j=1}^4 exp(w^T h_j^t)}$</li></ul></li></ul><h3 id="Model-Parameters"><a href="#Model-Parameters" class="headerlink" title="Model Parameters"></a>Model Parameters</h3><ul><li>word emb dim: 300</li><li>rnn hidden dim: 150</li><li>optimizer: Adamax, lr=0.002</li><li>batch：10</li><li>epochs：30</li><li>dropout：0.2</li></ul><h2 id="Dual-Co-Matching-Network"><a href="#Dual-Co-Matching-Network" class="headerlink" title="Dual Co-Matching Network"></a>Dual Co-Matching Network</h2><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>previous work: <ul><li>只计算了question-aware P表示和 option-aware P表示；</li><li>一些pretrainLM的做法是将P和Q串联成为一个句子，A单独作为另一个句子；</li></ul></li><li>本文：<ul><li>model the relationship among passage，question and answer <strong>bidirectionally</strong></li><li>在计算question-aware P表示和 option-aware P表示的同时，计算passage-aware Q表示和passage-aware O表示</li></ul></li></ul><h3 id="Model-Details-1"><a href="#Model-Details-1" class="headerlink" title="Model Details"></a>Model Details</h3><ul><li>Encoding<ul><li>$H^p = Bert(P) \in \mathbb{R}^{P\times l}$</li><li>$H^q = Bert(Q) \in \mathbb{R}^{Q\times l}$</li><li>$H^a = Bert(A) \in \mathbb{R}^{A\times l}$</li><li>$l$: Bert hidden state dimension</li></ul></li><li>Matching Layer<ul><li>attention between P and A:<ul><li>$W = softmax(H^p(H^a G+b)^T) \in \mathbb{R}^{P\times A}$<ul><li>$G \in \mathbb{R}^{l\times l}$</li></ul></li><li>$M^p = WH^a \in \mathbb{R}^{P\times l}$</li><li>$M^a = W^TH^p \in \mathbb{R}^{A\times l}$<ul><li>$W \in \mathbb{R}^{P\times A}$</li></ul></li></ul></li><li>attention  between P and Q in the same method, get:<ul><li>$M^q \in\mathbb{R}^{Q\times l}$</li><li>$W^\prime \in \mathbb{R}^{P\times Q}$</li><li><font color="blue">问题：为什么P和Q进行attention，不计算question-aware的passage表示？</font></li></ul></li><li>integration original contextual representation<ul><li>$S^a = F([M^a - H^a;M^a \cdot H^a]W_1 + b_1) \in \mathbb{R}^{P \times l}$</li><li>$S^p = F([M^p - H^p;M^p \cdot H^p]W_2 + b_2)\in \mathbb{R}^{A \times l}$</li><li>$F()$ is activation function $ReLU$</li><li>in the question side:<ul><li>$S^{p^\prime} \in \mathbb{R}^{P\times l}$</li><li>$S^q \in \mathbb{R}^{Q\times l}$</li></ul></li></ul></li></ul></li><li>Aggregation Layer<ul><li>get final representation for each candidate answer<ul><li>row-wise max pooling</li><li>$C^p = Pooling(S^p) \in \mathbb{R}^{l}$</li><li>$C^a = Pooling(S^a) \in \mathbb{R}^{l}$</li><li>$C^{p^\prime} = Pooling(S^{p^\prime}) \in \mathbb{R}^{l}$</li><li>$C^q = Pooling(S^q) \in \mathbb{R}^{l}$</li><li>$C = [C^p;C^a;C^{p^\prime};C^q]$</li></ul></li></ul></li><li>Output Layer<ul><li>$L(A_i|P,Q)=-log\frac{exp(V^TC_i)}{\sum_{j=1}^N exp(V^TC_j)}$</li></ul></li></ul><h3 id="Model-Parameters-1"><a href="#Model-Parameters-1" class="headerlink" title="Model Parameters"></a>Model Parameters</h3><p> No description</p><h2 id="Option-Comparison-Network-OCN"><a href="#Option-Comparison-Network-OCN" class="headerlink" title="Option Comparison Network (OCN)"></a>Option Comparison Network (OCN)</h2><h3 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>previous work:<ul><li>read each option independently.</li><li>compute a fixed-length representation for each option before comparing them.</li></ul></li><li>ideas:<ul><li>humans typically compare the options at multiple-granularity level before reading the article in detail and make reasoning more efficient.</li><li>人解决多选RC任务的策略，通常在仔细阅读文章之前会在不同粒度上比较候选答案。</li><li>通过比较候选答案，可以定位答案选项间的相互关系，在读文章时只关注与选项相互关系有关的文章信息。（更高效？more efﬁciently and effectively）</li></ul></li><li>本文：<ul><li>explicitly compare options at word-level to better identify their correlations to help reasoning</li><li><ol><li>首先使用一个skimmer network对每个option进行独立编码；</li></ol></li><li><ol><li>然后对每个option，将其与其他的options使用attention进行word-level的比较，来建立option之间的相互比较；</li></ol></li><li><ol><li>最后，带着聚集之后的option间的相关性，重读文章，进行推理和答案选择</li></ol></li></ul></li><li>Analysis:<ul><li>这篇文章的主要更新的是option的表示</li></ul></li></ul><h3 id="Model-Details-2"><a href="#Model-Details-2" class="headerlink" title="Model Details"></a>Model Details</h3><p>Notation:<br>&nbsp;&nbsp;&nbsp;&nbsp;Passage: $P=\{w_1^p,…,w_m^p\}$<br>&nbsp;&nbsp;&nbsp;&nbsp;Question: $Q= \{w_1^q,…,w_n^q\}$<br>&nbsp;&nbsp;&nbsp;&nbsp;Answer set: $O=\{O_1,…,O_K\}$<br>&nbsp;&nbsp;&nbsp;&nbsp;Each option: $O_k = \{w_1^o,…,w_{n_k}^o\}$</p><ul><li><p>Overall: 4 stages</p><ol><li>concatenate each (article, question, option) triple into a sequence and use a skimmer to encode them into vector sequences.</li><li>attention-based mechanism is leveraged to compare the options.</li><li>the article is reread with the correlation information gathered in last stage as extra input.</li><li>compute the probabilities for each option.</li></ol></li><li><p>Option Feature Extraction</p><ul><li>skimmer encoding: 将每个option与P和Q串联构成一个句子，使用BERT进行编码<ul><li>$[P^{enc};Q^{enc};O^{enc}_k] = BERT(<p;q;o_k>)$<ul><li>$P^{enc} \in \mathbb{R}^{d\times m}$</li><li>$Q^{enc} \in \mathbb{R}^{d\times n}$</li><li>$O^{enc}_k \in \mathbb{R}^{d\times n_k}$</li></ul></p;q;o_k></li></ul></li><li>由于Q和option的关联紧密，将两者串联，作为option的特征<ul><li>$O_k^q=[Q^{enc}|O^{enc}_k] \in \mathbb{R}^{d\times n_k^\prime}$<ul><li>$n_k^\prime = n+n_k$</li></ul></li></ul></li></ul></li><li>Option Correlation Features Extraction<ul><li>$Att(\cdot)$的计算方式：假设输入为$U\in \mathbb{R}^{d\times N}$和 $V\in \mathbb{R}^{d\times M}$<ul><li>$v \in \mathbb{R}^{3d}$ 是参数</li><li>$s_{ij}=v^T[U_{:i};V_{:j};U_{:i}\circ V_{:j}]$</li><li>$A= Att(U,V;v)=[\frac{exp(s_{ij})}{\sum_i exp(s_{ij})}]_{ij} \in \mathbb{R}^{N\times M}$</li></ul></li><li>option correlation feature extraction 分3步进行<ul><li><ol><li>option $O_k$ 与其他options进行one-by-one比较，收集 pair-wise correlation信息<ul><li>$\bar{O}_k^{(l)}=O^q_l Att(O^q_l,O_k^q;v_o)$</li><li>$\tilde{O}_k^{(l)}=[O_k^q-\bar{O}_k^{(l)};O_k^q \circ \bar{O}_k^{(l)}] \in \mathbb{R}^{2d\times n_k^\prime}$</li></ul></li></ol></li><li><ol><li>gather pair-wise correlation information<ul><li>$\tilde{O}_k^c=tanh(W_c [O_k^q;\{\tilde{O}_k^{(l)}\}_{l\neq k} ])$<ul><li>$W_c \in \mathbb{R}^{d\times (d+2d(|O|-1))}$</li></ul></li></ul></li></ol></li><li><ol><li>element-wise gating 机制控制option feature和option-wise correlation information的融合，以产生option correlation features $O_k^c$<ul><li>$g_k \in \mathbb{R}^{d\times n_k^\prime}$<ul><li>$g_{k,:i}=\sigma (W_g [Q_{K,:i}^q; \tilde{O}_{k,:i}^c; \tilde{O}]+b_g)$</li><li>$g_{k,:i}$ 表示 g 向量的第i列</li></ul></li><li>$\tilde{O}$ 的计算：关于 Q 的attention pooling<ul><li>$A_q = softmax(v_a^T Q^{enc})^T, v_a \in \mathbb{R}^d$</li><li>$\tilde{O}=Q^{enc}A^q \in \mathbb{R}^{d}$</li></ul></li><li>option correlation features: $O_k^c\in \mathbb{R}^{d\times n_k^\prime}$<ul><li>$O_{k,:i}^c = g_{k,:i} \circ O_{k,:i}^q + (1-g_{k,:i}) \circ \tilde{O}_{k,:i}^c$</li><li>Note: $O_k^c$ 不被压缩成fixed-length向量，文中的解释为-这样可以使我们的模型更灵活的使用correlation信息。</li></ul></li></ul></li></ol></li></ul></li></ul></li><li>Article ReReading<ul><li>co-attention + self-attention</li><li>对于每个option $O_k$ 计算 co-attention:<ul><li>$A_k^c = Att(O_k^c,P^{enc};v_p) \in \mathbb{R}^{n_k^\prime \times m}$</li><li>$A_k^p = Att(P^{enc},O_k^c;v_p) \in \mathbb{R}^{m\times n_k^\prime}$</li><li>$\hat{O}_k^p = [P^{enc};O_k^c A_k^c]A_k^p \in \mathbb{R}^{2d\times n_k^\prime}$</li></ul></li><li>fused with correlation information<ul><li>$\tilde{O}_k^p = ReLU(W_p[O_k^c;\hat{O}_k^p]+b_p) \in \mathbb{R}^{d\times n_k^\prime}$</li></ul></li><li>self-attention to get full-info option representation $O_k^f\in \mathbb{R}^{d\times n_k^\prime}$<ul><li>$\tilde{O}_k^s = \tilde{O}_k^p Att(\tilde{O}_k^p, \tilde{O}_k^p;v_r)$</li><li>$\tilde{O}_k^f = [\tilde{O}_k^p;\tilde{O}_k^s;\tilde{O}_k^p-\tilde{O}_k^s;\tilde{O}_k^p \circ \tilde{O}_k^s]$</li><li>$O_k^f = ReLU(W_f\tilde{O}_k^f +b_f)$</li></ul></li></ul></li><li>Answer prediciton<ul><li>score $s_k = v_s^T MaxPooling(O_k^f)$<ul><li>MaxPooling: row-wise</li><li>$v_s \in \mathbb{R}^d$</li></ul></li><li>probability：<ul><li>$P(K|Q,P,O)=\frac{exp(s_k)}{\sum_i exp(s_i)}$</li></ul></li><li>loss:<ul><li>$J(\theta)=-\frac{1}{N}\sum_i log(P(\hat{k}_i | Q_i,P_i,O_i)) + \lambda||\theta||_2^2$</li></ul></li></ul></li></ul><h3 id="Model-Parameters-2"><a href="#Model-Parameters-2" class="headerlink" title="Model Parameters"></a>Model Parameters</h3><ul><li>for BERT base:<ul><li>batch:12</li><li>epochs:3</li><li>lr: $3\times 10^{-5}$ </li></ul></li><li>for BERT large:<ul><li>batch:24</li><li>epochs:5</li><li>lr: $1.5\times 10^{-5}$ </li></ul></li><li>$\lambda$: 0.01</li><li>lengths:<ul><li>P: 400</li><li>Q: 30</li><li>A: 16</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> research </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Practicable Course List (continually updated)</title>
      <link href="/2019/03/12/course-list/"/>
      <url>/2019/03/12/course-list/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>The main areas of concern are:</p><blockquote><p><a href="#NLP">Natural Language Processing</a><br><a href="#MLandDL">Machine Learning and Deep Learning</a><br><a href="#RL">Reinforcement Learning</a><br><a href="#MathBasic">Foundation of Mathematics</a></p></blockquote><h2 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h2><ul><li>Stanford，CS224n<ul><li><a href="http://web.stanford.edu/class/cs224n/index.html" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/index.html</a></li><li>video：<a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z" target="_blank" rel="noopener">Winter 2019</a></li></ul></li><li>CMU，CS 11-731<ul><li>Machine Translation and Seq-to-Seq Models，2018</li><li><a href="http://www.phontron.com/class/mtandseq2seq2018/" target="_blank" rel="noopener">http://www.phontron.com/class/mtandseq2seq2018/</a></li></ul></li><li>CMU，CS 11-747<ul><li>Neural Networks for NLP，Spring 2018</li><li>link:<ul><li><a href="http://www.phontron.com/class/nn4nlp2018/schedule.html" target="_blank" rel="noopener">2018</a></li><li><a href="http://phontron.com/class/nn4nlp2020/?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank" rel="noopener">spring,2020</a></li></ul></li><li>video：<a href="https://www.youtube.com/playlist?list=PL8PYTP1V4I8Ajj7sY6sdtmjgkt7eo2VMs" target="_blank" rel="noopener">2019</a></li></ul></li><li>Oxford<ul><li><a href="https://github.com/oxford-cs-deepnlp-2017/lectures" target="_blank" rel="noopener">https://github.com/oxford-cs-deepnlp-2017/lectures</a></li></ul></li><li>Berkeley<ul><li>Applied Natural Language Processing</li><li><a href="http://people.ischool.berkeley.edu/~dbamman/info256.html" target="_blank" rel="noopener">http://people.ischool.berkeley.edu/~dbamman/info256.html</a></li></ul></li><li>Penn，CIS 700<ul><li>Advanced Machine Learning for Natural Language Processing，Dan Roth</li><li><a href="http://www.cis.upenn.edu/~danroth/Teaching/CIS-700-006/index.html" target="_blank" rel="noopener">http://www.cis.upenn.edu/~danroth/Teaching/CIS-700-006/index.html</a></li></ul></li><li>CS 4650 and 7650<ul><li><a href="https://github.com/jacobeisenstein/gt-nlp-class" target="_blank" rel="noopener">https://github.com/jacobeisenstein/gt-nlp-class</a></li></ul></li><li>University of Washington<ul><li>CSE 447/547M: Natural Language Processing</li><li><a href="https://courses.cs.washington.edu/courses/cse447/19wi/" target="_blank" rel="noopener">https://courses.cs.washington.edu/courses/cse447/19wi/</a></li></ul></li><li>CS 521:  Statistical Natural Language Processing<ul><li>by Natalie Parde at the University of Illinois;</li><li>introduces advanced topics in statistical and neural NLP and provides an overview of active research in those topic areas</li><li><a href="http://www.natalieparde.com/teaching/cs521_spring2020.html?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank" rel="noopener">link</a></li></ul></li><li></li></ul><h2 id="MLandDL"><a href="#MLandDL" class="headerlink" title="MLandDL"></a>MLandDL</h2><ul><li>Stanford，CS229<ul><li><a href="http://cs229.stanford.edu/index.html#info" target="_blank" rel="noopener">http://cs229.stanford.edu/index.html#info</a></li></ul></li><li>李宏毅<ul><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses.html" target="_blank" rel="noopener">http://speech.ee.ntu.edu.tw/~tlkagk/courses.html</a></li></ul></li><li>Berkeley，STAT 157<ul><li>Introduction to Deep Learning</li><li><a href="http://courses.d2l.ai/berkeley-stat-157/index.html" target="_blank" rel="noopener">http://courses.d2l.ai/berkeley-stat-157/index.html</a></li></ul></li><li>Berkeley，2019，无监督学习<ul><li>CS294-158 Deep Unsupervised Learning Spring 2019</li><li><a href="https://sites.google.com/view/berkeley-cs294-158-sp19/home" target="_blank" rel="noopener">https://sites.google.com/view/berkeley-cs294-158-sp19/home</a></li></ul></li><li>Stanford，CS 236: Deep Generative Models<ul><li><a href="https://deepgenerativemodels.github.io/" target="_blank" rel="noopener">https://deepgenerativemodels.github.io/</a></li></ul></li><li>MIT，6.883<ul><li>Science of Deep Learning：Bridging Theory and Practice</li><li><a href="https://people.csail.mit.edu/madry/6.883/" target="_blank" rel="noopener">https://people.csail.mit.edu/madry/6.883/</a></li></ul></li><li>Maching Learning Summer School<ul><li><a href="http://mlss.cc/" target="_blank" rel="noopener">http://mlss.cc/</a></li></ul></li></ul><h2 id="RL"><a href="#RL" class="headerlink" title="RL"></a>RL</h2><ul><li>UCL，David Sliver<ul><li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a></li></ul></li><li>Berkeley S294-112<ul><li>Deep Reinforcement Learning</li><li><a href="http://rail.eecs.berkeley.edu/deeprlcourse/" target="_blank" rel="noopener">http://rail.eecs.berkeley.edu/deeprlcourse/</a></li></ul></li><li>Stanford<ul><li>CS234: Reinforcement Learning Winter 2019</li><li><a href="http://web.stanford.edu/class/cs234/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs234/</a></li></ul></li><li>Berkeley CS188<ul><li>Introduction to Artificial Intelligence</li><li><a href="https://inst.eecs.berkeley.edu/~cs188/sp19/" target="_blank" rel="noopener">https://inst.eecs.berkeley.edu/~cs188/sp19/</a></li></ul></li></ul><h2 id="MathBasic"><a href="#MathBasic" class="headerlink" title="MathBasic"></a>MathBasic</h2><ul><li>Stanford，CS229T/STATS231<ul><li>Statistical Learning Theory</li><li><a href="https://web.stanford.edu/class/cs229t/" target="_blank" rel="noopener">https://web.stanford.edu/class/cs229t/</a></li></ul></li><li>CMU，10-708，Probabilistic Graphical Models<ul><li>Eric Xing， Spring 2014</li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708/lecture.html" target="_blank" rel="noopener">http://www.cs.cmu.edu/~epxing/Class/10708/lecture.html</a></li></ul></li><li>NYU，MathsDL-spring18<ul><li>Topics course Mathematics of Deep Learning, NYU, Spring 18. CSCI-GA 3033.</li><li><a href="https://github.com/joanbruna/MathsDL-spring18" target="_blank" rel="noopener">https://github.com/joanbruna/MathsDL-spring18</a></li></ul></li></ul><h2 id="Recommend-Archive-Links"><a href="#Recommend-Archive-Links" class="headerlink" title="Recommend Archive Links"></a>Recommend Archive Links</h2><ul><li><a href="https://deep-learning-drizzle.github.io/index.html#contents" target="_blank" rel="noopener">https://deep-learning-drizzle.github.io/index.html#contents</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> ForStudy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> study </tag>
            
            <tag> resource </tag>
            
            <tag> course </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Knowledge-based MRC Papers</title>
      <link href="/2019/03/10/mrc-knowedge-paper-info/"/>
      <url>/2019/03/10/mrc-knowedge-paper-info/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>A list of recent papers with respect to Knowledge-based Machine Reading Comprehension.<br><a id="more"></a></p><p>This is the old collection version, please go to <a href="http://xingluxi.github.io/2019/10/23/mrc-knowledge-paper-list/">link</a> for the latest version.</p><style>    table th:nth-of-type(2){    width: 60%;    }</style><h2 id="Works-on-Knowledge-aware-MRC"><a href="#Works-on-Knowledge-aware-MRC" class="headerlink" title="Works on Knowledge-aware MRC"></a>Works on Knowledge-aware MRC</h2><!-- aware/based/enhanced --><div class="table-container"><table><thead><tr><th style="text-align:center">Conf.</th><th style="text-align:left">Title</th><th style="text-align:left">Authors/Org.</th><th style="text-align:center">Note</th></tr></thead><tbody><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="https://doi.org/10.18653/v1/P17-1132" target="_blank" rel="noopener">Leveraging knowledge bases in lstms for improving machine reading</a></td><td style="text-align:left">Yang, et al.<br>CMU</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="https://www.aclweb.org/anthology/D17-1216" target="_blank" rel="noopener">Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension</a></td><td style="text-align:left">Hongyu Lin, et al.</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/D17-1086" target="_blank" rel="noopener">World knowledge for reading comprehension: Rare entity prediction with hierarchical lstms using external descriptions</a></td><td style="text-align:left">Long, et al.<br>McGill University</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2018</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P18-1076" target="_blank" rel="noopener">Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge</a></td><td style="text-align:left">Mihaylov, et al.<br>Heidelberg University</td><td style="text-align:center"><a href="/2019/01/09/paper-knreader/" title="knreader-note">knreader-note</a></td></tr><tr><td style="text-align:center">2018</td><td style="text-align:left"><a href="https://openreview.net/forum?id=B1twdMCab" target="_blank" rel="noopener">Dynamic Integration of Background Knowledge in Neural NLU Systems</a></td><td style="text-align:left">Dirk Weissenborn, et al.</td><td style="text-align:center"><a href="/2019/03/06/paper-2018-refinewordemb/" title="note">note</a></td></tr><tr><td style="text-align:center">EMNLP<br>2018</td><td style="text-align:left"><a href="http://aclweb.org/anthology/D18-1454" target="_blank" rel="noopener">Commonsense for Generative Multi-Hop Question Answering Tasks</a></td><td style="text-align:left">Lisa Bauer</td><td style="text-align:center"><a href="/2019/02/21/paper-emnlp2018-mhpgm/" title="mhpgm-note">mhpgm-note</a></td></tr><tr><td style="text-align:center">AAAI<br>2018</td><td style="text-align:left"><a href="https://arxiv.org/abs/1811.00625" target="_blank" rel="noopener">Incorporating Structured Commonsense Knowledge in Story Completion</a></td><td style="text-align:left"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">(2018v1)<br>ACL<br>2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1809.03449" target="_blank" rel="noopener">Exploring Machine Reading Comprehension with Explicit Knowledge</a><br>Explicit Utilization of General Knowledge in Machine Reading Comprehension</td><td style="text-align:left">York University</td><td style="text-align:center"><a href="/2019/07/17/paper-acl2019-kar/" title="note">note</a></td></tr><tr><td style="text-align:center">ACL<br>2018</td><td style="text-align:left"><a href="https://aclweb.org/anthology/D18-1455" target="_blank" rel="noopener">Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text</a></td><td style="text-align:left">CMU</td><td style="text-align:center"><a href="/2019/07/08/paper-emnlp2018-graft-net/" title="note">note</a></td></tr><tr><td style="text-align:center">AAAI<br>2019</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1809.03568.pdf" target="_blank" rel="noopener">Improving Question Answering by Commonsense-Based Pre-Training</a></td><td style="text-align:left">MSRA</td><td style="text-align:center"><a href="/2019/07/06/paper-aaai2019-cs-based-pre-train/" title="note">note</a></td></tr><tr><td style="text-align:center">NAACL<br>2019</td><td style="text-align:left"><a href="https://www.aclweb.org/anthology/N19-1270" target="_blank" rel="noopener">Improving Machine Reading Comprehension with General Reading Strategies</a></td><td style="text-align:left">TencentAI Lab</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1902.00993" target="_blank" rel="noopener">Improving Question Answering with External Knowledge</a></td><td style="text-align:left"></td><td style="text-align:center"><a href="/2019/08/26/paper-2019-edl-md/" title="note">note</a></td></tr><tr><td style="text-align:center">CIKM<br>2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1908.04530" target="_blank" rel="noopener">Incorporating Relation Knowledge into Commonsense Reading Comprehension with Multi-task Learning</a></td><td style="text-align:left">Alibaba<br>DAMO</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2019</td><td style="text-align:left"><a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1226/" target="_blank" rel="noopener">Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension</a></td><td style="text-align:left">PKU<br>Baidu</td><td style="text-align:center"><a href="/2019/07/29/paper-acl2019-kt-net/" title="note">note</a></td></tr><tr><td style="text-align:center">EMNLP<br>2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1909.02151" target="_blank" rel="noopener">KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning</a></td><td style="text-align:left"></td><td style="text-align:center"><a href="/2019/09/09/paper-emnlp2019-kagnet/" title="note">note</a></td></tr><tr><td style="text-align:center">2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1908.06725v1" target="_blank" rel="noopener">Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models</a></td><td style="text-align:left">Alibaba<br>DAMO</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1909.05311" target="_blank" rel="noopener">Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering</a></td><td style="text-align:left">MSRA</td><td style="text-align:center"><a href="/2019/09/17/paper-csqa-1909-05311/" title="note">note</a></td></tr><tr><td style="text-align:center">2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1909.08855" target="_blank" rel="noopener">Exploring ways to incorporate additional knowledge to improve Natural Language Commonsense Question Answering</a></td><td style="text-align:left">Arizona State<br> University</td><td style="text-align:center"><a href="/2019/09/26/paper-kn-mcqa-1909-08855/" title="note">note</a></td></tr><tr><td style="text-align:center">EMNLP<br>2019</td><td style="text-align:left"><a href="http://arxiv.org/abs/1909.09253" target="_blank" rel="noopener">What’s Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering</a></td><td style="text-align:left">AI2</td><td style="text-align:center"><a href="/2019/10/06/paper-emnlp2019-miss-fact/" title="note">note</a></td></tr><tr><td style="text-align:center"></td><td style="text-align:left"></td><td style="text-align:left"></td></tr></tbody></table></div><h2 id="MRC-with-Knowledge"><a href="#MRC-with-Knowledge" class="headerlink" title="MRC with Knowledge"></a>MRC with Knowledge</h2><ul><li><del>how to let the machine obtain Knowledge？</del></li><li>how to extract external knowledge?</li><li>how to represent knowledge？ in which kind of format？</li><li>how to fuse the external knowledge?</li><li>how to let the machine to learn Knowledge incrementally？</li><li>how to make the machine can automatically use Knowledge it already knows or it has been told？</li></ul>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> paper </tag>
            
            <tag> mrc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2018 | Dynamic Integration of Background Knowledge in Neural NLU Systems</title>
      <link href="/2019/03/06/paper-2018-refinewordemb/"/>
      <url>/2019/03/06/paper-2018-refinewordemb/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p><em>Dynamic Integration of Background Knowledge in Neural NLU Systems</em><br><em>ICLR,2018. Reject</em><br><em>Dirk Weissenborn, et.al.</em><br><em>Datasets: SQuAD, TriviaQA, SNLI, MNLI</em></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>the requisite background knowledge is indirectly acquired from static corpora.</li><li>background knowledge learned from task supervision and also by pre-training word embeddings.</li><li>从静态的训练数据中获取背景知识有两点缺陷:<ul><li>1/不是所有的对解决NLU任务重要的背景知识都可以从有限量的训练数据中抽取出来；</li><li>2/随着时间的变化，对于理解文本有帮助的事实也会发生变化；</li></ul></li></ul><p>This work:（不同于仅依赖于从训练数据中获取静态知识）</p><ul><li>develop a new reading architecture for the <strong>dynamic integration</strong> of <strong>explicit background knowledge</strong> in NLU models.</li><li>a new <strong>task-agnostic(任务无关)</strong> reading module provides <strong>reﬁned word representations</strong> to a task-speciﬁc NLU architecture by processing background knowledge in the form of free-text statements, together with the task-speciﬁc inputs.</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>输入是：待理解的文本，即context，和抽取出的相关知识的assertions.<br>然后，使用 word embedding refinement 的策略，增量式地读入context和assertions，最初使用上下文无关的词向量仅初始化.<br>这种 contextually refined word embedding <strong>可以看成是一种动态记忆，用来存储新结合的知识</strong>. </p><h3 id="External-Knowledge-as-Supplementary-Text-Inputs"><a href="#External-Knowledge-as-Supplementary-Text-Inputs" class="headerlink" title="External Knowledge as Supplementary Text Inputs"></a>External Knowledge as Supplementary Text Inputs</h3><ul><li>结合知识的形式：<ul><li>本文中并不限制外部信息的形式：无结构/结构化知识都可以作为补充信息</li></ul></li><li>结合何种知识：<ul><li>从知识源中抽取上下文相关的信息本身就是个复杂的研究，并且依赖于知识库的形式</li><li>全面抽取所有潜在的assertions，然后依赖于我们的阅读结构来学习抽取相关的信息</li><li>Assertion Retrieval<ul><li>抽取知识是为了获得句子之间的关联</li><li>抽取出连接头/尾实体在text中，尾/头实体在question中的知识</li><li>由于抽取出的assertion过多，使用排序分数对assertions进行打分（类似于tf-idf的打分方式，针对的是罕见但是重要的知识，选择top-k个）</li></ul></li></ul></li></ul><h3 id="Refine-Word-Embeddings-by-Reading"><a href="#Refine-Word-Embeddings-by-Reading" class="headerlink" title="Refine Word Embeddings by Reading"></a>Refine Word Embeddings by Reading</h3><p>将词向量看做一种记忆，不仅包含通用的知识，还包含上下文信息和抽取的知识信息.</p><p>本文提出的增量式 refinement 过程编码输入文本，然后使用多个阅读步得到的编码输入来更新词向量矩阵.<br>过程如图：</p><ul><li><img src="/images/paper-2018-refinewordemb/fig-1.png" alt="refinement"></li></ul><p>Notations:</p><ul><li>$E^0$: 初始的词向量</li><li>$E^\ell$: 第$\ell$步更新的词向量</li><li>$X^\ell$: 第$\ell$步的上下文信息</li><li>$FC(z)=W z + b, W\in \mathbb{R}^{n \times m}, b\in \mathbb{R}^{n}, z\in \mathbb{R}^m$</li></ul><h4 id="1-Unrefined-Word-Embeddings"><a href="#1-Unrefined-Word-Embeddings" class="headerlink" title="1.Unrefined Word Embeddings"></a>1.Unrefined Word Embeddings</h4><p>这一步的目标是根据预训练词向量$e_w^p \in \mathbb{R}^{n^\prime}$得到初始的non-contextual词表示，计算如下：</p><ul><li>$e_w^{p^\prime} = ReLU(FC(e_w^p))$</li><li>$g_w = \sigma(FC([e_w^{p^\prime} ; e_w^{char}]))$</li><li>$e_w^0 = g_w \cdot e_w^{p^\prime} + (1-g_w) \cdot e_w^{char}$<br>其中，$e_w^{char}$是通过cnn编码（n convolutional ﬁlters w of width 5 followed by a max-pooling operation over time.）得到</li></ul><h4 id="2-Contextually-Refined-Word-Representation"><a href="#2-Contextually-Refined-Word-Representation" class="headerlink" title="2.Contextually Refined Word Representation"></a>2.Contextually Refined Word Representation</h4><p>在编码输入文本时，</p><ul><li>给每个词concatenate一个长度为L(即进行refienment处理的次数)的one-hot向量表示对应的位($\ell$)置1，</li><li>得到输入文本$X_i^{\ell} \in \mathbb{R}^{d\times |x_i^l|}$</li><li>经过lstm进行context编码: $\hat{X}_i^{\ell} = ReLU(FC(BiLSTM(X_i^{\ell})))$</li><li>在任务中：$X^1$相当于是Passage(Premise)文本的表示，$X^2$相当于是Question(Hypothesis)的表示，额外的知识assertions是$X^3$<ul><li>在实验中，p\q的顺序对最终的结果没有显著的影响</li></ul></li></ul><p>更新词向量：</p><ul><li>首先对所有在文本中与此词的lemma相同的词进行一个maxpool: <ul><li>$\hat{e}_w^{\ell} = max\{\hat{x}_k^{\ell} | x^{\ell} \in X^{\ell}, lemma(x_k^{\ell}) = lemma(w) \}$</li></ul></li><li>然后，用context-independent的表示去计算一个context-sensitive的表示<ul><li>通过门控机制，是模型决定利用多少新读入的信息来改写词向量</li><li>$u_w^{\ell} = \sigma(FC( [e_w^{\ell -1}; \hat{e}_w^{\ell}] ))$</li><li>$e_w^{\ell} = u_w^{\ell} \cdot e_w^{\ell -1} + (1- u_w^{\ell})\cdot \hat{e}_w^{\ell}$</li></ul></li><li>关于pooling操作：在具有相同lemma的词上面进行pooling操作<ul><li>有效的联系可以缓解长距离依赖问题</li><li>更充分的利用输入作为相关背景知识</li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>这篇文章的实验是在NLI（SNLI）和DQA（SQuAD）的任务上进行。</p><p>对NLI任务上：</p><ul><li>使用全部的数据进行训练时的提升不是很大</li><li>但是使用部分数据进行训练时的提升相对较多</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> nli </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACL2018 | Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering</title>
      <link href="/2019/03/03/paper-acl2018-slqa/"/>
      <url>/2019/03/03/paper-acl2018-slqa/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering<br>ACL 2018<br>Wei Wang, Ming Yan, Chen Wu.<br>Alibaba Group.<br>Multi-Granularity; Hierarchical Attention Fusion; Architecture<br>Datasets: SQuAD; TriviaQA</p></blockquote><p>看到题目，首先就产生了三个关注点：</p><ul><li>multi-granularity, 代表哪些粒度？ (word level 和 sentence level)</li><li>hierarchical, 有哪些层次？（co-attention 和 self-attention）</li><li>fusion, 怎样进行融合？对粒度的融合（global level）</li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>启发于人类通常的阅读模式：<ul><li>浏览全篇文章，大致了解文章内容（通读全文）</li><li>浏览问题，记住问题，找到P和Q之间的联系，理解Q的意图</li><li>定位一个大致/粗略的潜在的答案区域，使注意力聚集到定位的上下文（重点阅读上下文）</li><li>再次回顾问题，确定一个最优答案</li></ul></li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><ul><li>建模问题和文章中特定区域关联的同时，借助分层策略逐步集中注意力，是答案边界清晰</li><li>提出 hierarchical attention network：<ul><li>逐步定位答案边界</li><li>建模P和Q之间的不同粒度层级间的关系</li></ul></li><li>在Encoder层：<ul><li>为了更好的建模Q和P的多个aspects：同时使用预训练的glove表示和ELMo表示作为一个词的表示</li><li>针对ELMo的融合，设计了一个 Representation-aware fusion 方法来结合ELMo输出向量和BiLSTM建模的上下文表示</li></ul></li><li>在本文提出的Hierarchical Attention Fusion Network中：<ul><li>利用co-attention和self-attention机制，逐步的将注意力聚集到最优的答案span<ul><li>co-attention with shallow semantic fusion</li><li>self-attention with deep semantic fusion </li><li>memory-wise bilinear alignment function</li></ul></li><li>特点：<ul><li>fine-grained fusion 结合attention vector 更好的建模P和Q的关系</li><li>multi-granularity attention 应用于 word-level 和 sentence-level</li></ul></li><li>与其他方法不同的是：<ul><li>利用全局表示（原始的上下文表示）构建attention表示</li><li>利用fusion layer来对attention表示进行进一步的微调</li></ul></li></ul></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul><li>本文提出模型 SLQA+ (Semantic Learning for Question Answering) 的整体架构：<ul><li>encoder、attention、matching、output</li><li><img src="/images/paper-acl2018-slqa/slqa-model.png" alt="slqa+模型图"></li></ul></li></ul><h3 id="1-Encoder-Layer"><a href="#1-Encoder-Layer" class="headerlink" title="1.Encoder Layer"></a>1.Encoder Layer</h3><ul><li>输入是 P和Q的每个词的glove向量和ELMo向量：<ul><li>问题：$\{e_t^Q\}_{t=1}^m$，$\{c_t^Q\}_{t=1}^m$</li><li>文章：$\{e_t^P\}_{t=1}^n$，$\{c_t^P\}_{t=1}^n$</li></ul></li><li>输出：original context representation<ul><li>BiLSTM 的输出结果，再次和ELMo 串联concat</li><li>问题：Q = $u_t^Q = [BiLSTM_Q(e_t^Q, c_t^Q), c_t^Q]$</li><li>文章：P = $u_t^P = [BiLSTM_P(e_t^P, c_t^P), c_t^P]$</li></ul></li></ul><h3 id="2-Hierarchical-Attention-Fusion-Layer"><a href="#2-Hierarchical-Attention-Fusion-Layer" class="headerlink" title="2.Hierarchical Attention Fusion Layer"></a>2.Hierarchical Attention Fusion Layer</h3><ul><li>original context representation和通过attention得到的aligned representation可以反映上下文不同粒度的语义</li></ul><h4 id="2-1-Co-attention-amp-Fusion"><a href="#2-1-Co-attention-amp-Fusion" class="headerlink" title="2.1 Co-attention &amp; Fusion"></a>2.1 Co-attention &amp; Fusion</h4><ul><li>co-attention 过程：<ul><li>计算一个soft-alignment 矩阵：<ul><li><script type="math/tex; mode=display">S_{ij} = Att(u_t^Q, u_t^P) = ReLU(W^T_{lin}u_t^Q)^T \cdot ReLU(W^T_{lin}u_t^P)</script></li></ul></li><li>计算 P2Q attention<ul><li><script type="math/tex; mode=display">\alpha_{j} = softmax(S_{:j})</script></li><li><script type="math/tex; mode=display">\tilde{Q}_{:t} = \sum_j \alpha_{ij} \cdot Q_{:j}, \forall j \in [1,...,m]</script></li></ul></li><li>计算 Q2P attention<ul><li><script type="math/tex; mode=display">\beta_j = softmax(S_{i:})</script></li><li><script type="math/tex; mode=display">\tilde{P}_{k:} = \sum_i \beta_{ik} \cdot P_{i:}, \forall i \in [1,..,n]</script></li></ul></li></ul></li><li><font color="red"><strong>Fusion</strong></font>：本文重点<ul><li>定义fusion kernel<ul><li>$P^\prime=Fuse(P,\tilde{Q})$</li><li>$Q^\prime=Fuse(Q,\tilde{P})$</li></ul></li><li>simple fusion 过程：<ul><li><script type="math/tex; mode=display">m(P,\tilde{Q}) = tanh(W_f[P;\tilde{Q};P\circ\tilde{Q};P-\tilde{Q}] + b_f)</script></li></ul></li><li>输出：利用gate机制，融合/refine 注意力表示和original contextual 表示<ul><li>希望利用 original context representation 提供的重要的 global level 的语义信息提供指导，进一步引入gate机制控制不同层次表示的融合</li><li>文档：$P^\prime = g(P,\tilde{Q})\cdot m(P,\tilde{Q}) + (1-g(P, \tilde{Q})）\cdot P$</li><li>问题：$Q^\prime = g(Q,\tilde{P})\cdot m(Q,\tilde{P}) + (1-g(Q, \tilde{P})）\cdot Q$</li><li>$g(\cdot)$的定义见2.3</li></ul></li></ul></li></ul><h4 id="2-2-Self-attention-amp-Fusion"><a href="#2-2-Self-attention-amp-Fusion" class="headerlink" title="2.2 Self-attention &amp; Fusion"></a>2.2 Self-attention &amp; Fusion</h4><p>文档的self-attention fusion过程：</p><ul><li>首先将manual feature引入，与refined question-aware passage表示进行串接<ul><li>$D = BiLSTM([P^\prime; feat_{man}])$</li></ul></li><li>self-alignment <strong>fusion</strong> process<ul><li>$L = softmax(D \cdot W_1 \cdot D^T)$</li><li>$\tilde{D} = L \cdot D$</li><li>$D^\prime = Fuse(D,\tilde{D})$</li></ul></li><li>双向LSTM获得最终的上下文文档表示：<ul><li>$D^{\prime\prime} = BiLSTM(D^\prime)$</li></ul></li></ul><p>问题端的self-attention fusion过程</p><ul><li>获得新的问题上下文表示：$Q^{\prime\prime} = BiLSTM(Q^\prime)$<ul><li>$Q^\prime$ 来自于co-attention+fusion的结果</li></ul></li><li>linear self-alignment, 使用 linear transformation (linear sequence attention，同drqa中的问题编码)，将问题编码为向量<ul><li>$\gamma = softmax(w_q^T Q^{\prime\prime})$</li><li>$q = \sum_{j} \gamma_j \cdot Q^{\prime\prime}, \forall j \in [1,…,m]$</li></ul></li></ul><h4 id="2-3-Fusion-Functions"><a href="#2-3-Fusion-Functions" class="headerlink" title="2.3 Fusion Functions"></a>2.3 Fusion Functions</h4><ul><li>simple concat：简单讲两个channel的输入进行串联（计算m(·)时）</li><li>Full Projection：heuristic matching，形如<ul><li>$[P;\tilde{Q};P\circ\tilde{Q};P-\tilde{Q}]$</li></ul></li><li>Scalar-based fusion：训练一个标量参数<ul><li>$g(P,\tilde{Q}) = g_p$</li></ul></li><li><font color="green"><strong>Vector-based Fusion</strong></font>: 效果最好的<ul><li>$g(P,\tilde{Q}) = \sigma(w_g^T \cdot [P;\tilde{Q};P\circ\tilde{Q};P-\tilde{Q}] + b_g)$</li><li>$w_g$是待训练的权重向量</li></ul></li><li>Matrix-based Fusion：<ul><li>$g(P,\tilde{Q}) = \sigma(W_g^T \cdot [P;\tilde{Q};P\circ\tilde{Q};P-\tilde{Q}] + b_g)$</li><li>$W_g$是待训练的权重矩阵</li></ul></li></ul><h3 id="3-Output-Layer"><a href="#3-Output-Layer" class="headerlink" title="3.Output Layer"></a>3.Output Layer</h3><ul><li>bilinear match function<ul><li>$P_{start} = softmax(q \cdot W_s^T D^{\prime\prime})$</li><li>$P_{end} = softmax(q \cdot W_e^T D^{\prime\prime})$</li><li>可以看做是对问题的回顾</li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><ul><li>ELMo 与上下文表示融合的新思路</li><li><strong>原始的context representation可以看做是global level的信息，在fusion中的作用很大</strong>(每一步的fusion都是将attention representation与original context representation进行融合)，利用original context representation对attention之后的表示进行融合，微调带有注意力的表示</li><li>计算attention时，采取 linear + relu 的bilinear的效果最优</li><li>输出层的 <font color="blue"><strong>bilinear match function</strong></font> 对结果的提升很大</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EMNLP2018 | Commonsense for Generative Multi-Hop Question Answering Tasks</title>
      <link href="/2019/02/21/paper-emnlp2018-mhpgm/"/>
      <url>/2019/02/21/paper-emnlp2018-mhpgm/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Commonsense for Generative Multi-Hop Question Answering Tasks<br>EMNLP 2018<br>Lisa Bauer et al. UNC.<br>Multi-Hop reasoning; generative-mrc; commmonsense knowledge<br>Datasets: NarrativeQA; QAngaroo-WikiHop<br>Source code: <a href="https://github.com/yicheng-w/CommonSenseMultiHopQA" target="_blank" rel="noopener">https://github.com/yicheng-w/CommonSenseMultiHopQA</a></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>Multi-Hop Generative Task (<em>e.g. NarrativeQA</em>) requires models to <strong>reasong, gather and synthesize disjoint pieces of information</strong> within the context to generate an answer.</li><li>This type of multi-step reasoning requires understanding <strong>implicit relations</strong> (<em>external, background commonsense knowledge</em>).</li><li>Related work:<ul><li>some <strong>fact-based</strong> datasets (e.g. SQuAD) do not need to place heavy emphasis on multi-step reasoning capabilities.</li><li>some multi-hop datasets (e.g. QAngaroo) prompt a strong focus on multi-hop reasoning in very long texts.<ul><li>QAngaroo is an extractive dataset where answers are guaranteed to be spans within the context, thus, it more focuse on <strong>fact finding</strong> and <strong>linking</strong>.</li></ul></li></ul></li><li>This work:<ul><li>a. a strong generative baseline, <strong>Multi-Hop Pointer-Generator Model</strong>, uses a <strong>multi-attention</strong> to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer.</li><li>b. a novel system for <strong>selecting grounded multi-hop relational commonsense information</strong> from ConceptNet via a <strong>pointwise mutual information</strong> and <strong>term-frequency</strong> based scoring function.</li><li>c. a <strong>selectively gated attention</strong> mechanism to insert <strong>selected commonsense paths</strong> between the hops of document-context reasoning.</li><li>multi-hop commonsense paths: multiple connected edges within ConceptNet graph that give us information beyond a single relationship triple.</li><li>different aspects of the commonsense relationship path at each hop to bridge different inference gaps in the multi-hop task.</li></ul></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul><li>Model Overview:<ul><li><img src="/images/paper-emnlp2018-mhpgm/mhpgm-1-overview.png" alt="MHPGM"></li></ul></li></ul><h3 id="1-Multi-Hop-Pointer-Generator-Baseline"><a href="#1-Multi-Hop-Pointer-Generator-Baseline" class="headerlink" title="1.Multi-Hop Pointer-Generator Baseline"></a>1.Multi-Hop Pointer-Generator Baseline</h3><p>模型的输入:</p><ul><li>context: $X^C={w_1^C, w_2^C,…,w_n^C}$</li><li>query: $X^Q = {w_q^Q, w_2^Q,…,w_m^Q}$</li></ul><p>模型的输出:</p><ul><li>answer tokens: $X^a = {w_1^a,w_2^a,…,w_p^a}$</li></ul><h4 id="1-1-Embedding-Layer"><a href="#1-1-Embedding-Layer" class="headerlink" title="1.1.Embedding Layer"></a>1.1.Embedding Layer</h4><p>pretrained word embeddings with ELMO: $e_i^Q$/$e_i^C$ $\in \mathbb{R}^{d+1024}$</p><h4 id="1-2-Reasoning-Layer"><a href="#1-2-Reasoning-Layer" class="headerlink" title="1.2.Reasoning Layer"></a>1.2.Reasoning Layer</h4><p>$k$ 个 reasoning cell 增量式地更新 context representation.</p><p>第 $t$ 个reasoning cell的计算过程:</p><ul><li>输入: 前一时刻的输出 $\lbrace c_i^{t-1} \rbrace _{i=1}^n$ 和 query 的向量 $\lbrace e_i^Q \rbrace _{i=1}^m$</li><li>a.通过 cell-specific Bi-LSTM 计算一个 step-specific 的 context 和 query 的编码<ul><li>$u^t=BiLSTM(c^{t-1})$; $v_t=BiLSTM(e^Q)$</li></ul></li><li>b.通过bi-attention 计算 context 中的相关方面 来模拟一个hop的推理:<ul><li>context-to-query attention:<ul><li>$S_{ij}^t = W_1^t u_i^t + W_2^t v_j^t + W_3^t (u_i^t \odot v_j^t)$</li><li><script type="math/tex; mode=display">p_{ij}^t = \frac{exp(S_{ij}^t)}{\sum_{k=1}^m exp(S_{ik}^t)}</script></li><li><script type="math/tex; mode=display">(c_q)^t_i = \sum_{j=1}^m p_{ij}^t v_j^t \in \mathbb{R}^{n \times dim}</script></li></ul></li><li>query-to-context attention vector:<ul><li>$m_i^t = \max_{1 \leq j\leq m} S_{ij}^t$</li><li><script type="math/tex; mode=display">p_i^t = \frac{exp(m_i^t)}{\sum_{j=1}^n exp(m_j^t)}</script></li><li><script type="math/tex; mode=display">q_c^t = \sum_{i=1}^n p_i^t u_i^t \in \mathbb{R}^{dim}</script></li></ul></li><li>更新的context representation:<ul><li>$c_i^t = [u_i^t; (c_q)_i^t; u_i^t \odot (c_q)_i^t; q_c^t \odot (c_q)_i^t]$</li></ul></li><li>$c^0 = e^C$</li><li>最后一时刻的输出是 $c^k$</li></ul></li></ul><h4 id="1-3-Self-Attention-Layer"><a href="#1-3-Self-Attention-Layer" class="headerlink" title="1.3.Self-Attention Layer"></a>1.3.Self-Attention Layer</h4><ul><li>帮助模型处理long context中的长期依赖</li><li>输入是 reasoning 层最后一时刻的输出 经过 一个BiLSTM之后的表示 $c^{SA}$</li><li>计算流程：<ul><li>$S_{ij}^{SA} = W_4 c_i^{SA} + W_5 c_j^{SA} + W_6(c_i^{SA}\odot c_j^{SA})$<!-- * $$p_{ij}^{SA} = \frac{exp(S_{ij}^{SA})}{\sum_{k=1}^n exp(S_{ij}^{SA})}$$ --></li><li>$p_{ij}^{SA} = exp(S_{ij}^{SA}) / \left( \sum_{k=1}^n exp(S_{ij}^{SA}) \right)$</li><li>$c^\prime = \sum_{j=1}^n p_{ij}^{SA} c_j^{SA}$</li><li>$c^{\prime\prime} = BiLSTM([c^\prime;c^{SA};c^\prime \odot c^{SA}])$</li><li>$c = c^k + c^{\prime\prime}$</li></ul></li></ul><h4 id="1-4-Pointer-Generator-Decoding-Layer"><a href="#1-4-Pointer-Generator-Decoding-Layer" class="headerlink" title="1.4.Pointer-Generator Decoding Layer"></a>1.4.Pointer-Generator Decoding Layer</h4><ul><li><p>输入: </p><ul><li>$x_t$, 前一时刻解码出的词向量表示</li><li>$s_{t-1}$, 前一时刻的隐藏层状态</li><li>$a_{t-1}$, 上下文向量</li></ul></li><li><p>计算:</p><ul><li>当前时刻的隐藏层状态:<ul><li>$s_t = LSTM([x_t; a_{t-1}], s_{t-1})$</li></ul></li><li>计算在生成词典上的概率分布:<ul><li>$P_{gen} = softmax(W_{gen}s_t + b_{gen})$</li></ul></li><li>计算 attention (使用 Bahdanau 的attention计算过程):<ul><li><script type="math/tex; mode=display">\alpha_i = v^\intercal tanh(W_c c_i + W_s s_t + b_{attn})</script></li><li><script type="math/tex; mode=display">\hat{\alpha}_i = \frac{exp(\alpha_i)}{\sum_{j=1}^n exp(\alpha_j)}</script></li><li><script type="math/tex; mode=display">\mathbf{a}_i = \sum_{i=1}^n \hat{\alpha}_i c_i</script></li></ul></li><li>计算选择生成还是复制的概率:<ul><li>$\mathbf{o} = \sigma(W_a \mathbf{a}_t + W_x x_{t} + W_s s_t + b_{ptr})$</li><li>$\mathbf{p}^{sel} = softmax(\mathbf{o}) \in \mathbb{R}^2$</li></ul></li><li>最终 $t$ 时刻输出的分布为:<ul><li><script type="math/tex; mode=display">P_t(w) = p_1^{sel} P_{gen}(w) + p_2^{sel} \sum_{i:w_i^C=w} \hat{\alpha}_i</script></li></ul></li></ul></li></ul><h3 id="2-Commonsense-Selection-and-Representation"><a href="#2-Commonsense-Selection-and-Representation" class="headerlink" title="2.Commonsense Selection and Representation"></a>2.Commonsense Selection and Representation</h3><p>为什么需要常识知识：知识关系有时候没有直接在文本中指出</p><p>由于语义网络/知识图谱的规模较大，包含很多无关信息，需要设计有效的选择算法来确定需要的信息 (有用的知识且可以在context-query对中落地(<em>grounded</em>:在context-query中出现) )</p><ul><li><p>常识知识选择策略：包含两方面</p><ul><li>1.通过<strong>树结构</strong>，收集潜在的相关知识，目的是选择出具有high recall的候选推理路径；</li><li>2.通过<strong>三步</strong>打分策略对候选路径进行排序和过滤，以确保加入信息的质量和多样性；<ul><li>Initial Node Scoring, Cumulative Node Scoring, Path Selection</li></ul></li></ul></li><li><p>图: Commonsense selection approach</p><ul><li><img src="/images/paper-emnlp2018-mhpgm/mhpgm-2-path.png" alt="Commonsense selection approach"></li></ul></li></ul><h4 id="2-1-Tree-Construction"><a href="#2-1-Tree-Construction" class="headerlink" title="2.1.Tree Construction"></a>2.1.Tree Construction</h4><p>树的根节点为query中的一个词, 通过一些分支操作来构建多步推理路径.针对问题中的一个词/concept $c_1$, 进行如下操作:</p><ol><li>Direct Interaction:方向交互<ul><li>从ConceptNet中选择与$c_1$和文本上下文中concept( $c_2 \in C$ )有直接链接的关系$r_1$(多个)</li><li>例如图中的第一列</li></ul></li><li>Multi-Hop<ul><li>继续在ConceptNet中选择文本中另外的concept($c_3 \in C$)与$c_2$有链接的关系$r_2$</li></ul></li><li>Outside Knowledge<ul><li>无文本上下文的约束,寻找收集外部知识</li><li>在ConceptNet中通过关系$r_3$寻找$c_3$的邻居(one-hop)得到 $c_4 \in nbh(c_3)$</li></ul></li><li>Context-Grounding<ul><li>再次利用context进行约束, 来确保3中额外的知识是对任务有帮助的</li><li>即, 使$c_3$通过$c_4$可找到的二阶邻居$c_5 \in C$是在文本中出现的</li></ul></li></ol><ul><li>这构建路径的过程中，有几点疑问:<ul><li>a. 如果针对一个query中的concept无法查找到这么长的路径如何处理？</li></ul></li></ul><h4 id="2-2-Rank-and-Filter"><a href="#2-2-Rank-and-Filter" class="headerlink" title="2.2.Rank and Filter"></a>2.2.Rank and Filter</h4><p>在构建树的过程中, 会收集大量的潜在相关路径, 相应地会引入很多噪声以及与问题无关的路径, 所以需要对2.1中收集到的知识路径进行排序过滤掉噪音.</p><ol><li>Initial Node Scoring:<ul><li>初始化节点分数</li><li>选择有节点可以提供对context的理解的重要信息的路径</li><li>使用 tf 来估计context中concept的重要度和显著度:<ul><li>$score(c) = \frac{count(c)}{|C|}, c \in \lbrace c_2,c_3,c_5 \rbrace$</li></ul></li><li>对于outside Knowledge选择出来的节点$c_4$, 希望它与推理路径中的$c_1$到$c_3$保持<strong>逻辑一致性</strong><ul><li>利用点互信息</li><li><script type="math/tex; mode=display">PMI(c_4,c_{1-3}) = log(\frac{\mathbb{P}(c_4,c_{1-3})}{\mathbb{P}(c_4) \mathbb{P}(c_{1-3})})</script></li><li><script type="math/tex; mode=display">\mathbb{P}(c_4,c_{1-3}) = \frac{Num.\ of\ paths\ connecting\ c_1,c_2,c_3,c_4}{Num.\ of\ distinct\ paths\ of\ length\ 4}</script></li><li><script type="math/tex; mode=display">\mathbb{P}(c_4) = \frac{Num.\ of\ nodes\ that\ can\ reach\ c_4}{|ConceptNet|}</script></li><li><script type="math/tex; mode=display">\mathbb{P}(c_{1-3}) = \frac{Num.\ of\ paths\ connecting\ c_1,c_2,c_3}{Num.\ of\ paths\ of\ length\ 3}</script></li><li>由于PMI对low-frequency敏感, 改进为 normalized PMI<ul><li>$score(c_4) = PMI(c_4, c_{1-3}) / (-log\mathbb{P}(c_4,c_{1-3}))$</li></ul></li></ul></li><li>由于每个连接分支代表了one-hop，所以不同层级的hops或者是拥有不同父节点的nodes无法与其他节点相比较, 最终对每个节点的打分进行归一化:<ul><li>在同级(siblings)节点中进行归一化</li><li>$n-socre(c) = softmax_{siblings(c)}(score(c))$</li></ul></li></ul></li><li>Cumulative Node Scoring<ul><li>累积节点打分</li><li>选择多跳的Commonsense路径中有相关信息的,根据节点以其子节点的相关性和显著性对节点进行再次打分</li><li>采取bottom-up的累积计算方式</li><li>对于叶子节点(leaf node):<ul><li>$c-socre = n-score$</li></ul></li><li>对于非叶子节点<ul><li>$c-score(c_l) = n-socre(c_l) + f(c_l)$</li><li>$f(c_l)$是该节点的$c-socre$打分top-2的子节点的$c-score$平均分值</li></ul></li></ul></li><li>Path Selection<ul><li>路径选择</li><li>采取top-down breath-first(广度优先)的方式选择路径</li><li>从根节点(回顾: query中的一个concept)开始, 递归的选择其两个具有最高累计得分的子节点, 直到选择到叶子节点.</li><li>选择路径数: $2^4=16 paths$</li><li>路径直接以token序列的方式传给模型</li></ul></li></ol><h3 id="3-Commonsense-Model-Incorporation"><a href="#3-Commonsense-Model-Incorporation" class="headerlink" title="3.Commonsense Model Incorporation"></a>3.Commonsense Model Incorporation</h3><p>有选择性的结合需要的知识，使用常识知识来弥补推理步之间的gaps.<br>提出 Necessary and Optional Information Cell (NOIC) 一种 selectively gated 注意力机制。</p><ul><li><p>输入:以词序列的形式表示给定的 list of commonsense logic paths:</p><ul><li>$X^{CS} = {w_1^{CS}, w_2^{CS},…,w_l^{CS}}$</li><li>$w_l^{CS}$表示构成一条路径的token序列</li><li>使用词向量表示: $e_{ij}^{CS}\in \mathbb{R}^d$</li></ul></li><li><p>NOIC 是基于 Baseline Reasoning Cell 的扩展:</p><ul><li>在第 $t$ 个推理步, 得到 baseline reasoning cell 的输出之后, 为Commonsense信息计算cell-specific表示:<ul><li>将Commonsense paths上的所有向量串接, 每条路径得到一个表示向量$u_i^{CS}$</li><li>通过映射进行降维, 使维度与 $c_i^t$ 的维度相等<ul><li>$v_i^{CS} = ReLU(W u_i^{CS} + b)$</li></ul></li><li>利用attention机制使Commonsense与context之间进行交互:<ul><li><script type="math/tex; mode=display">S_i^{CS}=W_1^{CS}c_i^t + W_2^{CS} v_j^{CS} + W_3^{CS} (c_i^t \odot v_j^{CS})</script></li><li><script type="math/tex; mode=display">p_{ij}^{CS} = \frac{exp(S_{ij}^{CS})}{\sum_{k=1}^l exp(S_{ij}^{CS})}</script></li><li><script type="math/tex; mode=display">c_i^{CS} = \sum_{j=1}^l p_{ij}^{CS} v_j^{CS}</script></li></ul></li><li>最后, 将Commonsense-aware的context表示和baseline reasoning cell的结果进行组合得到NOIC的输出<ul><li><script type="math/tex; mode=display">z_i = \sigma(W_z [c_i^{CS}; c_i^t] + b_z)</script></li><li><script type="math/tex; mode=display">(c_o)_i^t = z_i \odot c_i^t + (1-z_i) \odot c_i^{CS}</script></li></ul></li></ul></li><li>通过这种方式做到在每一个推理步对知识进行有选择的结合</li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h3 id="Ablations"><a href="#Ablations" class="headerlink" title="Ablations"></a>Ablations</h3><ol><li>Model Ablations</li><li>Commonsense Ablations</li><li>Commonsense Selection</li></ol><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><p>TBA</p>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> multi-hop </tag>
            
            <tag> reasoning </tag>
            
            <tag> commonsense </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Commonsense to Controllable TG</title>
      <link href="/2019/01/23/paper-tg-with-commonsense/"/>
      <url>/2019/01/23/paper-tg-with-commonsense/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>本文将介绍 两篇 关于利用常识知识进行可控地文本生成的论文，都是清华大学黄民烈老师组的研究工作：</p><blockquote><p>Commonsense Knowledge Aware Conversation Generation with Graph Attention. IJCAI,2018.<br>Story Ending Generation with Incremental Encoding and Commonsense Knowledge. AAAI,2019.</p></blockquote><p>相比于文本生成问题，我更关注怎样将常识知识引入到文本的理解，即编码过程。</p><h2 id="1-CS-Know-to-Dialogue-with-GAT"><a href="#1-CS-Know-to-Dialogue-with-GAT" class="headerlink" title="1.CS Know. to Dialogue with GAT"></a>1.CS Know. to Dialogue with GAT</h2><p>这篇文章的主要目的是利用大规模的常识知识帮助对话的理解与生产。<br>在对话任务中，给定一个用户 post：</p><ul><li>在编码端：采取静态策略<ul><li>从(常识)知识库中抽取出与 post 相关的知识子图</li><li>在编码端知识子图是静态的，包含与 post 相关的信息</li><li>用 Graph Attention(GAT) 机制进行编码</li></ul></li><li>在解码端：采取动态策略<ul><li>有关注的读取 Knowledge Graph 以及 Knowledge Graph 中的知识三元组</li></ul></li><li>文章中强调了一点：与已有的其他模型(分开、独立使用 knowledge triples/entities)不同，此模型将每个 knowledge graph 作为一个整体进行处理，可以编码更多的结构信息以及有关联的语义信息。</li></ul><p>模型的整体结构: <img src="/images/paper-tg-with-commonsense/tg-ccm-architecture.png" alt="CCM-overview"></p><h3 id="Task-Definition-and-Overview"><a href="#Task-Definition-and-Overview" class="headerlink" title="Task Definition and Overview"></a>Task Definition and Overview</h3><ul><li>user post(input): $X=x_1 x_2 … x_n$</li><li>some commonsense knowledge graphs: $G={g_1, g_2, …, g_{N_G}}$</li><li>desired response(output): $Y=y_1 y_2 … y_m$</li><li>使用 post 中的每个词作为查询，去常识知识库中 为每个词抽取出一个对应的子图:<ul><li>eg: 对于 post $X=x_1 x_2 … x_n$, 其对应的抽取出来的图是 $G={g_1, g_2,…,g_{N_G}}$</li><li>每个子图由一个三元组集合构成: $g_i = {\tau_1, \tau_2, …, \tau_{N_{g_i}}}$</li><li>每个三元组(head entity, relation, tail entity): $\tau = (h,r,t)$</li><li>使用 TransE 来表示 KG 中的实体和关系;</li><li>最终每个三元组 $\tau$ 表示为: <ul><li>$k = (h,r,t) = MLP(TransE(h,r,t))$<ul><li>$h/r/t$ 为各自的 TransE Embedding</li><li>使用 $MLP$ 是为了缩小 知识库 和 无结构对话文本 之间的 表示差距(bridge the representation gap)</li></ul></li></ul></li><li>对于在知识库中没有检索到匹配的词，使用一个特殊的 <code>Not_A_Fact</code> 来表示</li></ul></li></ul><h3 id="Knowledge-Interpreter"><a href="#Knowledge-Interpreter" class="headerlink" title="Knowledge Interpreter"></a>Knowledge Interpreter</h3><p>Knowledge Interpreter overview: <img src="/images/paper-tg-with-commonsense/tg-ccm-enc-1.png" alt="KI-overview"><br><!-- ![ki-ov](/images/tg-ccm-enc-1.png) --></p><ul><li>知识感知的词表示: $e(x_t) = [w(x_t);g_i]$</li><li>Knowledge Interpreter 的主要目的是用每个词对应的知识子图向量来增强词表示<ul><li>使用 post 中的每个词 $w_t$ 作为 key entity (红色点)去抽取出一个子图 $g_i$ (上图中黄色的部分)</li><li>knowledge interpreter 的输出为静态图注意力机制计算得到的 graph vector $g_i$</li></ul></li><li>Static Graph Attention<ul><li>使用功能 GAT 的好处是：不仅可以编码结构语义信息还可以考虑到图中节点间的关系</li><li>SGA 的输入: knowledge triple vectors<ul><li><script type="math/tex; mode=display">K(g_i) = {k_1, k_2, ..., k_{N_{g_i}}}</script></li></ul></li><li>SGA 的输出: knowledge graph vector<ul><li><script type="math/tex; mode=display">g_i = \sum_{n=1}^{N_{g_i}} \alpha_n^s [h_n;h_t]</script></li><li><script type="math/tex; mode=display">\alpha_n^s = \frac{exp(\beta_n^s)}{\sum_{j=1}^{N_{g_i}} exp(\beta_j^s)}</script></li><li><script type="math/tex; mode=display">\beta_n^s = (W_r r_n)^T tanh(W_h h_n + W_t t_n)</script></li><li>其中, $(h_n, r_n, t_n) = k_n$</li><li>(Note: 在原始的GAT中没有引入relation向量计算attention的score)</li></ul></li></ul></li></ul><h2 id="2-CS-Know-to-Story-Ending-Generation"><a href="#2-CS-Know-to-Story-Ending-Generation" class="headerlink" title="2.CS Know. to Story Ending Generation"></a>2.CS Know. to Story Ending Generation</h2><p>这篇文章的主要目的是生成连贯、合理且有逻辑的故事结尾。</p><ul><li>故事例子：故事与知识的关联<ul><li><img src="/images/paper-tg-with-commonsense/story-data-example.png" alt="story-data-example"></li></ul></li></ul><p>动机：</p><ul><li>为故事生成一个合理的结尾需要对故事有很强的理解，需要以下两个方面</li><li>Context Clues：故事的逻辑、因果关系以及时序依赖（logic/causality/chronological order）通常跨越多个句子，通过句子中的事件/实体序列来获取；</li><li>Implicit/Commonsense Knowledge：超越文本表明信息的隐含知识</li></ul><p>针对以上的需求/目的，本文提出了：</p><ul><li>Incremental Encoding Schema：表示文本线索<ul><li>逐句编码</li></ul></li><li>Multi-Source Attention：帮助故事理解，充分利用常识知识<ul><li>为每个词抽取一个 one-hop Knowledge graph，计算graph的表示</li><li>在编码当前句子时，不仅对前一个句子中的每个词的上下文表示进行Attention，还对每个词的KG进行Attention</li></ul></li></ul><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>模型的整体结构: <img src="/images/paper-tg-with-commonsense/story-architecture.png" alt="storygen-model"></p><h3 id="Incremental-Encoding-Scheme"><a href="#Incremental-Encoding-Scheme" class="headerlink" title="Incremental Encoding Scheme"></a>Incremental Encoding Scheme</h3><p>最直接的对故事进行上下文编码的方法有：</p><ul><li>将所有的句子进行串联，构成一个长句子，然后用LSTM进行计算</li><li>使用带有 Hierarchical-Attention 的层次LSTM</li><li>但是这两种都不能够有效的表示上下文的线索信息，相邻句子中事件/实体的时序顺序(chronological order)和因果关系(causal relationship)</li></ul><p>本文提出的增量编码模式：编码当前句子 $X_i$ 中的第j个位置的隐状态计算如下</p><ul><li>$h_j^{(i)} = LSTM(h_{j-1}^{(i)}, e(x_j^{(i)}), c_{lj}^{(i)}), i&gt;2$<ul><li>其中，$e(\cdot)是词向量$， $c_{lj}^{(i)}$ 是关于前一个句子的上下文向量（基于隐状态$h_{j-1}^{(i)}$）</li></ul></li></ul><h3 id="Multi-Source-Attention"><a href="#Multi-Source-Attention" class="headerlink" title="Multi-Source Attention"></a>Multi-Source Attention</h3><p>通过多源Attention来获取上下文向量表示文本线索信息，由两部分组成：</p><ul><li>$c_{lj}^{(i)} = W_l( [c_{hj}^{(i)} ; c_{xj}^{(i)}] ) + b_l$</li><li>$c_{hj}^{(i)}$ 是 state context vector，即前一个句子隐状态的权重和<ul><li><script type="math/tex; mode=display">c_{hj}^{(i)}  = \sum_{k=1}^{l_{i-1}} \alpha_{hk}^{(i)} h_k^{(i-1)}</script></li><li><script type="math/tex; mode=display">\alpha_{hk}^{(i)} = \frac{ exp( \beta_{h_{k,j}}^{(i)} ) }{ \sum_{m=1}^{l_{i-1}} exp( \beta_{h_{m,j}}^{(i)} ) }</script></li><li><script type="math/tex; mode=display">\beta_{h_{k,j}}^{(i)} = h_{j-1}^{(i)} W_s h_k^{(i-1)}</script></li><li>$h_k^{(i-1)}$ 前一个句子第k时刻的隐状态</li></ul></li><li>$c_{xj}^{(i)}$ 是 knowledge context vector， 即前一个句子的知识表示权重和<ul><li><script type="math/tex; mode=display">c_{xj}^{(i)}  = \sum_{k=1}^{l_{i-1}} \alpha_{xk}^{(i)} g(x_k^{(i-1)})</script></li><li><script type="math/tex; mode=display">\alpha_{xk}^{(i)} = \frac{ exp( \beta_{x_{k,j}}^{(i)} ) }{ \sum_{m=1}^{l_{i-1}} exp( \beta_{x_{m,j}}^{(i)} ) }</script></li><li><script type="math/tex; mode=display">\beta_{x_{k,j}}^{(i)} = h_{j-1}^{(i)} W_k g(x_k^{(i-1)})</script></li><li>$g(x_k^{(i-1)})$ 是前一个句子中第k个词的graph vector</li><li>$h_{j-1}^{(i)}$ 是第i个句子中的第j个位置的隐状态，即当前句子中的前一时刻的隐状态</li></ul></li></ul><h3 id="Knowledge-Graph-Representation"><a href="#Knowledge-Graph-Representation" class="headerlink" title="Knowledge Graph Representation"></a>Knowledge Graph Representation</h3><p>对知识图编码有两种方式：</p><ul><li>graph attention</li><li>contextual attention (knowledgeable-reader)</li></ul><p>针对词$x$抽取（以$x$为头实体）出的图: $G(x) = {R_1,R_2, …,R_{N_x}}$</p><p>$R_i$ 为知识三元组 $(h,r,t)$， 用词向量表示h/t， r向量作为参数进行学习；<br>$N_x$ 为 $x$ 的邻居数量；</p><h4 id="Graph-Attention-的图表示方式"><a href="#Graph-Attention-的图表示方式" class="headerlink" title="Graph Attention 的图表示方式"></a>Graph Attention 的图表示方式</h4><ul><li><script type="math/tex; mode=display">g(x) = \sum_{i=1}^{N_x} \alpha_{R_i} [h_i；t_i]</script></li><li><script type="math/tex; mode=display">\alpha_{R_i} = \frac{ exp(\beta_{R_i}) }{ \sum_{j}^{N_x} exp(\beta_{R_j})}</script></li><li><script type="math/tex; mode=display">\beta_{R_i} = (W_r r_i)^T tanh( W_h h_i + W_t t_i )</script></li></ul><h4 id="Contextual-Attention-的图表示方式"><a href="#Contextual-Attention-的图表示方式" class="headerlink" title="Contextual Attention 的图表示方式"></a>Contextual Attention 的图表示方式</h4><ul><li><script type="math/tex; mode=display">g(x) = \sum_{i=1}^{N_x} \alpha_{R_i} M_{R_i}</script></li><li><script type="math/tex; mode=display">M_{R_i} = BiGRU(h_i, r_i, t_i)</script></li><li><script type="math/tex; mode=display">\alpha_{R_i} = \frac{ exp(\beta_{R_i}) }{ \sum_{j}^{N_x} exp(\beta_{R_j})}</script></li><li><script type="math/tex; mode=display">\beta_{R_i} =h_{(x)}^T W_c M_{R_i}</script></li><li>$h_{(x)}$ 是词$x$的隐状态</li></ul><h2 id="CS-Know-Process"><a href="#CS-Know-Process" class="headerlink" title="CS Know. Process"></a>CS Know. Process</h2><p>对于常识知识的抽取方式以及和数据的融合方式</p><ul><li>CCM中：<ul><li>实体和关系的向量通过transE进行学习，维度均为100</li><li>去除了由多个词构成的头/尾实体</li><li>最终的数据量：<ul><li>三元组：120850</li><li>实体数：21471</li><li>关系数：44</li></ul></li></ul></li><li>StoryEndGen中：<ul><li>只抽取出one-hop的三元组</li><li>为每个词抽取最多10个三元组;</li><li>头/尾实体为名词(noun)或动词(verb)</li><li>最终的数据量：<ul><li>关系数：45</li><li>三元组：16652</li></ul></li></ul></li><li>两篇论文都是直接给出了处理好的数据，并没有给出数据的预处理过程，github链接分别为：<ol><li><a href="https://github.com/tuxchow/ccm" target="_blank" rel="noopener">https://github.com/tuxchow/ccm</a></li><li><a href="https://github.com/JianGuanTHU/StoryEndGen" target="_blank" rel="noopener">https://github.com/JianGuanTHU/StoryEndGen</a></li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> tg </tag>
            
            <tag> note </tag>
            
            <tag> commonsense </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Survey about Works of MOSAIC</title>
      <link href="/2019/01/21/note-research-ai2-mosaic/"/>
      <url>/2019/01/21/note-research-ai2-mosaic/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>MOSAIC 是 AI2 (ALLEN Institute for Artificial Intelligence) 研究院中进行机器常识智能研究的小组<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://mosaic.allenai.org/">[1]</span></a></sup>.</p><p>现有的研究项目：(Update at July, 2019)</p><ol><li><strong>Commonsense Knowledge Graphs: Exploring semi-structured representations of commonsense.</strong><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://mosaic.allenai.org/projects/commonsense-knowledge-graphs">[2]</span></a></sup></li><li>Visual Commonsense Reasoning: 视觉常识推理项目<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://visualcommonsense.com/">[3]</span></a></sup></li><li>Mosaic Commonsense Benchmarks: Measuring progress on Machine Common Sense.<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://mosaic.allenai.org/projects/mosaic-commonsense-benchmarks">[4]</span></a></sup></li></ol><p>对应的主要Papers：</p><ol><li>ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. AAAI,2019.</li><li>From Recognition to Cognition: Visual Commonsense Reasoning</li><li>SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. EMNLP,2018.</li></ol><p>其他Papers:</p><ol><li>Event2Mind: Commonsense Inference on Events, Intents, and Reactions. ACL,2018.</li><li>Modeling Naive Psychology of Characters in Simple Commonsense Stories. ACL,2018.</li><li>Reasoning about Actions and State Changes by Injecting Commonsense Knowledge. EMNLP,2018.</li></ol><p>重点关注 <code>Commonsense Knowledge Graphs</code> 这部分的工作</p><h2 id="Commonsense-Knowledge-Graphs"><a href="#Commonsense-Knowledge-Graphs" class="headerlink" title="Commonsense Knowledge Graphs"></a>Commonsense Knowledge Graphs</h2><blockquote><ol><li>Event2Mind: Commonsense Inference on Events, Intents, and Reactions. ACL,2018.</li><li>ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. AAAI,2019.</li></ol></blockquote><h3 id="Event2Mind"><a href="#Event2Mind" class="headerlink" title="Event2Mind"></a>Event2Mind</h3><p>常识推理任务</p><p>推理类型：2类（语用推理 pragmatic inference）</p><ul><li><p>intent 意图</p><ul><li>可以解释为什么施事者导致事件发生，是行动或事件的心理前提</li></ul></li><li><p>emotional reaction 情绪反应</p><ul><li>反应定义为解释施事者和参与事件的其他人的心理状态将如何因此改变</li><li>反应可以被认为是一种行为或事件的心理后置条件</li></ul></li></ul><p>任务目标：给定一个 event phrase，预测 X intent / X reaction / Y reaction</p><ul><li>pre-condition: X intent</li><li>post-condition: X/Y reaction</li></ul><p>例子：</p><ul><li><img src="/images/note-research-ai2-mosaic/event2mind-example.png" alt="event2mind-example"></li></ul><h3 id="ATOMIC"><a href="#ATOMIC" class="headerlink" title="ATOMIC"></a>ATOMIC</h3><p>关于 if-then 推理类型的推理知识库（Knowledge base about inferential knowledge on if-then reasoning types）</p><ul><li>例子：<img src="/images/note-research-ai2-mosaic/atomic-example.png" alt="atomic-example"></li></ul><p>nine if-then relation types to distinguish: （不同上不全面的刻画维度）</p><ul><li><strong>cause vs. effect</strong><ul><li>[Xintent，Xneed] vs. [Xwant，Xreaction，Effect on X，Effect on Y，Ywant，Yreaction]</li></ul></li><li><strong>agents vs. themes</strong>：agent（X，who cause the event），themes（Other）<ul><li>[Xintent，Xneed，Xwant，Xreaction，Effect on X， Xattribute] vs. [Effect on Y，Ywant， Yreaction]</li></ul></li><li><strong>voluntary vs. involuntary</strong> events<ul><li>[Xintent，Xneed，Xwant，Ywant] vs. [Xreaction，Effect on X，Effect on Y，Yreaction，Xattribute]</li></ul></li><li><strong>actions vs. mental states</strong><ul><li>[] vs. []</li></ul></li><li><strong>dynamic vs. static</strong><ul><li>[Xintent，Xneed，Xwant，Xreaction，Effect on X，Effect on Y，Ywant，Yreaction] vs. [Xattribute]</li></ul></li></ul><p>taxonomy of if-then reasoning types:</p><ul><li>based on the content being predicted:<ul><li><font color="green">**if-event-then-mental-state**</font>* mental pre-/post- conditions of an event* 3 relations: <font color="blue">**X inent/ X reaction/ Other reaction**</font></li><li><font color="green">**if event-then event**</font>* events that constitute probable pre- and postconditions of a given event* 5 relations: <font color="blue">**X need/ X want/ Effect on X/ Other want/ Effect on Other**</font></li><li><font color="green">**if event-then-person**</font>* a stative relation that describes how the subject of an event is described or perceived* 1 relations: <font color="blue">**X attribute**</font></li></ul></li><li>based on their causal relations<ul><li>causes（因）</li><li>effect（果）</li><li>stative（状态）</li></ul></li><li>图例：<img src="/images/note-research-ai2-mosaic/atomic-taxonomy-example.png" alt="example"></li></ul><p>与现有knowledge base的对比：</p><ul><li><img src="/images/note-research-ai2-mosaic/atomic-comp.png" alt="kb-comp"></li></ul><h2 id="DARPA-Leaderboards"><a href="#DARPA-Leaderboards" class="headerlink" title="DARPA Leaderboards"></a>DARPA Leaderboards</h2><h3 id="Abductive-Natural-Language-inference-alpha-NLI"><a href="#Abductive-Natural-Language-inference-alpha-NLI" class="headerlink" title="Abductive Natural Language inference (alpha NLI)"></a>Abductive Natural Language inference (alpha NLI)</h3><ul><li>Abductive Natural Language Inference (αNLI) is a new commonsense benchmark dataset designed to test an AI system’s capability to apply abductive reasoning and common sense to form possible explanations for a given set of observations. </li><li>Formulated as a binary-classification task, the goal is to <strong>pick the most plausible explanatory hypothesis given two observations from narrative contexts</strong>.</li><li>Abduction is the process of inference to the best explanation for a given set of observations<ul><li>Humans often use commonsense knowledge to perform abductive inference</li><li>For example, if a person observes wet grass, they’d infer that it probably rained. But, with additional knowledge that it has been sunny for many days, the might infer that the sprinklers were probably on.</li><li>As defined by the philosopher Charles Sanders Peirce(in 1903), abduction is:<ul><li>the process of forming explanatory hypotheses given some observations</li></ul></li></ul></li><li>Several Datasets have been proposed recently to tackle the task of inference in natural language, but they captures deductive inferences<ul><li>i.e. they hypothesis is necessary to have occurred if the premise is true</li></ul></li><li>Abductive inference a ampliative in nature and the inferences made acts as one of the possible explanations for the given set of observations, which further need to be explored/tested.</li></ul><blockquote><p>Example</p><p>Each instance in the dataset takes the form of a simple story. Given the beginning and the ending of a story, the task is to choose a more likely hypothesis between two given choices.</p><p><em>Obs1: Jenny was addicted to sending text messages.</em></p><p><em>Obs2: Jenny narrowly avoided a car accident.</em></p><p><em>Hyp1:</em> Since her friend’s texting and driving car accident, Jenny keeps her phone off while driving.</p><p><em>Hyp2:</em> <strong>Jenny was looking at her phone while driving so she wasn’t paying attention.</strong></p><p>correct: Hyp2</p></blockquote><ul><li>human performance：92.9%</li></ul><h3 id="HellaSwag-Can-a-Machine-Really-Finish-Your-Sentence"><a href="#HellaSwag-Can-a-Machine-Really-Finish-Your-Sentence" class="headerlink" title="HellaSwag: Can a Machine Really Finish Your Sentence?"></a>HellaSwag: Can a Machine Really Finish Your Sentence?</h3><ul><li>HellaSWAG is a dataset for studying grounded commonsense inference. </li><li>It consists of 70k multiple choice questions about grounded situations: each question comes from one of two domains — activitynet or wikihow — with four answer choices about <strong>what might happen next in the scene</strong>. The correct answer is the (real) sentence for the next event; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans.</li></ul><blockquote><p>Example</p><p>A woman is outside with a bucket and a dog. The dog is running around trying to avoid a bath. She</p><ul><li>a) rinses the bucket off with soap and blow dries the dog’s head.</li><li>b) uses a hose to keep it from getting soapy.</li><li><strong>c) gets the dog wet, then it runs away again.</strong></li><li>d) gets into the bath tub with the dog.</li></ul><p>correct: (c)</p></blockquote><ul><li>human performance：95.6%</li></ul><h3 id="Physical-IQa-Pyhsical-Interaction-QA"><a href="#Physical-IQa-Pyhsical-Interaction-QA" class="headerlink" title="Physical IQa: Pyhsical Interaction QA"></a>Physical IQa: Pyhsical Interaction QA</h3><ul><li>for naive physics reasoning focusing on how we interact with everyday objects in everyday situations.</li><li>focuses on affordances of objects, i.e., what actions each physical object affords (e.g., it is possible to use a shoe as a doorstop), and what physical interactions a group of objects afford (e.g., it is possible to place an apple on top of a book, but not the other way around).</li><li>The dataset requires reasoning about both the prototypical use of objects (e.g., shoes are used for walking) and non-prototypical but practically plausible use of objects (e.g., shoes can be used as a doorstop). </li><li>The dataset includes 20,000 QA pairs that are either multiple-choice or true/false questions.</li></ul><blockquote><p>Example</p><p>You need to break a window. Which object would you rather use?</p><ul><li><strong>a) a metal stool</strong></li><li>b) a giant bear</li><li>c) a bottle of water</li></ul><p>correct: (a)</p></blockquote><ul><li>human performance：94.9%</li></ul><h3 id="Social-IQA-Social-Interaction-QA"><a href="#Social-IQA-Social-Interaction-QA" class="headerlink" title="Social IQA: Social Interaction QA"></a>Social IQA: Social Interaction QA</h3><ul><li>for testing social commonsense intelligence. </li><li>Contrary to many prior benchmarks that focus on physical or taxonomic knowledge, Social IQa focuses on reasoning about people’s actions and their social implications. </li><li>For example, given an action like “Jesse saw a concert” and a question like “Why did Jesse do this?”, humans can easily infer that Jesse wanted “to see their favorite performer” or “to enjoy the music”, and not “to see what’s happening inside” or “to see if it works”. </li><li>The actions in Social IQa span a wide variety of social situations, and answer candidates contain both human-curated answers and adversarially-filtered machine-generated candidates. </li><li><p>Social IQa contains over 37,000 QA pairs for evaluating models’ abilities to reason about the social implications of everyday events and situations.</p></li><li><p>Types of reasoning:</p><ul><li><strong>Motivation</strong></li><li><strong>What Happens Next</strong></li><li><strong>Emotional Reaction</strong></li></ul></li></ul><blockquote><p>Example</p><p>In the school play, Robin played a hero in the struggle to the death with the angry villain. How would others feel as a result?</p><ul><li>a) sorry for the villain</li><li><strong>b) hopeful that Robin will succeed</strong></li><li>c) like Robin should lose the fight</li></ul><p>correct: (b)</p></blockquote><ul><li>human performance：88.1%</li></ul><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://mosaic.allenai.org/<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://mosaic.allenai.org/projects/commonsense-knowledge-graphs<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://visualcommonsense.com/<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://mosaic.allenai.org/projects/mosaic-commonsense-benchmarks<a href="#fnref:4" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> MRC-Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> research </tag>
            
            <tag> note </tag>
            
            <tag> cs-know </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>20190121 | 清华大学AI研究院知识智能研究中心发布会的记录</title>
      <link href="/2019/01/21/note-20190121-thukc/"/>
      <url>/2019/01/21/note-20190121-thukc/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Minute and thoughts of the launch conference of THUKC (清华大学人工智能研究院知识智能研究中心).<br>Mainly about the part of Commonsense-Awared Natural Language Generation (常识知识感知的语言生成, 黄民烈)</p><h2 id="Commonsense-Extraction"><a href="#Commonsense-Extraction" class="headerlink" title="Commonsense Extraction"></a>Commonsense Extraction</h2><ul><li>what is Commonsense Knowledge (CS Know.)</li><li>what is the Boundary of commonsense</li><li>Commonsense Extraction<ul><li>From Embedding<ul><li>Extracting Commonsense Properties from Embeddings with Limited Human Guidance. 2018</li></ul></li><li>Commonsense Knowledge base completion<ul><li>Commonsense Knowledge base Completion. 2018</li></ul></li><li>From RAW data(text, image)<ul><li>Automatic Extraction of Commonsense LocatedNear Knowledge. 2018</li></ul></li></ul></li></ul><h2 id="Commonsense-Knowledge-in-RC"><a href="#Commonsense-Knowledge-in-RC" class="headerlink" title="Commonsense Knowledge in RC"></a>Commonsense Knowledge in RC</h2><ul><li>代表工作：Knowledgeable Reader</li></ul><h2 id="Commonsense-Knowledge-to-Intent-Reaction-Emotion-etc"><a href="#Commonsense-Knowledge-to-Intent-Reaction-Emotion-etc" class="headerlink" title="Commonsense Knowledge to Intent, Reaction, Emotion, etc"></a>Commonsense Knowledge to Intent, Reaction, Emotion, etc</h2><ul><li>代表工作：<ul><li>Event2Mind: Commonsense Inference on Events, Intents and Reaction</li><li>Modeling Naive Psychology of Characters in Simple Commonsense Stories</li></ul></li><li>AI2 实验室做了大量这方面的工作，相关调研：<a href="/2019/01/21/note-research-ai2-mosaic/" title="link">link</a></li></ul><h2 id="Commonsense-to-Controllable-TG"><a href="#Commonsense-to-Controllable-TG" class="headerlink" title="Commonsense to Controllable TG"></a>Commonsense to Controllable TG</h2><ul><li>黄民烈老师组的一些很好的工作（黄老师称这些工作是常识知识感知的语言生成的初步探索）：<ul><li>Commonsense Knowledge Aware Conversation Generation with Graph Attention. IJCAI,2018.(distinguished paper)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://juejin.im/post/5b6a9e085188251a8d37136d">[1]</span></a></sup></li><li>Story Ending Generation with Incremental Encoding Commonsense Knowledge. AAAI,2019.</li><li>学习笔记：<a href="/2019/01/23/paper-tg-with-commonsense/" title="note_link">note_link</a></li></ul></li><li>Three Fundamental problems in current Neural Language Generation Models<ul><li>semantics(real understanding)</li><li>Consistency(long text generation)</li><li>Logic(reasonable and making sense)</li><li>在评价方面，关于如何评价生成的文本具有逻辑性还是个挑战（相关研究点：文本生成评价指标(BLEU、ROUGE等)的研究）</li></ul></li><li>New Architecture: <ul><li><strong>symbolic knowledge + planning + neural computing</strong></li></ul></li></ul><h2 id="发布的重要资源"><a href="#发布的重要资源" class="headerlink" title="发布的重要资源"></a>发布的重要资源</h2><p>知识智能研究中心：<a href="http://ai.tsinghua.edu.cn/kirc/#" target="_blank" rel="noopener">http://ai.tsinghua.edu.cn/kirc/#</a></p><ul><li>中英文跨语言百科知识图谱XLORE — <span id="inline-blue">世界知识</span><ul><li>特点：<ul><li>大规模跨语言百科知识图谱</li><li>通过融合维基百科和百度百科，并对百科知识进行结构化和跨语言链接构建而成。</li><li>以结构化形式描述客观世界中的概念、实例、属性及其丰富语义关系。</li><li>XLORE目前包含约247万概念、44.6万属性/关系、1628万实例和260万跨语言链接。</li></ul></li><li>项目地址：<a href="https://xlore.org/" target="_blank" rel="noopener">https://xlore.org/</a></li></ul></li><li>基于义原的开放语言知识库 OpenHowNet — <span id="inline-green">语言知识</span>、<span id="inline-orange">常识知识</span><ul><li>！HowNet 核心数据首次开源</li><li>项目地址：<a href="https://openhownet.thunlp.org/" target="_blank" rel="noopener">https://openhownet.thunlp.org/</a></li></ul></li></ul><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://juejin.im/post/5b6a9e085188251a8d37136d<a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> MRC-Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> report </tag>
            
            <tag> mrc </tag>
            
            <tag> research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Natural Language Inference | Analysis and Paper List</title>
      <link href="/2019/01/16/nli-paper-info/"/>
      <url>/2019/01/16/nli-paper-info/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="NLI-Introduction"><a href="#NLI-Introduction" class="headerlink" title="NLI Introduction"></a>NLI Introduction</h2><h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><ul><li>词语匹配的多元性（同义词、一词多义）lexical gap、lexical variant；</li><li>短语匹配的结构性、多粒度计算语义；</li><li>文本匹配的层次性；</li><li>利用不相似部分的信息；</li><li>自然语言的基础现象：Variability of Semantic expression<ul><li>是语言歧义的对偶问题；</li><li>语义表达的多变性、语言结构的多样性；</li></ul></li></ul><h3 id="Approachs"><a href="#Approachs" class="headerlink" title="Approachs"></a>Approachs</h3><p>models on NLI(Natural Language Infernce), concluded as: <strong>sentence-encoding-based</strong> vs. <strong>interaction-based</strong></p><ul><li>基于encoding的基本框架：<ul><li>input encoding —&gt;concat vector —&gt;vector &amp; element-wise difference/product—&gt; prediction</li><li>Pros: <ul><li>1、可预处理；2、易于建立索引；</li></ul></li><li>Cons:<ul><li>1、失去语义焦点，易语义偏移；2、词上下文重要性难以衡量；</li></ul></li></ul></li><li>基于交互的基本框架：<ul><li>input encoding —&gt; inference(interaction) —&gt; composition —&gt; prediction</li><li>Pros:<ul><li>1、把握语义焦点；2、可以对上下文重要性进行合理的建模；</li></ul></li><li>Cons:<ul><li>1、信息损失（主要由交互带来）；</li></ul></li></ul></li></ul><h3 id="Measure-of-Similarity"><a href="#Measure-of-Similarity" class="headerlink" title="Measure of Similarity"></a>Measure of Similarity</h3><h2 id="Models-of-Sentence-Encoding-Based"><a href="#Models-of-Sentence-Encoding-Based" class="headerlink" title="Models of Sentence Encoding Based"></a>Models of Sentence Encoding Based</h2><ul><li>BiLSTM-Max</li><li>NSE</li><li>Deep Gated Attn. BiLSTM</li><li>Residual Stacked Encoder</li><li>Reinforced Self-Attention Network</li><li>Distance-based Self-Attention Network</li><li>Hierarchical BiLSTM with Max Pooling</li><li>Dynamic Self-Attention Model</li></ul><h2 id="Models-of-Interaction-Based"><a href="#Models-of-Interaction-Based" class="headerlink" title="Models of Interaction Based"></a>Models of Interaction Based</h2><ul><li>Decomposable Attention</li><li>ESIM</li><li>KIM</li><li>Densely Interactive Inference Network (DIIN)</li><li>BIMPM</li><li>Multi-Way Attention</li><li>DR-BiLSTM</li><li>CAFE</li><li>Densely-Connected Recurrent and Co-Attentive Network</li><li>DMAN</li><li>SLRC</li><li>AFN</li></ul>]]></content>
      
      
      <categories>
          
          <category> NLI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> paper </tag>
            
            <tag> research </tag>
            
            <tag> nli </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WIP| Analysis of FEVER</title>
      <link href="/2019/01/15/mrc-analysis-fever/"/>
      <url>/2019/01/15/mrc-analysis-fever/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="FEVER-Introduction"><a href="#FEVER-Introduction" class="headerlink" title="FEVER Introduction"></a>FEVER Introduction</h2><ul><li>FEVER data: <a href="http://fever.ai/resources.html" target="_blank" rel="noopener">link</a><ul><li>包含 185,445 条断言(claims)</li></ul></li><li>FEVER shared Task(with NAACL 2018) info: <a href="http://fever.ai/task.html" target="_blank" rel="noopener">details here</a></li><li>FEVER official baseline code: <a href="https://github.com/sheffieldnlp/fever-naacl-2018" target="_blank" rel="noopener">github repo</a></li></ul><p>FEVER(Fact Extraction and VERification) 任务中，给定一个未经验证的断言(claim)，即一个句子，要求模型/系统从Wikipedia中找到对应的证据句(evidence)，来验证这个断言，是否可以被证实(<strong>SUPPORTED</strong>)、反驳(<strong>REFUTED</strong>)或是没有足够的信息判断(<strong>NOT ENOUGH INFO</strong>)。对于SUPPORTED和REFUTED的判断，需要给出证据句子。其中 16.82%的例子中，需要多个证据句子来进行判断，12.15%的情况下，证据句来源于多篇文档。</p><h3 id="Data-Statistics"><a href="#Data-Statistics" class="headerlink" title="Data Statistics"></a>Data Statistics</h3><div class="table-container"><table><thead><tr><th>split</th><th>SUPPORTED</th><th>REFUTED</th><th>NEI</th></tr></thead><tbody><tr><td>Train</td><td>80035</td><td>29775</td><td>35639</td></tr><tr><td>Dev</td><td>6666</td><td>6666</td><td>6666</td></tr><tr><td>Test</td><td>6666</td><td>6666</td><td>6666</td></tr></tbody></table></div><h3 id="Baseline-System"><a href="#Baseline-System" class="headerlink" title="Baseline System"></a>Baseline System</h3><p>baseline 系统是 pipelined 形式，由三部分组成: 1.document retrieval, 2.sentence-level evidence selection, 3.textual entailment.</p><p>其中文本蕴含识别(recognizing textual entailment)部分采用的是 谷歌的 Decomposable Attention 模型</p><h3 id="Score-Metrics"><a href="#Score-Metrics" class="headerlink" title="Score Metrics"></a>Score Metrics</h3><p>official scorer: <a href="https://github.com/sheffieldnlp/fever-scorer" target="_blank" rel="noopener">https://github.com/sheffieldnlp/fever-scorer</a></p><p>判断 claim 的类型(SUPPORTED/REFUTED/NOT ENOUGH INFO)是个三分类问题，对此使用 accuracy 来评价。<br>对于 SUPPORTED 和 REFUTED 的类别，还需要提供证据片段，对此使用 F1 来评价。</p><h2 id="FEVER-Shared-Task-Top-3-Systems-Solutions"><a href="#FEVER-Shared-Task-Top-3-Systems-Solutions" class="headerlink" title="FEVER Shared Task Top-3 Systems Solutions"></a>FEVER Shared Task Top-3 Systems Solutions</h2><blockquote><p>The Fact Extraction and VERification (FEVER) Shared Task</p></blockquote><h3 id="Top-1-UNC-NLP"><a href="#Top-1-UNC-NLP" class="headerlink" title="Top-1: UNC-NLP"></a>Top-1: UNC-NLP</h3><blockquote><p>Combining Fact Extraction and Verification with Neural Semantic Matching Networks<br>AAAI 2019</p></blockquote><h3 id="Top-2-UCL-Machine-Reading-Group"><a href="#Top-2-UCL-Machine-Reading-Group" class="headerlink" title="Top-2: UCL Machine Reading Group"></a>Top-2: UCL Machine Reading Group</h3><blockquote><p>UCL Machine Reading Group: Four Factor Framework For Fact Finding (HexaF)</p></blockquote><h3 id="Top-3-Athene-UKP-TU-Darmstadt"><a href="#Top-3-Athene-UKP-TU-Darmstadt" class="headerlink" title="Top-3: Athene UKP TU Darmstadt"></a>Top-3: Athene UKP TU Darmstadt</h3><blockquote><p>Multi-Sentence Textual Entailment for Claim Verification</p></blockquote><h3 id="Shared-Task-Overview"><a href="#Shared-Task-Overview" class="headerlink" title="Shared Task Overview"></a>Shared Task Overview</h3><h2 id="FEVER-Workshop-Notes"><a href="#FEVER-Workshop-Notes" class="headerlink" title="FEVER Workshop Notes"></a>FEVER Workshop Notes</h2><ul><li>FEVER workshop info: <a href="http://fever.ai/workshop.html" target="_blank" rel="noopener">details here</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> paper </tag>
            
            <tag> mrc </tag>
            
            <tag> research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MRC 的评测与所需能力</title>
      <link href="/2019/01/14/mrc-measure-skills/"/>
      <url>/2019/01/14/mrc-measure-skills/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="Reading-Comprehension-Skills"><a href="#Reading-Comprehension-Skills" class="headerlink" title="Reading Comprehension Skills"></a>Reading Comprehension Skills</h2><blockquote><p>An Analysis of Prerequisite Skills for Reading Comprehension.<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://aclweb.org/anthology/W16-6001">[1]</span></a></sup><br>Prerequisite skills for reading comprehension: Multi-perspective analysis of mctest datasets and systems. AAAI,2017.<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="http://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/14-Sugawara-14614.pdf">[2]</span></a></sup><br>Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability. ACL,2017.<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://aclanthology.info/papers/P17-1075/p17-1075">[3]</span></a></sup></p></blockquote><p>(从数据的角度)将阅读理解所需要的能力分为两类: <span id="inline-blue">认知能力(prerequisite skill)</span> 和 <span id="inline-blue">语言能力(readability)</span></p><h3 id="1-prerequisite-skills"><a href="#1-prerequisite-skills" class="headerlink" title="1.prerequisite skills"></a>1.prerequisite skills</h3><p>认知能力: measure different types of reasoning and knowledge required to answer the question</p><p>定义了 13 种认知能力<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="王炳宁《A_Cognitive_Perspective_of_Machine_Comprehension_And_Recent_Advances》">[4]</span></a></sup>, 如下:</p><ol><li>object tracking 目标跟踪<ul><li>同时锁定和跟踪多个目标，如集合或个体，也被称为列举或枚举</li></ul></li><li>mathematical reasoning 数学推理<ul><li>能够完成统计或者量化操作</li></ul></li><li>coreference resolution 指代消解<ul><li>将指代词映射到相应的实体上</li></ul></li><li>logical reasoning 逻辑推理<ul><li>逻辑操作，例如对量词、否定、条件以及转移推理</li></ul></li><li>analogy 类比<ul><li>能够了解一些隐喻，如转喻和提喻</li></ul></li><li>causal relation 因果关系<ul><li>理解文本中的因果关系</li></ul></li><li>spatiotemporal relation 空间关系<ul><li>理解空间或者时间上的关系</li></ul></li><li>ellipsis 省略<ul><li>识别出文章中隐含或者忽略的信息，如参数、谓词、量词、时间等等</li></ul></li><li>bridging 间接引用<ul><li>能够根据词法或者句法的信息进行推理</li></ul></li><li>elaboration 阐述<ul><li>能够根据已有事实/常识进行推理</li></ul></li><li>meta-knowledge 元知识<ul><li>理解读者、作者或者文体信息（如：谁是这个故事的主人公）</li></ul></li><li>schematic clause relation 短语关系<ul><li>理解包含有并列、从句或者关系子句的复杂句子</li></ul></li><li>punctuation 标点符号<ul><li>理解文章中标点符号代表的意义</li></ul></li></ol><p>备注：</p><ul><li>认知能力与RC的一个联系是：当回答一个问题时，需要用到的认知能力越多，该问题越难回答</li><li>9和10的区别在于：9利用词项/句法信息还是10通用的常识信息</li><li>8到11是对Commonsense reasoning的细致分类</li><li>1-11是涉及到多句的，12-13涉及单句</li></ul><h3 id="2-readability"><a href="#2-readability" class="headerlink" title="2.readability"></a>2.readability</h3><p>语言能力: 如何将低层次的文本符号（字、词）组合为高层次的含义的能力<br>measures the text ease of processing and a wide range of linguistic features/human readability measurements are used.</p><ul><li>词义辨析：正确理解和区分词的意思。</li><li>句法识别：识别出文本的句法信息，使推理过程不受句法 变化的影响。</li><li>语义组合：根据句法信息将基本单元（字、词）的语义组 合成高等单元（句子，篇章）的语义。</li></ul><h2 id="Metrics-Evaluation"><a href="#Metrics-Evaluation" class="headerlink" title="Metrics/Evaluation"></a>Metrics/Evaluation</h2><p>如何评价一个example的难度？</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://aclweb.org/anthology/W16-6001<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/14-Sugawara-14614.pdf<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://aclanthology.info/papers/P17-1075/p17-1075<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">王炳宁《A_Cognitive_Perspective_of_Machine_Comprehension_And_Recent_Advances》<a href="#fnref:4" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> MRC-Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> research </tag>
            
            <tag> metric </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器阅读理解数据集</title>
      <link href="/2019/01/12/mrc-dataset-info/"/>
      <url>/2019/01/12/mrc-dataset-info/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Contributed by Luxi Xing and Yuqiang Xie. IIE, CAS.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Based on the style of the ANSWER for each datasets, we split the datasets into several category and we will give details for each datasets.</p><blockquote><ol><li><a href="#Extractive">Extractive</a></li><li><a href="#Multi-Choice">Multi-Choice</a></li><li><a href="#Generative">Generative</a></li><li><a href="#Sequential">Sequential</a></li><li><a href="#Others">Others</a><br><del>Cloze-Style</del></li></ol></blockquote><h2 id="Extractive"><a href="#Extractive" class="headerlink" title="Extractive"></a>Extractive</h2><div class="table-container"><table><thead><tr><th>Dataset</th><th>Language</th><th>Domain</th><th>#Train<br>#Doc</th><th>#Dev</th><th>#Test</th><th>Year</th><th>Features</th></tr></thead><tbody><tr><td>SQuADv1</td><td>English</td><td>Wikipedia</td><td>87599<br>442</td><td>10570<br>48</td><td>9533<br>46</td><td>2016</td><td></td></tr><tr><td>SQuADv2</td><td>English</td><td>Wikipedia</td><td>130319<br>442</td><td>11873<br>35</td><td>8862<br>28</td><td>2018</td><td>have no answer</td></tr><tr><td>TriviaQA</td><td>English</td><td>Web<br>Wikipedia</td><td>528979<br>61888</td><td>68621<br>9951</td><td>65059<br>9509</td><td>2017</td><td>avg. length is 2895</td></tr><tr><td>HotPotQA</td><td>English</td><td>Wikipedia</td><td>90564</td><td>7405</td><td>7405</td><td>2017</td><td>multiple supporting doc. to answer</td></tr><tr><td>Natural Questions</td><td>English</td><td>Wikipedia</td><td>307k</td><td>8k</td><td>8k</td><td>2019</td><td>whole wikipedia article;<br>long answer</td></tr></tbody></table></div><h2 id="Multi-Choice"><a href="#Multi-Choice" class="headerlink" title="Multi-Choice"></a>Multi-Choice</h2><div class="table-container"><table><thead><tr><th>Dataset</th><th>Language</th><th>Domain</th><th>#Train<br>#Doc</th><th>#Dev</th><th>#Test</th><th>Year</th><th>Features</th></tr></thead><tbody><tr><td>RACE</td><td>English</td><td>Multi</td><td>87866<br>25137</td><td>4887<br>1389</td><td>4934<br>1407</td><td>2017</td><td>high<br>middle</td></tr><tr><td>MCScript</td><td>English</td><td>InScript</td><td>9731<br>1470</td><td>1411<br>219</td><td>2797<br>430</td><td>2018</td><td></td></tr><tr><td>ARC</td><td>English</td><td>Science</td><td></td><td></td><td>14M/7787</td><td>2018</td><td>hard</td></tr><tr><td>OpenBook</td><td>English</td><td></td><td>4957</td><td>500</td><td>500</td><td>2018</td><td>multi-hop;<br>commonsense;<br>science fact</td></tr><tr><td>MultiRC</td><td>English</td><td>7 domain</td><td>9872<br>871</td><td></td><td></td><td>2018</td><td>multi correct answers</td></tr><tr><td>QAngaroo<br>wikihop</td><td>English</td><td></td><td>43738</td><td>5129</td><td>2451</td><td>2017</td><td>multi evidence pieces;<br>multi options</td></tr><tr><td>DREAM</td><td>English</td><td></td><td></td><td></td><td></td><td>2019</td><td>dialogue-based multi-choice</td></tr></tbody></table></div><h2 id="Generative"><a href="#Generative" class="headerlink" title="Generative"></a>Generative</h2><p>Free-form answer generation</p><div class="table-container"><table><thead><tr><th>Dataset</th><th>Language</th><th>Domain</th><th>#Train</th><th>#Dev</th><th>#Test</th><th>Year</th><th>Features</th></tr></thead><tbody><tr><td>MSMARCO<br>v1</td><td>English</td><td>Web</td><td></td><td></td><td>100k</td><td>2016</td><td></td></tr><tr><td>MSMARCO<br>v2</td><td>English</td><td>Web</td><td></td><td></td><td>100k</td><td>2018</td><td></td></tr><tr><td>NarrativeQA</td><td>English</td><td>book/moive<br>(wikipedia)</td><td>32747</td><td>3461</td><td>10557</td><td>2018</td><td>based on summary</td></tr><tr><td>DuReader</td><td>Chinese</td><td>Web</td><td></td><td></td><td></td><td>2017</td></tr></tbody></table></div><ul><li>Metric for evaluate the Generative-style QA dataset is: ROUGE-L/BLEU</li></ul><h3 id="MS-MARCO"><a href="#MS-MARCO" class="headerlink" title="MS MARCO"></a>MS MARCO</h3><ul><li>More details about MSMARCO, please reference to <a href="https://github.com/IndexFziQ/MSMARCO-MRC-Analysis" target="_blank" rel="noopener">this REPO</a></li></ul><h3 id="Narrative-QA"><a href="#Narrative-QA" class="headerlink" title="Narrative QA"></a>Narrative QA</h3><ul><li>dataset consists of two settings:<ul><li>based on the summary： average 659 tokens</li><li>based on the full book/movie script: average 62528 tokens</li></ul></li></ul><h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h2><div class="table-container"><table><thead><tr><th>Dataset</th><th>Language</th><th>Domain</th><th>Document</th><th>Question</th><th>Size<br>(Train/Dev/Test)</th><th>Year</th><th>Features</th></tr></thead><tbody><tr><td>QuAC</td><td>English</td><td>Wikipedia</td><td></td><td></td><td></td><td>2018</td><td></td></tr><tr><td>CoQA</td><td>English</td><td>Wikipedia</td><td></td><td></td><td></td><td>2018</td><td></td></tr><tr><td>DREAM</td><td>English</td><td>Daily life</td><td>6444</td><td>10197</td><td></td><td>2019</td><td>dialogue-based multi-choice;<br>multi-turn multi-party</td></tr></tbody></table></div><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><h3 id="Chinese-Datasets"><a href="#Chinese-Datasets" class="headerlink" title="Chinese Datasets"></a>Chinese Datasets</h3><ul><li>People Daily &amp; Children’s Fairy Tale (PD&amp;CFT): <a href="https://github.com/ymcui/Chinese-RC-Dataset" target="_blank" rel="noopener">https://github.com/ymcui/Chinese-RC-Dataset</a><ul><li>cloze-style</li></ul></li><li>dureader: <a href="https://github.com/baidu/DuReader" target="_blank" rel="noopener">https://github.com/baidu/DuReader</a><ul><li>generative</li></ul></li><li>cmrc2018: <a href="https://github.com/ymcui/cmrc2018" target="_blank" rel="noopener">https://github.com/ymcui/cmrc2018</a><ul><li>extractive</li></ul></li></ul><h3 id="Multi-Documents-Datasets"><a href="#Multi-Documents-Datasets" class="headerlink" title="Multi-Documents Datasets"></a>Multi-Documents Datasets</h3><ul><li>TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.</li><li>SearchQA: A new Q&amp;A dataset augmented with context from a search engine.</li><li>HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.</li></ul><h3 id="Commonsense-Reasoning"><a href="#Commonsense-Reasoning" class="headerlink" title="Commonsense Reasoning"></a>Commonsense Reasoning</h3><ul><li>Winograd Schema Challenge. <a href="https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html" target="_blank" rel="noopener">https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html</a></li><li>COPA: Choice of Plausible Alternatives. <a href="https://www.cs.york.ac.uk/semeval-2012/task7/index.html" target="_blank" rel="noopener">https://www.cs.york.ac.uk/semeval-2012/task7/index.html</a></li><li>ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension</li><li>CommonsenseQA</li></ul><h3 id="FEVER-Fact-Extraction-and-VERification"><a href="#FEVER-Fact-Extraction-and-VERification" class="headerlink" title="FEVER: Fact Extraction and VERification"></a>FEVER: Fact Extraction and VERification</h3><ul><li>FEVER data: <a href="http://fever.ai/resources.html" target="_blank" rel="noopener">link</a></li><li>FEVER shared Task(with NAACL 2018) info: <a href="http://fever.ai/task.html" target="_blank" rel="noopener">details here</a></li><li>FEVER official baseline code: <a href="https://github.com/sheffieldnlp/fever-naacl-2018" target="_blank" rel="noopener">github repo</a></li><li>my note and analysis of FEVER: <a href="/2019/01/15/mrc-analysis-fever/" title="fever-note">fever-note</a></li></ul><h3 id="Google-Natural-Questions"><a href="#Google-Natural-Questions" class="headerlink" title="Google Natural Questions"></a>Google Natural Questions</h3><p>official details: <a href="https://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.html" target="_blank" rel="noopener">https://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.html</a><br>github: <a href="https://github.com/google-research-datasets/natural-questions" target="_blank" rel="noopener">https://github.com/google-research-datasets/natural-questions</a></p><p>自然问题数据集(NQ)是一个公开的自然发生问题(即由寻求信息的人提出的问题)</p><ul><li>用于训练和评估开放领域问答系统的新的、大规模语料库;</li><li>也是第一个复制人类查找问题答案的端到端流程的语料库;</li><li>专注于通过阅读整个页面来查找答案，而不是从一个短段落中提取答案;</li><li>来源于Wikipedia;</li><li>人工注释答案:<ul><li>要求注释者通过通读整个维基百科页面来找到答案，就好像这个问题是他们自己提出的一样。</li><li>注释者需要找到一个长答案和一个短答案，长答案涵盖推断问题所需的所有信息，短答案需要用一个或多个实体的名称简洁地回答问题</li></ul></li></ul><p><strong>自然语言理解挑战：</strong></p><ul><li>NQ的目的是使QA系统能够阅读和理解完整的维基百科文章，其中可能包含问题的答案，也可能不包含问题的答案。</li><li>系统首先需要确定这个问题的定义是否足够充分，是否可以回答<ul><li>许多问题本身基于错误的假设，或者过于模糊，无法简明扼要地回答。</li></ul></li><li>然后，系统需要确定维基百科页面中是否包含推断答案所需的所有信息。</li><li>作者认为，相比在知道长答案后在寻找短答案，长答案识别任务(找到推断答案所需的所有信息)需要更深层次的语言理解。</li></ul><p>human upper bound:</p><ul><li>long answer selection task: 87% F1</li><li>short answer selection task: 76% F1</li></ul><p>dataset statistics:</p><ul><li>train: 307k</li><li>dev: 8k</li><li>test: 8k</li></ul>]]></content>
      
      
      <categories>
          
          <category> MRC-Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> paper </tag>
            
            <tag> mrc </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension</title>
      <link href="/2019/01/12/paper-emnlp2018-dcu/"/>
      <url>/2019/01/12/paper-emnlp2018-dcu/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension<br>EMNLP 2018<br>Yi Tay et al.<br>多粒度/尺度序列编码; 膨胀组合单元</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>Sequence encoder是MRC中的重要部件: helps to model compositionality of words, capturing rich and complex linguistic and syntactic structure in language</li><li>Sequence encoder的问题:<ul><li>文档较长，文档数多时计算开销大;</li><li>限制获取长距离上下文;</li><li>限制了多句和文档内部的推理;</li></ul></li><li>本文工作: 采取组合式编码方式<ul><li>主要思路是将多个尺度的信息组合在一起进行编码，利用多尺度 n 元语法信息来实现语义融合，得到更好的文档表达，用于后续的推理和Attention操作.</li><li>设计了一种 dilated compositions 机制来建模多个尺度之间的关系，相当于通过门控的方式决定要保留多少信息<ul><li>多尺度 包括：word-level、phrase-level、sentence-level、paragraph-level etc.</li><li>一种 divide-and-conquer 的序列编码方式</li></ul></li></ul></li><li>本文贡献:<ul><li>提出了一个 compositional encoder DCU (Dilated Compositional Units), DCU 既可以进行独立编码，也可以以RNN-style的方式进行更有表示能力的编码；</li><li>DCU 可以加速序列编码速度，并且保持相邻词之间的交互关系；</li><li>建模长句子时，模型可以获得前方更多的信息，是对所有上下文的全局概览；</li><li>相当于一个门控单元对不同粒度的关系进行建模，有利于捕获文档内部细粒度关系；</li></ul></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul><li>DCU用于MRC时的整体结构，左侧为DCU Encoder，右侧为DCU分别用于span prediction model和Multiple choice model的结构:<br><img src="/images/paper-emnlp2018-dcu/dcu-1-overall.png" alt="dcu-model"></li></ul><p>本节只介绍 DCU 的操作和 encoding 操作，不对整体的mrc模型进行介绍</p><h3 id="Dilated-Compositional-Mechanism"><a href="#Dilated-Compositional-Mechanism" class="headerlink" title="Dilated Compositional Mechanism"></a>Dilated Compositional Mechanism</h3><p>Notations:</p><ul><li>Input Sequence: $S=[w_1, w_2, …, w_l]$</li><li>Range list: $R={r_1, r_2, …, r_k}$<ul><li>$k$ 表示进行 $k$ 次 fold/unfold 操作</li></ul></li></ul><h4 id="1-Fold"><a href="#1-Fold" class="headerlink" title="1.Fold"></a>1.Fold</h4><p>对于每个 $r_j$:</p><ul><li>将$S$中的每 $r_j$ 个词进行串接(concat, neural bag-of-words 表示), 原输入长度缩减为 $l/r_j$</li><li>对于新得到的、包含 $l/r_j$ 个 tokens/blocks 的序列中的每个表示进行如下计算:<ul><li>$\bar{w}_t = \sigma_r(W_a(w_t)) + b_a$</li><li>$W_a \in \mathbb{R}^{d\times d}, b\in \mathbb{R}^d$</li></ul></li><li>Fold 的操作次数等于 range list 的大小</li><li>对于 range list 中不同的 $r$ 值, 参数 $W_a$ 和 $b_a$ 不共享</li></ul><h4 id="2-Unfold"><a href="#2-Unfold" class="headerlink" title="2.Unfold"></a>2.Unfold</h4><p>将transformed之后的序列展开为原长</p><ul><li>下图中为 $r_j=2$ 时的 Fold-Unfold 操作:<br><img src="/images/paper-emnlp2018-dcu/dcu-2-op.png" alt="dcu-fold-unfold"></li></ul><h4 id="3-Multi-Granular-Reasoning"><a href="#3-Multi-Granular-Reasoning" class="headerlink" title="3.Multi-Granular Reasoning"></a>3.Multi-Granular Reasoning</h4><p>多尺度推理</p><ul><li>将不同尺度的unfold之后的token表示进行串接，然后通过两层前馈神经网络得到一个门向量<ul><li>$g_t = F_2(F_1([w_1^t,w_2^t,…,w_t^k]))$</li><li>$F(\cdot) = ReLU(Wx+b)$</li></ul></li><li>$g_t$ 相当于一个从多尺度中学习的门控向量，尺度值最低的那些词会拥有相同的 $g_t$ 值</li></ul><h3 id="Encoding-Operation"><a href="#Encoding-Operation" class="headerlink" title="Encoding Operation"></a>Encoding Operation</h3><h4 id="1-Simple-Encoding"><a href="#1-Simple-Encoding" class="headerlink" title="1.Simple Encoding"></a>1.Simple Encoding</h4><ul><li>$z_t = tanh(W_p w_t) + b_p$</li><li>$y_t = \sigma(g_t) \ast w_t + (1-\sigma(g_t))z_t$</li></ul><h4 id="2-Recurrent-Encoding"><a href="#2-Recurrent-Encoding" class="headerlink" title="2.Recurrent Encoding"></a>2.Recurrent Encoding</h4><p>DCU 相当于循环神经网络中的 cell</p><ul><li>$c_t = g_t \odot c_{t-1} + (1-g_t)\odot z_t$</li><li>$o_t = W_o(w_t) + b_o$</li><li>$h_t = o_t \odot c_t$</li></ul><blockquote><p>问题：<br>此处有一点疑问是，为什么通过DCU得到的门控向量 $g_t$ 没有参与到后续的编码过程, 而只是作为了控制初始 $w_t$ 词向量的门控输入</p></blockquote><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><ul><li>Multi-Choice 模型:<ul><li>数据输入: include the standard EM (exact match) binary feature to each word. In this case, we use a three-way EM adaptation, i.e., EM(P, Q), EM(Q, A) and EM(P, A). The projected embeddings are then passed into a single layered highway network</li><li>输出(答案选择层): 将每个候选答案的答案向量转化为标量<ul><li>$a_j^f=softmax(W_2(\sigma_r(W_1([a_j])+b_1)+b_2))$</li></ul></li></ul></li><li>range valuse: ${1,2,4,10,25}$</li><li>最大序列长度 (RACE/SearchQA/NarrativeQA): $500/200/1100$</li></ul><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>只关注了一下RACE和NarrativeQA上的结果，在没有使用RNN-based编码器的情况下，取得了不错的效果，在论文完成时(2018.03)是RACE上的top-1</p><h4 id="RACE"><a href="#RACE" class="headerlink" title="RACE"></a>RACE</h4><p><img src="/images/paper-emnlp2018-dcu/dcu-results-race.png" alt="results-race"></p><h4 id="NarrativeQA"><a href="#NarrativeQA" class="headerlink" title="NarrativeQA"></a>NarrativeQA</h4><p><img src="/images/paper-emnlp2018-dcu/dcu-results-narrativeqa.png" alt="results-nqa"></p><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><ul><li>对于长文本的编码问题是MRC中的重点问题之一【—&gt;表示问题】<ul><li>长文本和多篇文本</li></ul></li><li>本文提供了一种跨尺度的交互，或是融合跨尺度的信息的有效方式</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> reasoning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WIP| On the Capabilities and Limitations of Reasoning for Natural Language Understanding</title>
      <link href="/2019/01/11/paper-1901-02522/"/>
      <url>/2019/01/11/paper-1901-02522/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>[Under reading]</p><blockquote><p>On the Capabilities and Limitations of Reasoning for Natural Language Understanding<br>Khashabi et al.<br>作者来自 University of Pennsylvania, Indiana University 和 AllennAI<br>题目: 论自然语言理解推理的能力与局限</p></blockquote><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>一些NLU系统在克服查找 Style Reasoning 的语言可变性(linguistic variability)方面具有很强的实力, 但是他们的准确率会随着推理步数的增加而下降.<ul><li>key: style reasoning; linguistic variability; reasoning step;</li></ul></li><li>本文基于上述观察第一次提出了一个正式框架, 旨在解决<strong>使用语言表示隐藏概念空间时引入的</strong>:<ul><li><span id="inline-blue">1.模糊性(ambiguity) </span></li><li><span id="inline-blue">2.冗余性(redundancy) </span></li><li><span id="inline-blue">3.不完整性(incompleteness) </span></li><li><span id="inline-blue">4.不准确性(inaccuracy) </span></li></ul></li><li>模型使用了两个相互关联的(interrelated)空间:<ul><li><span id="inline-green">conceptual meaning space</span>: unambiguous and complete but hidden.</li><li><span id="inline-yellow">linguistic symbol space</span>: captures a noisy grounding of the meaning space in the symbols or words of a language.</li></ul></li><li>本文引用此框架来研究无向图中的连通性(connectivity)问题: 是构成更复杂的多步推理的基础核心推理问题<ul><li>证明了构建高质量算法来检测 latent meaning graph 中的连通性是可能的, 前提: 基于一个可观察的 noisy symbol graph, 并且这些噪声低于我们定量的噪声等级.</li><li>此外, 证明了一个不可能的结果: 如果一个query需要大量的推理步数, no reasoning system operating over the symbol graph is likely to recover any useful property of the meaning graph.</li><li>这一点同时强调了对于推理问题和系统，<strong>需要的是限制两个空间的距离</strong>，而不是投入更多的推理步数(hops)</li></ul></li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>Reasoning的定义: the process of combining facts(事实) and beliefs(信念), in order to make decisions.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> nlu </tag>
            
            <tag> reasoning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Text Summarization - Main Problems</title>
      <link href="/2019/01/09/nlp-challenge-ts/"/>
      <url>/2019/01/09/nlp-challenge-ts/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>文本摘要中的主要问题与挑战</p><h2 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h2><p>区分是挑战还是技术问题</p><h3 id="1-关注重点的词、句；"><a href="#1-关注重点的词、句；" class="headerlink" title="1.关注重点的词、句；"></a>1.关注重点的词、句；</h3><ul><li>Attention、pos、tf-idf</li><li>intra-temporal attention</li><li>encoder-less self-attention 【generating Wikipedia by Summarizing long Sequences】</li></ul><h3 id="2-不准确的复制-生成事实细节；"><a href="#2-不准确的复制-生成事实细节；" class="headerlink" title="2.不准确的复制\生成事实细节；"></a>2.不准确的复制\生成事实细节；</h3><ul><li>不准确的复制\生成事实细节；<ul><li>copy</li></ul></li></ul><h3 id="3-重复性短语、句子"><a href="#3-重复性短语、句子" class="headerlink" title="3.重复性短语、句子"></a>3.重复性短语、句子</h3><ul><li>重复性短语、句子；<ul><li>temporal Attention</li><li>coverage</li><li>intra-attention</li></ul></li></ul><h3 id="4-处理长文档"><a href="#4-处理长文档" class="headerlink" title="4.处理长文档"></a>4.处理长文档</h3><ul><li>处理长文档；<ul><li>Sentence-level Attention</li><li>selective gate</li><li>self-attention：缓解长距离依赖</li></ul></li></ul><h3 id="5-生成可读性好的摘要"><a href="#5-生成可读性好的摘要" class="headerlink" title="5.生成可读性好的摘要"></a>5.生成可读性好的摘要</h3><ul><li>生成可读性好的摘要；<ul><li>RL</li></ul></li></ul><h3 id="6-生成新的词（基于理解的基础上）；"><a href="#6-生成新的词（基于理解的基础上）；" class="headerlink" title="6.生成新的词（基于理解的基础上）；"></a>6.生成新的词（基于理解的基础上）；</h3><ul><li>生成新的词（基于理解的基础上）；</li></ul><h3 id="7-词汇问题"><a href="#7-词汇问题" class="headerlink" title="7.词汇问题"></a>7.词汇问题</h3><ul><li>罕见词（rare but important）、未登录词；<ul><li>add n-gram match term to loss【A neural Attention model for Sentence Summarization】</li><li>pointer  </li></ul></li><li>使用大规模词典；candidate sampling</li></ul><h2 id="关注要解决的问题："><a href="#关注要解决的问题：" class="headerlink" title="关注要解决的问题："></a>关注要解决的问题：</h2><ul><li>如何在生成过程中使decoder的注意力更集中，使Attention更聚焦，由于输入序列的长度 比较长？即便使用Attention模型，也不能很好的聚焦到对应的源端token（loss focus）；<ul><li>encoder的输出在用于Attention计算时包含噪声</li><li>使重点更突出；而不是过滤？区别？</li></ul></li><li>copy 机制的贡献程度？<ul><li>以及coverage  </li></ul></li><li>如何判定信息冗余与信息丢失</li></ul>]]></content>
      
      
      <categories>
          
          <category> Text Summarization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> research </tag>
            
            <tag> challenge </tag>
            
            <tag> ts </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Machine Translation - Main Problems</title>
      <link href="/2019/01/09/nlp-challenge-nmt/"/>
      <url>/2019/01/09/nlp-challenge-nmt/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>神经机器翻译中的主要问题与挑战</p><h2 id="1-难以跨领域"><a href="#1-难以跨领域" class="headerlink" title="1.难以跨领域"></a>1.难以跨领域</h2><p>神经机器翻译（NMT）在处理领域之外的数据时的表现很糟：<br>当前的机器翻译系统会生成非常流畅的输出，这些输出与领域外数据的输入无关。因此像Google翻译这样的通用机器翻译系统在法律或金融等专业领域的表现尤其糟糕。与基于短语的系统等传统方法相比，NMT系统的效果更差。</p><h2 id="2-小数据集上表现不佳"><a href="#2-小数据集上表现不佳" class="headerlink" title="2.小数据集上表现不佳"></a>2.小数据集上表现不佳</h2><p>NMT在小数据集上表现不佳：一般而言，大多数机器学习都是这样，但这个问题在NMT上尤为突出。 NMT的优点在于，随着数据量的增加，它的表现要（比基于短语的机器翻译）更好，但在数据量很低的情况下，NMT的表现确实更差。事实上，正如作者所说，“在资源条件较差的情况下，NMT会产生与输入内容无关的流畅输出。”这可能是Motherboard的文章探讨的一些关于NMT表现奇怪的另一个原因。</p><h2 id="3-罕见词汇"><a href="#3-罕见词汇" class="headerlink" title="3.罕见词汇"></a>3.罕见词汇</h2><p>NMT在罕见词汇上的表现不佳：尽管比基于短语的翻译的表现更好，但NMT对于罕见或未见过的词语翻译的表现不佳。对于存在大量变形词的语言及大量命名实体的领域，这可能成为一个问题，因为变形词和命名实体一般非常罕见。</p><h2 id="4-长句翻译"><a href="#4-长句翻译" class="headerlink" title="4.长句翻译"></a>4.长句翻译</h2><p>长句的翻译问题：对长句编码及生成长句仍然是一个没有解决的问题。 机器翻译系统随句子长度的增加，其表现会越来越糟，NMT系统尤其如此。使用注意力有帮助，但问题远未“解决”。在许多领域，如法律领域，冗长复杂的句子是很常见的。</p><h2 id="5-注意力机制不等于简单对齐"><a href="#5-注意力机制不等于简单对齐" class="headerlink" title="5.注意力机制不等于简单对齐"></a>5.注意力机制不等于简单对齐</h2><p>注意力（Attention）机制不等于简单对齐：这是一个非常微妙但重要的问题。在传统的SMT系统（如基于短语的MT）中，对齐翻译为模型的检测提供了有用的调试信息。但是注意机制不能被视为传统意义上的对齐，即使论文经常将注意力机制作为“软对齐”引起注意。在NMT系统中，除了源语言中的动词之外，目标语言中的动词也可以作为主语和宾语成分。</p><h2 id="6-翻译质量"><a href="#6-翻译质量" class="headerlink" title="6.翻译质量"></a>6.翻译质量</h2><ul><li>难以控制翻译质量：每个单词都有多种翻译，典型的机器翻译系统在源句的翻译结构上表现很好。为了保持句子结构的大小合理，会使用集束搜索（beam search）。通过改变集束宽度，可以找到低概率但正确的平移。而对于NMT系统，调整集束的宽度似乎没有任何影响，甚至可能会有不良影响。</li><li>翻译的忠实度（adequacy）与流畅度（fluency）</li><li>信达雅</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>参考链接：<a href="http://deliprao.com/archives/301" target="_blank" rel="noopener">http://deliprao.com/archives/301</a></li><li>论文地址：<a href="http://www.aclweb.org/anthology/W/W17/W17-3204.pdf" target="_blank" rel="noopener">http://www.aclweb.org/anthology/W/W17/W17-3204.pdf</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NMT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> research </tag>
            
            <tag> challenge </tag>
            
            <tag> nmt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019年讨论组日程记录</title>
      <link href="/2019/01/09/schedule-2019/"/>
      <url>/2019/01/09/schedule-2019/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="2019年讨论组论文报告会"><a href="#2019年讨论组论文报告会" class="headerlink" title="2019年讨论组论文报告会"></a>2019年讨论组论文报告会</h1><a id="more"></a><style>    table th:nth-of-type(1){    width: 90px;    }    table th:nth-of-type(2){    width: 90px;    }    table th:nth-of-type(3){    width: 50%;    }    table th:nth-of-type(4){    width: 100px;    }</style><div class="table-container"><table><thead><tr><th style="text-align:left">Date</th><th style="text-align:left">Reporter</th><th style="text-align:left">Title/Papers</th><th style="text-align:left">Appx.</th></tr></thead><tbody><tr><td style="text-align:left">01.13</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Multi-Granular Sequence Encoding via Dilated Compositional Units for RC</td><td style="text-align:left"><a href="http://aclweb.org/anthology/D18-1238" target="_blank" rel="noopener">EMNLP2018</a><br><a href="/2019/01/12/paper-emnlp2018-dcu/" title="note link">note link</a></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">孙雅静</td><td style="text-align:left">The Design and Implementation of XiaoIce, an Empathetic Social Chatbot</td><td style="text-align:left"><a href="https://arxiv.org/abs/1812.08989" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Analysis on MS MARCOv1 Leaderboard Top 3:<br>1.S-Net<br>2.V-Net<br>3.MARS<br>Multi-task Learning</td><td style="text-align:left"><a href="https://github.com/IndexFziQ/MSMARCO-MRC-Analysis" target="_blank" rel="noopener">details-1</a><br><a href="https://github.com/IndexFziQ/Thinking-about-Multi-Task-Learning" target="_blank" rel="noopener">details-2</a></td></tr><tr><td style="text-align:left">01.18</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Self-Supervised Learning Introduction</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.18</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Meta-Learning Introduction</td><td style="text-align:left"><a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html" target="_blank" rel="noopener">blog</a></td></tr><tr><td style="text-align:left">01.18</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Meta-Learning a Dynamic Language Model</td><td style="text-align:left">ICLR workshop</td></tr><tr><td style="text-align:left">01.18</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Continual Leanring Introduction</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.24</td><td style="text-align:left">孙雅静</td><td style="text-align:left">1. A Dual-Attention Hierarchical RNN for Dialogue Act Classification<br>2.LifeLong Learning with Dynamically Expandable Networks</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.24</td><td style="text-align:left">谢玉强</td><td style="text-align:left">A Survey to Self-Supervised Learning</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.24</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Self-Supervised Learning<br>Unsupervised Learning by Cross-Channel Prediction</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.24</td><td style="text-align:left">彭伟</td><td style="text-align:left">Attention-over-Attention NN for RC</td><td style="text-align:left">ACL,2017</td></tr><tr><td style="text-align:left">02.20</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability</td><td style="text-align:left">ACL,2017</td></tr><tr><td style="text-align:left">02.20</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Multi-Style Generative Reading Comprehension</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">02.20</td><td style="text-align:left">彭伟</td><td style="text-align:left">Match-LSTM</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">02.20</td><td style="text-align:left">孙雅静</td><td style="text-align:left">个性化对话:<br>1.Persona-Chat对话数据集<br>2.Personalizing a Dialogue Systems with Transfer Reinforcement Learning</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">02.20</td><td style="text-align:left">于静</td><td style="text-align:left">Lifelong Learning Cross Media Search</td><td style="text-align:left">ACM-MM</td></tr><tr><td style="text-align:left">03.01</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">TG with Commonsense</td><td style="text-align:left"><a href="/2019/01/23/paper-tg-with-commonsense/" title="tg-with-cs-note">tg-with-cs-note</a></td></tr><tr><td style="text-align:left">03.01</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Incorporating Structured Commonsense Knowledge in Story Completion</td><td style="text-align:left">AAAI,2019</td></tr><tr><td style="text-align:left">03.01</td><td style="text-align:left">孙雅静</td><td style="text-align:left">What makes a good conversation? How controllable attributes affect human judgments</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.08</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Learning Personalized End-to-End Goal-Oriented Dialog</td><td style="text-align:left">AAAI,2019</td></tr><tr><td style="text-align:left">03.08</td><td style="text-align:left">于静</td><td style="text-align:left">1.Compositional Attention Networks for Machine Reasoning<br>2.Visual Dialog</td><td style="text-align:left">ICLR,2018<br>CVPR,2017</td></tr><tr><td style="text-align:left">03.08</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Commonsense for Generative Multi-Hop Question Answering Tasks</td><td style="text-align:left"><a href="http://xingluxi.github.io/2019/02/21/paper-emnlp2018-mhpgm/">details</a></td></tr><tr><td style="text-align:left">03.09</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">1.Cross-Lingual Language Model Pretraining<br>2.An Effective Approach to Unsupervised Machine Translation</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.09</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Self-supervised learning in video and multi-modal</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.09</td><td style="text-align:left">彭伟</td><td style="text-align:left">Dynamic Coattention Networks For Question Answering</td><td style="text-align:left">ICLR,2017</td></tr><tr><td style="text-align:left">03.15</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Improving Machine Reading Comprehension with General Reading Strategies</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.15</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Improving Question Answering with External Knowledge</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.15</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Learning to Select Knowledge for Response Generation in Dialog System</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.22</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering</td><td style="text-align:left">ACL, 2018</td></tr><tr><td style="text-align:left">03.22</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Tackling the Story Encding Biases in The Story Cloze Test</td><td style="text-align:left">ACL,2018</td></tr><tr><td style="text-align:left">03.22</td><td style="text-align:left">于静</td><td style="text-align:left">文本匹配模型介绍</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.29</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Multi-Choice Machine Reading Comprehension Task - PartI</td><td style="text-align:left"><a href="/2019/03/28/mrc-analysis-multichoice/" title="details">details</a></td></tr><tr><td style="text-align:left">03.29</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Sentence embedding Alignment for LifeLong Relation Extraction</td><td style="text-align:left">NAACL,2019</td></tr><tr><td style="text-align:left">04.12</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Does it care what you asked? Understanding Importance of Verbs in QA Model</td><td style="text-align:left">EMNLP,2018 workshop</td></tr><tr><td style="text-align:left">04.12</td><td style="text-align:left">孙雅静</td><td style="text-align:left">1.Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems<br>2.Global-to-Local Memory Pointer Networks for Task-Oriented Dialogue</td><td style="text-align:left">ACL,2018<br>ICLR,2019</td></tr><tr><td style="text-align:left">04.12</td><td style="text-align:left">李云鹏</td><td style="text-align:left">Relation Extraction Survey - 1</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.19</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Commonsense Reasoning for Natural Language Understanding</td><td style="text-align:left"><a href="/2019/04/18/mrc-cs-reasoning-for-nlu-survey/" title="detail">detail</a></td></tr><tr><td style="text-align:left">05.24</td><td style="text-align:left">彭伟</td><td style="text-align:left">1.A Deep Cascade Model for Multi-Document Reading Comprehension<br>2.Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.24</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">MASS: Masked Sequence to Sequence Pre-training for Language Generation</td><td style="text-align:left">ICML,2019</td></tr><tr><td style="text-align:left">05.31</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">HotpotQA at ACL 2019</td><td style="text-align:left"><a href="http://xingluxi.github.io/2019/05/30/mrc-paper-hotpotqa/">details</a></td></tr><tr><td style="text-align:left">05.31</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Challenges in Building Intelligent Open-domain Dialog Systems</td><td style="text-align:left">2019</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td></tr></tbody></table></div><h1 id="往期内容列表"><a href="#往期内容列表" class="headerlink" title="往期内容列表"></a>往期内容列表</h1><ul><li><a href="/2019/01/09/schedule-2017-md/" title="2017年讨论组内容列表">2017年讨论组内容列表</a></li><li><a href="/2019/01/09/schedule-2018-md/" title="2018年讨论组内容列表">2018年讨论组内容列表</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Group-Discussion </category>
          
      </categories>
      
      
        <tags>
            
            <tag> schedule </tag>
            
            <tag> group </tag>
            
            <tag> discussion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KIM</title>
      <link href="/2019/01/09/paper-kim/"/>
      <url>/2019/01/09/paper-kim/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Neural Natural Language Inference Models Enhanced with External Knowledge<br>ACL,2018.<br>Qian Chen, et al.<br>offical code link: <a href="https://github.com/lukecq1231/kim" target="_blank" rel="noopener">here</a>.</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>作者针对NLI任务提出了一个问题：</p><ul><li>是否可以从数据中学习NLI所需要的所有知识？</li><li>如果不能，如何使外部知识帮助神经网络模型？如何构建NLI模型利用外部知识？</li></ul><p>本文的工作是作者基于其ESIM模型之上完成的</p><h2 id="Model-Framework"><a href="#Model-Framework" class="headerlink" title="Model Framework"></a>Model Framework</h2><ul><li>模型整体结构分为四部分:<ul><li><img src="/images/paper-kim/kim.png" alt="kim"></li><li>1、representing</li><li>2、collecting local inference information</li><li>3、aggregating and composing local information</li><li>4、make global decision at sentence level</li><li>其中，将外部知识加入到了: co-attention、local inference collection、inference composition 模块中</li></ul></li></ul><h3 id="External-Knowledge"><a href="#External-Knowledge" class="headerlink" title="External Knowledge"></a>External Knowledge</h3><p>首先确定需要哪些知识来帮助NLI任务</p><ul><li>external, inference-related knowledge</li><li>intuitively Knowledge: <strong>synonymy(同义)</strong>、<strong>antonymy(反义)</strong>、<strong>hypernymy(上位)</strong>、<strong>hyponymy(下位)</strong><ul><li>上下位的关系可以帮助捕捉 entailment 信息</li><li>反义和co-hyponymy(共享相同上位词)的关系可以更好的建模 contradiction 关系</li></ul></li><li>这篇文章主要是引入了基础的词汇级的语义知识<ul><li>建模词$w_i$和$w_j$知识为 $r_{ij}$</li><li>重点是：如何构建 $r_ij$</li></ul></li></ul><h3 id="M1-Encoding-Premise-and-Hypothesis"><a href="#M1-Encoding-Premise-and-Hypothesis" class="headerlink" title="M1.Encoding Premise and Hypothesis"></a>M1.Encoding Premise and Hypothesis</h3><ul><li>premise: $a=(a_1,…,a_m)$</li><li>hypothesis: $b=(b_1,…,b_n)$</li><li>通过BiLSTM进行编码，得到context-dependent 隐藏状态：<ul><li>$a_i^s = Encoder(E(a),i)$</li><li>$b_j^s = Encoder(E(b),j)$</li></ul></li></ul><h3 id="M2-Knowledge-Enriched-Co-Attention"><a href="#M2-Knowledge-Enriched-Co-Attention" class="headerlink" title="M2.Knowledge-Enriched Co-Attention"></a>M2.Knowledge-Enriched Co-Attention</h3><ul><li>知识关系特征：$r_{ij} \in \mathbb{R}^{d_r}$</li><li>Co-attention:<ul><li>$e_{ij} = (a^s_i)^T b_j^s + F(r_{ij})$</li><li>$F(r_{ij}) = \lambda \mathbb{1}(r_{ij})$<ul><li>$\mathbb{1}(r_{ij})$ 判断 $r_{ij}$ 是否为0向量</li></ul></li><li>得到的 co-attention 矩阵 $e \in \mathbb{R}^{m\times n}$</li></ul></li><li>根据co-attention对premise和hypothesis的表示进行更新：<ul><li><script type="math/tex; mode=display">a_i^c=\sum_{j=1}^n \alpha _{ij} b_j^s</script><ul><li><script type="math/tex; mode=display">\alpha_{ij} = exp(e_{ij}) / \sum_{k=1}^n exp(e_{ik})</script></li></ul></li><li><script type="math/tex; mode=display">b_j^c=\sum_{i=1}^m \beta _{ij} a_i^s</script><ul><li><script type="math/tex; mode=display">\beta_{ij} = exp(e_{ij}) / \sum_{k=1}^m exp(e_{kj})</script></li></ul></li><li>$\alpha \in \mathbb{R}^{m\times n}$、$\beta \in \mathbb{R}^{m\times n}$</li></ul></li></ul><h3 id="M3-Local-inference-Collection-with-External-Knowledge"><a href="#M3-Local-inference-Collection-with-External-Knowledge" class="headerlink" title="M3.Local inference Collection with External Knowledge"></a>M3.Local inference Collection with External Knowledge</h3><ul><li>通过比较$a_i^s$、$a_i^c$ 和 他们的关系（从外部知识获得），可以获得词级的推理信息</li><li>Knowledge-enriched local inference:<ul><li><script type="math/tex; mode=display">a_i^m = G([a_i^s; a_i^c; a_i^s - a_i^c; a_i^s \circ a_i^c; \sum_{j=1}^n \alpha_{ij}r_{ij}])</script></li><li><script type="math/tex; mode=display">b_j^m = G([b_j^s; b_j^c; b_j^s - b_j^c; b_j^s \circ b_j^c; \sum_{i=1}^m \beta_{ij}r_{ji}])</script></li><li>最后一项的目的是：收集对齐词之间的关系特征，是从外部知识获得的word-level inference information</li><li>$G$ 是非线性映射，用于降维，relu + shortcut connection<ul><li><script type="math/tex; mode=display">a_i^m + \sum_{j=1}^n \alpha_{ij}r_{ij}</script></li><li><script type="math/tex; mode=display">b_j^m + \sum_{i=1}^m \beta_{ij}r_{ij}</script></li></ul></li></ul></li></ul><h3 id="M4-Knowledge-Enhanced-Inference-Composition"><a href="#M4-Knowledge-Enhanced-Inference-Composition" class="headerlink" title="M4.Knowledge-Enhanced Inference Composition"></a>M4.Knowledge-Enhanced Inference Composition</h3><ul><li>决定总体的推理关系：BiLSTM —&gt; mean;max;weighted Pooling，得到定长向量<ul><li>Composition = BiLSTM<ul><li>$a_i^v = Composition(a^m,i)$</li><li>$b_j^v = Composition(b^m,j)$</li></ul></li><li>Weighted Pooling<ul><li><script type="math/tex; mode=display">a^w = \sum_{i=1}^m \left( \frac{exp(H(\sum_{j=1}^n \alpha_{ij}r_{ij}))}{\sum_{i=1}^m exp(H(\sum_{j=1}^n\alpha_{ij}r_{ij}))} \right) a_i^v</script></li><li><script type="math/tex; mode=display">b^w = \sum_{j=1}^n \left( \frac{exp(H(\sum_{i=1}^m \beta_{ij}r_{ji}))}{\sum_{j=1}^n exp(H(\sum_{i=1}^m\beta_{ij}r_{ji}))} \right) b_j^v</script></li><li>$H$ 函数是 1层的前馈神经网络，激活函数是relu</li></ul></li></ul></li><li>最后得到定长向量之后，再过一层MLP，激活函数为tanh，和一层 softmax，得到分类结果</li></ul><h2 id="Experiment-Settings"><a href="#Experiment-Settings" class="headerlink" title="Experiment Settings"></a>Experiment Settings</h2><h3 id="Representation-of-External-Knowledge"><a href="#Representation-of-External-Knowledge" class="headerlink" title="Representation of External Knowledge"></a>Representation of External Knowledge</h3><h4 id="Lexical-Semantic-Relations"><a href="#Lexical-Semantic-Relations" class="headerlink" title="Lexical Semantic Relations"></a>Lexical Semantic Relations</h4><ul><li>relations of lexical pairs: 检索范围是wordnet<ul><li>synonymy: 如果词对是同义词，使值为1，否则为0</li><li>antonymy: 如果词对是反义词，使值为1，否则为0</li><li>hypernymy: 如果一个词是另一个词的上位词，取值 $1-\frac{n}{8}$，否则为0<ul><li>其中 $n$ 是两个词在层次上的边数</li><li>忽略边数超过8的</li></ul></li><li>hyponymy: inverse of the hypernymy feature</li><li>co-hyponymys: 如果两个词有相同的上位词且不是同义词，取值为1，否则为0</li><li>向量 $r$ 的维度为 $d_r = 5$</li><li>在 wordnet 中还有额外的一些（15种）关系，但是对结果的提升没有贡献</li></ul></li><li>relation embeddings<ul><li>pretrain based on wordnet</li><li>TransE</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> note </tag>
            
            <tag> nli </tag>
            
            <tag> wordnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Knowledgeable Reader Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge</title>
      <link href="/2019/01/09/paper-knreader/"/>
      <url>/2019/01/09/paper-knreader/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Title: Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge<br>Authors: Todor Mihaylov, et al.<br>Published: ACL,2019</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p><img src="/../images/paper-knreader/4.jpeg" alt="1"><br><img src="/../images/paper-knreader/5.jpeg" alt="2"></p><h2 id="Model-Framework"><a href="#Model-Framework" class="headerlink" title="Model Framework"></a>Model Framework</h2><p><img src="/../images/paper-knreader/7.jpeg" alt="4"><br><img src="/../images/paper-knreader/8.jpeg" alt="4"><br><img src="/../images/paper-knreader/9.jpeg" alt="4"><br><img src="/../images/paper-knreader/10.jpeg" alt="4"><br><img src="/../images/paper-knreader/11.jpeg" alt="4"><br><img src="/../images/paper-knreader/12.jpeg" alt="4"><br><img src="/../images/paper-knreader/13.jpeg" alt="4"><br><img src="/../images/paper-knreader/14.jpeg" alt="4"><br><img src="/../images/paper-knreader/15.jpeg" alt="4"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="/../images/paper-knreader/17.jpeg" alt="5"><br><img src="/../images/paper-knreader/18.jpeg" alt="5"><br><img src="/../images/paper-knreader/19.jpeg" alt="5"></p><!-- ![knreader](/images/paper-knreader/knreader.png) -->]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> cloze </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Knowledge-based NLU-Papers</title>
      <link href="/2019/01/09/kmrc-paper-info/"/>
      <url>/2019/01/09/kmrc-paper-info/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>A list of recent papers on knowledge-based Natural Language Understand.</p><blockquote><p><em>Contributed by Luxi Xing and Yuqiang Xie, National Engineering Laboratory for Information Security Technologies, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China.</em> </p></blockquote><hr><style>    table th:nth-of-type(2){    width: 60%;    }</style><h2 id="NLI-with-Knowledge"><a href="#NLI-with-Knowledge" class="headerlink" title="NLI with Knowledge"></a>NLI with Knowledge</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Conf.</th><th style="text-align:left">Title</th><th style="text-align:left">Authors/Org.</th><th style="text-align:center">Note</th></tr></thead><tbody><tr><td style="text-align:center">ACL<br>2018</td><td style="text-align:left">Neural Natural Language Inference Models Enhanced with External Knowledge</td><td style="text-align:left">Qian Chen</td><td style="text-align:center"><a href="/2019/01/09/paper-kim/" title="KIM-note">KIM-note</a></td></tr><tr><td style="text-align:center">2018</td><td style="text-align:left">Improving Natural Language Inference Using External Knowledge in the Science Questions Domain</td><td style="text-align:left">Xiaoyan Wang</td></tr></tbody></table></div><h2 id="MRC-with-Knowledge"><a href="#MRC-with-Knowledge" class="headerlink" title="MRC with Knowledge"></a>MRC with Knowledge</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Conf.</th><th style="text-align:left">Title</th><th style="text-align:left">Authors/Org.</th><th style="text-align:center">Note</th></tr></thead><tbody><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="https://doi.org/10.18653/v1/P17-1132" target="_blank" rel="noopener">Leveraging knowledge bases in lstms for improving machine reading</a></td><td style="text-align:left">Yang, et al.<br>CMU</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/D17-1086" target="_blank" rel="noopener">World knowledge for reading comprehension: Rare entity prediction with hierarchical lstms using external descriptions</a></td><td style="text-align:left">Long, et al.<br>McGill University</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2018</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P18-1076" target="_blank" rel="noopener">Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge</a></td><td style="text-align:left">Mihaylov, et al.<br>Heidelberg University</td><td style="text-align:center"><a href="/2019/01/09/paper-knreader/" title="knreader-note">knreader-note</a></td></tr><tr><td style="text-align:center">EMNLP<br>2018</td><td style="text-align:left"><a href="https://www.aclweb.org/anthology/D18-1454" target="_blank" rel="noopener">Commonsense for Generative Multi-Hop Question Answering Tasks</a></td><td style="text-align:left">Lisa Bauer</td><td style="text-align:center"><a href="/2019/02/21/paper-emnlp2018-mhpgm/" title="mhpgm-note">mhpgm-note</a></td></tr></tbody></table></div><h2 id="Dialog-with-Knowledge"><a href="#Dialog-with-Knowledge" class="headerlink" title="Dialog with Knowledge"></a>Dialog with Knowledge</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Conf.</th><th style="text-align:left">Title</th><th style="text-align:left">Authors/Org.</th><th style="text-align:center">Note</th></tr></thead><tbody><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P17-1162" target="_blank" rel="noopener">Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings</a></td><td style="text-align:left">He, et al.<br>Stanford</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">IJCAI18</td><td style="text-align:left"><a href="https://www.ijcai.org/proceedings/2018/0643.pdf" target="_blank" rel="noopener">Commonsense Knowledge Aware Conversation Generation with Graph Attention</a></td><td style="text-align:left">Zhou, et al.<br>THU</td><td style="text-align:center"><a href="/2019/01/23/paper-tg-with-commonsense/" title="note">note</a></td></tr><tr><td style="text-align:center">AAAI<br>2018</td><td style="text-align:left"><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/02/A_Knowledge_Grounded_Neural_Conversation_Model.pdf" target="_blank" rel="noopener">A Knowledge-Grounded Neural Conversation Model</a></td><td style="text-align:left">Ghazvininejad, et al.<br>Microsoft Research</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AAAI<br>2018</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1709.04264.pdf" target="_blank" rel="noopener">Flexible End-to-End Dialogue System for Knowledge Grounded Conversation</a></td><td style="text-align:left">Zhu, et al.<br>HKUST</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1902.04911" target="_blank" rel="noopener">Learning to Select Knowledge for Response Generation in Dialog Systems</a></td><td style="text-align:left">Baidu</td></tr></tbody></table></div><h2 id="Text-Generation-with-Knowledge"><a href="#Text-Generation-with-Knowledge" class="headerlink" title="Text Generation with Knowledge"></a>Text Generation with Knowledge</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Conf.</th><th style="text-align:left">Title</th><th style="text-align:left">Authors/Org.</th><th style="text-align:center">Note</th></tr></thead><tbody><tr><td style="text-align:center">AAAI<br>2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1808.10113" target="_blank" rel="noopener">Story Ending Generation with Incremental Encoding and Commonsense Knowledge</a></td><td style="text-align:left">Jian Guan, et al.<br>THU</td><td style="text-align:center"><a href="/2019/01/23/paper-tg-with-commonsense/" title="note">note</a></td></tr></tbody></table></div><h2 id="Representation-with-Knowledge"><a href="#Representation-with-Knowledge" class="headerlink" title="Representation with Knowledge"></a>Representation with Knowledge</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Conf.</th><th style="text-align:left">Title</th><th style="text-align:left">Authors/Org.</th><th style="text-align:center">Note</th></tr></thead><tbody><tr><td style="text-align:center">ICLR<br>2017</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1608.00318v1.pdf" target="_blank" rel="noopener">A Neural Knowledge Language Model</a></td><td style="text-align:left">Ahn.et al.<br>Université de Montréal</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P17-1187" target="_blank" rel="noopener">Improved Word Representation Learning with Sememes</a></td><td style="text-align:left">Niu, et al.<br>THU</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2018</td><td style="text-align:left"><a href="http://aclweb.org/anthology/D18-1033" target="_blank" rel="noopener">Cross-lingual Lexical Sememe Prediction</a></td><td style="text-align:left">Qi, et al.<br>THU</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AAAI<br>2018</td><td style="text-align:left"><a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16321/16167" target="_blank" rel="noopener">Improving Neural Fine-Grained Entity Typing with Knowledge Attention</a></td><td style="text-align:left">Xin, et al.<br>THU</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2019</td><td style="text-align:left"><a href="https://www.aclweb.org/anthology/P19-1571" target="_blank" rel="noopener">Modeling Semantic Compositionality with Sememe Knowledge</a></td><td style="text-align:left">Qi, et al.<br>THU</td><td style="text-align:center"><a href="/2019/08/27/paper-acl2019-sc-with-sememe/" title="note">note</a></td></tr><tr><td style="text-align:center">2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1908.05646" target="_blank" rel="noopener">SenseBERT: Driving Some Sense into BERT</a></td><td style="text-align:left">AI21 Labs</td><td style="text-align:center"><a href="/2019/08/18/paper-2019-sense-bert/" title="note">note</a></td></tr><tr><td style="text-align:center"></td><td style="text-align:left"></td><td style="text-align:left"></td></tr></tbody></table></div><h4 id="Level"><a href="#Level" class="headerlink" title="Level:"></a>Level:</h4><ol><li>KMRC area.</li><li>Relevant research area.</li><li>Heuristic.</li><li>Review.</li></ol><h4 id="Field"><a href="#Field" class="headerlink" title="Field:"></a>Field:</h4><ul><li><strong>KMRC:</strong> <strong>K</strong>nowledge-based <strong>M</strong>achine <strong>R</strong>eading <strong>C</strong>omprehension;</li><li><strong>KDS:</strong> <strong>K</strong>nowledge-based <strong>D</strong>ialogue <strong>S</strong>ystem;</li><li><strong>KIR:</strong> <strong>K</strong>nowledge-based <strong>I</strong>nformation <strong>R</strong>etrieval;</li><li><strong>SC:</strong> <strong>S</strong>ememe <strong>C</strong>omputation;</li><li><strong>NKLM:</strong> <strong>N</strong>eural <strong>K</strong>nowledge <strong>L</strong>anguage <strong>M</strong>odel.</li></ul>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> paper </tag>
            
            <tag> tg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2018年讨论组日程表</title>
      <link href="/2019/01/09/schedule-2018-md/"/>
      <url>/2019/01/09/schedule-2018-md/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="2018年讨论组论文报告会"><a href="#2018年讨论组论文报告会" class="headerlink" title="2018年讨论组论文报告会"></a>2018年讨论组论文报告会</h1><a id="more"></a><style>    /* 第一列表格宽度 */    table th:nth-of-type(1){    width: 90px;    }    /* 第二列表格宽度 */    table th:nth-of-type(2){    width: 90px;    }    /* 第三列表格宽度 */    table th:nth-of-type(3){    width: 50%;    }    /* 第四列表格宽度 */    table th:nth-of-type(4){    width: 100px;    }</style><div class="table-container"><table><thead><tr><th style="text-align:left">Date</th><th style="text-align:left">Reporter</th><th style="text-align:left">Title/Papers</th><th style="text-align:left">Confere.</th></tr></thead><tbody><tr><td style="text-align:left">01.13</td><td style="text-align:left">邢璐茜</td><td style="text-align:left"><a href="https://papers.nips.cc/paper/6775-deliberation-networks-sequence-generation-beyond-one-pass-decoding.pdf" target="_blank" rel="noopener">Deliberation Networks: Sequence Generation Beyond One-Pass Decoding</a></td><td style="text-align:left">NIPS,2017</td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">谢玉强</td><td style="text-align:left">词法分析调研</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">孙雅静</td><td style="text-align:left">句法分析调研</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">彭  伟</td><td style="text-align:left">The Colorful World in the Neural Network</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.17</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">SIX Challenges In the Text Summarization</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.17</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">深度学习与文本分类</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.17</td><td style="text-align:left">孙雅静</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention is all you need</a></td><td style="text-align:left">NIPS,2017</td></tr><tr><td style="text-align:left">03.17</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Neural Word Segmentation Learning for Chinese</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.17</td><td style="text-align:left">彭  伟</td><td style="text-align:left">Neural Network + CRF</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">NLG survey(partly)</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">NLG survey(partly)</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">开题报告</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Neural Word Segmentation Learning for Chinese</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">孙雅静</td><td style="text-align:left">1. <a href="https://arxiv.org/pdf/1711.02281.pdf" target="_blank" rel="noopener">Non autoregressive neural machine translation</a> <br>2. <a href="https://arxiv.org/pdf/1802.06901.pdf" target="_blank" rel="noopener">Deterministic Non autoregressive neural sequence modeling by iterative refinement</a></td><td style="text-align:left">ICLR,2018 <br> arXiv,2018</td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">彭  伟</td><td style="text-align:left">毕设报告 + LSTM-CRF</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.12</td><td style="text-align:left">邢璐茜</td><td style="text-align:left"><a href="https://aclanthology.info/pdf/D/D17/D17-1122.pdf" target="_blank" rel="noopener">Inter-Weighted Alignment Network for Sentence Pair Modeling</a></td><td style="text-align:left">EMNLP,2017</td></tr><tr><td style="text-align:left">05.12</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">知识库问答系统-综述</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.12</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Low Resource NMT 开题预讲</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.12</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Recursive Neural Network Architecture</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.12</td><td style="text-align:left">谢玉强</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/P16-1122" target="_blank" rel="noopener">Inner Attention based Recurrent Neural Networks for Answer Selection</a></td><td style="text-align:left">ACL,2016</td></tr><tr><td style="text-align:left">06.09</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC 综述(Part I)</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">06.09</td><td style="text-align:left">魏相鹏</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/N18-1202" target="_blank" rel="noopener">Deep contextualized word representations</a></td><td style="text-align:left">NAACL,2018<br>best paper</td></tr><tr><td style="text-align:left">06.09</td><td style="text-align:left">雷扬帆</td><td style="text-align:left"><a href="http://aclweb.org/anthology/N18-1038" target="_blank" rel="noopener">Learning Visually Grounded Sentence Representations</a></td><td style="text-align:left">NAACL-HLT,2018</td></tr><tr><td style="text-align:left">06.09</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Generative Adversarial Network</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">06.09</td><td style="text-align:left">谢玉强</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1801.00102" target="_blank" rel="noopener">Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference</a></td><td style="text-align:left">EMNLP,2018</td></tr><tr><td style="text-align:left">06.30</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC 综述(Part II)</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">06.30</td><td style="text-align:left">魏相鹏</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1805.09461" target="_blank" rel="noopener">Deep Reinforcement Learning for Sequence to Sequence Models</a></td><td style="text-align:left">arXiv,2018</td></tr><tr><td style="text-align:left">06.30</td><td style="text-align:left">雷扬帆</td><td style="text-align:left"><a href="https://www.ijcai.org/proceedings/2017/579" target="_blank" rel="noopener">Bilateral Multi-Perspective Matching for Natural Language Sentences</a></td><td style="text-align:left">IJCAI,2017</td></tr><tr><td style="text-align:left">06.30</td><td style="text-align:left">孙雅静</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P18-1087" target="_blank" rel="noopener">Transformation networks for target-oriented sentiment classification</a></td><td style="text-align:left">ACL,2018</td></tr><tr><td style="text-align:left">06.30</td><td style="text-align:left">谢玉强</td><td style="text-align:left"><a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" target="_blank" rel="noopener">Factorization Machines</a></td><td style="text-align:left">ICDM,2010</td></tr><tr><td style="text-align:left">08.03</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">1. <a href="http://anthology.aclweb.org/attachments/P/P18/P18-2047.Notes.pdf" target="_blank" rel="noopener">A Simple and Effective Approach to Coverage-Aware NMT</a><br>2. <a href="http://www.aclweb.org/anthology/P18-2053" target="_blank" rel="noopener">Bag-of-Words as Traget for NMT</a></td><td style="text-align:left">ACL,2018<br>short paper</td></tr><tr><td style="text-align:left">08.03</td><td style="text-align:left">孙雅静</td><td style="text-align:left">对话系统综述</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">08.03</td><td style="text-align:left">谢玉强</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1801.03603" target="_blank" rel="noopener">Syntax-aware Entity Embedding for Neural Relation Extraction</a></td><td style="text-align:left">AAAI,2018</td></tr><tr><td style="text-align:left">08.03</td><td style="text-align:left">彭  伟</td><td style="text-align:left">语言表示</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">08.12</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC 综述(Part III)</td><td style="text-align:left">R-Net<br>FastQA</td></tr><tr><td style="text-align:left">08.12</td><td style="text-align:left">雷扬帆</td><td style="text-align:left"><a href="https://www.ijcai.org/proceedings/2018/0613.pdf" target="_blank" rel="noopener">Multiway Attention Networks for Modeling Sentence Pairs</a></td><td style="text-align:left">IJCAI,2018</td></tr><tr><td style="text-align:left">08.12</td><td style="text-align:left">孙雅静</td><td style="text-align:left">对话系统综述-safe response</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">08.12</td><td style="text-align:left">谢玉强</td><td style="text-align:left">MRC 模型介绍</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">08.12</td><td style="text-align:left">彭伟</td><td style="text-align:left"><a href="http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors" target="_blank" rel="noopener">Learned in Translation: Contextualized Word Vectors</a></td><td style="text-align:left">NIPS,2017</td></tr><tr><td style="text-align:left">08.20</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC: J-Net Exploring Question understanding &amp; Adaptation in NN-based QA</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">08.20</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">VAE与NMT<br>1. <a href="https://arxiv.org/abs/1605.07869" target="_blank" rel="noopener">Variational Neural Machine Translation</a></td><td style="text-align:left">EMNLP,2016</td></tr><tr><td style="text-align:left">08.20</td><td style="text-align:left">孙雅静</td><td style="text-align:left">对话系统: Safe response<br>1. <a href="http://aclweb.org/anthology/D17-1065" target="_blank" rel="noopener">Neural Response Generation via GAN with an Approxiamte Embedding Layer</a><br>2. <a href="http://www.aclweb.org/anthology/P18-1139" target="_blank" rel="noopener">Generating Informative Responses with Controlled Sentence Function</a></td><td style="text-align:left">ACL,2018</td></tr><tr><td style="text-align:left">09.04</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC: Adversarial Evaluation &amp; Unanswerable Question for SQuAD</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.04</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">鲁棒性神经机器翻译</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.04</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">文本匹配</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.04</td><td style="text-align:left">孙雅静</td><td style="text-align:left">对话: 对话一致性 <br>1. <a href="https://arxiv.org/pdf/1603.06155.pdf" target="_blank" rel="noopener">A Persona-Based Neural Conversation Model</a><br>2. <a href="http://aclweb.org/anthology/P17-1162" target="_blank" rel="noopener">Learning Symmetric collaborateve Dialogue with Dynamic Knowledge Graph Embedding</a><br>3.<a href="https://arxiv.org/pdf/1808.07042.pdf" target="_blank" rel="noopener">CoQA: A Conversational Question Answering Challenge</a></td><td style="text-align:left">arXiv,2016<br> ACL,2017 <br>arXiv,2018</td></tr><tr><td style="text-align:left">09.14</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC: Co-Match for Multi-Choice Reading Comprehension</td><td style="text-align:left">ACL,2018</td></tr><tr><td style="text-align:left">09.14</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">文档级神经机器翻译<br>1. <a href="https://arxiv.org/abs/1809.01576" target="_blank" rel="noopener">Document-Level Neural Machine Translation with Hierachical Attention Networks</a></td><td style="text-align:left">EMNLP,2018</td></tr><tr><td style="text-align:left">09.14</td><td style="text-align:left">谢玉强</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1608.07905.pdf" target="_blank" rel="noopener">Match-LSTM</a></td><td style="text-align:left">ICLR,2017</td></tr><tr><td style="text-align:left">09.14</td><td style="text-align:left">孙雅静</td><td style="text-align:left">文本改写<br>1. <a href="http://aclweb.org/anthology/Q18-1031" target="_blank" rel="noopener">Generating Sentences by Editing Prototypes</a></td><td style="text-align:left">TACL,2018</td></tr><tr><td style="text-align:left">09.22</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">CopyNet<br>1. <a href="https://papers.nips.cc/paper/5866-pointer-networks.pdf" target="_blank" rel="noopener">Pointer Network</a><br>2. <a href="http://aclweb.org/anthology/P16-1154" target="_blank" rel="noopener">Incorporating Copying mechanism in Sequence-to-Sequence</a><br>3. <a href="https://arxiv.org/abs/1704.04368" target="_blank" rel="noopener">Get to The Point: Summarization with Pointer-Generator Networks</a></td><td style="text-align:left">NIPS,2015 <br>ACL,2016<br>arXiv,2017</td></tr><tr><td style="text-align:left">09.22</td><td style="text-align:left">魏相鹏</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P18-2048" target="_blank" rel="noopener">Dynamic Sentence Sampling for Efficient Training of NMT</a></td><td style="text-align:left">ACL,2018</td></tr><tr><td style="text-align:left">09.22</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Modeling Sentence Tutorial</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.22</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Two Methods for Training Deeper Networks<br>1. <a href="https://arxiv.org/pdf/1505.00387.pdf" target="_blank" rel="noopener">Highway Network</a><br>2. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Residual Network</a></td><td style="text-align:left">ICML,2015<br>CVPR,2016</td></tr><tr><td style="text-align:left">09.22</td><td style="text-align:left">孙雅静</td><td style="text-align:left">GAN回顾</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.22</td><td style="text-align:left">彭伟</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></td><td style="text-align:left">ICLR,2015</td></tr><tr><td style="text-align:left">10.20</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">The Pre-Training Language Model<br>1. <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></td><td style="text-align:left">arXiv,2018</td></tr><tr><td style="text-align:left">10.20</td><td style="text-align:left">魏相鹏</td><td style="text-align:left"><a href="https://openreview.net/pdf?id=ryza73R9tQ" target="_blank" rel="noopener">Machine Translation with Weakly Paired Bilingual Documents</a></td><td style="text-align:left">ICLR,2019.<br>OpenReview</td></tr><tr><td style="text-align:left">10.20</td><td style="text-align:left">谢玉强</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1805.11360.pdf" target="_blank" rel="noopener">Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information</a></td><td style="text-align:left">AAAI,2019</td></tr><tr><td style="text-align:left">10.20</td><td style="text-align:left">孙雅静</td><td style="text-align:left"><a href="https://aclanthology.info/papers/P17-1171/p17-1171" target="_blank" rel="noopener">Reading Wikipedia to Answer Open-domain Questions</a></td><td style="text-align:left">ACL,2017</td></tr><tr><td style="text-align:left">10.20</td><td style="text-align:left">彭  伟</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention is all you need</a></td><td style="text-align:left">NIPS,2017</td></tr><tr><td style="text-align:left">11.17</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC: Multi-Step Reasoning &amp; Open-Domain QA <br>1. <a href="http://aclweb.org/anthology/P18-1157" target="_blank" rel="noopener">Stochastic Answer Networks for Machine Reading Comprehension</a><br>2. <a href="http://aclweb.org/anthology/P18-1161" target="_blank" rel="noopener">Denoising Distantly Supervised Open-Domain Question Answering</a></td><td style="text-align:left">ACL,2018<br>ACL,2018</td></tr><tr><td style="text-align:left">11.17</td><td style="text-align:left">孙雅静</td><td style="text-align:left">1. <a href="http://www.aclweb.org/anthology/P17-1046" target="_blank" rel="noopener">Sequential Matching Network：A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots</a><br>2. <a href="https://openreview.net/forum?id=ByftGnR9KX" target="_blank" rel="noopener">FlowQA: Grasping Flow in History for Conversational Machine Comprehension</a></td><td style="text-align:left">ACL,2017<br>ICLR,2019.OR</td></tr><tr><td style="text-align:left">11.17</td><td style="text-align:left">彭  伟</td><td style="text-align:left">文本分类<br>1. <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552" target="_blank" rel="noopener">Recurrent convolutional neural networks for text classification</a> <br>2. <a href="http://www.aclweb.org/anthology/N16-1174" target="_blank" rel="noopener">Hierarchical Attention Networks for Document Classification</a></td><td style="text-align:left">AAAI,2015<br>NAACL,2016</td></tr><tr><td style="text-align:left">12.09</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC: external knowledge<br><a href="http://aclweb.org/anthology/P18-1076" target="_blank" rel="noopener">Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge</a></td><td style="text-align:left">ACL,2018</td></tr><tr><td style="text-align:left">12.09</td><td style="text-align:left">魏相鹏</td><td style="text-align:left"><a href="http://aclweb.org/anthology/D18-1045" target="_blank" rel="noopener">Understanding Back-Translation at Scale</a></td><td style="text-align:left">EMNLP,2018</td></tr><tr><td style="text-align:left">12.09</td><td style="text-align:left">彭  伟</td><td style="text-align:left"><a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">End-to-End Memory Networks</a></td><td style="text-align:left">NIPS,2015</td></tr></tbody></table></div><h1 id="相关内容列表"><a href="#相关内容列表" class="headerlink" title="相关内容列表"></a>相关内容列表</h1><ul><li><a href="/2019/01/09/schedule-2017-md/" title="2017年讨论组内容列表">2017年讨论组内容列表</a></li><li><a href="/2019/01/09/schedule-2019/" title="2019年讨论组内容列表">2019年讨论组内容列表</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Group-Discussion </category>
          
      </categories>
      
      
        <tags>
            
            <tag> schedule </tag>
            
            <tag> group </tag>
            
            <tag> discussion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2017年讨论组日程表</title>
      <link href="/2019/01/09/schedule-2017-md/"/>
      <url>/2019/01/09/schedule-2017-md/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="2017年讨论组论文报告会"><a href="#2017年讨论组论文报告会" class="headerlink" title="2017年讨论组论文报告会"></a>2017年讨论组论文报告会</h1><a id="more"></a><style>    /* 第一列表格宽度 */    table th:nth-of-type(1){    width: 90px;    }    /* 第二列表格宽度 */    table th:nth-of-type(2){    width: 90px;    }    /* 第三列表格宽度 */    table th:nth-of-type(3){    width: 50%;    }    /* 第四列表格宽度 */    table th:nth-of-type(4){    width: 100px;    }</style><div class="table-container"><table><thead><tr><th style="text-align:left">日期</th><th style="text-align:left">报告人</th><th style="text-align:left">报告主题</th><th style="text-align:left">简介</th></tr></thead><tbody><tr><td style="text-align:left">03.25</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Generative Adversarial Nets</td><td style="text-align:left"><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">03.25</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Neural Architectures for Named Entity Recognition</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1603.01360.pdf" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">03.25</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">使用字符级解码器的机器翻译</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">CSGAN for Machine Translation</td><td style="text-align:left"><a href="https://arxiv.org/abs/1703.04887" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Hybrid Word-Character Models for NMT</td><td style="text-align:left"><a href="https://arxiv.org/abs/1604.00788" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Character-based Neural Machine Translation</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.06</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">GAN4NLP</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.06</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Neural Machine Translation with Reconstruction</td><td style="text-align:left"><a href="https://arxiv.org/abs/1611.01874" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">05.06</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Minimum Risk Training &amp; Modeling Coverage</td><td style="text-align:left"><a href="https://arxiv.org/abs/1512.02433" target="_blank" rel="noopener">link1</a><br><a href="https://arxiv.org/abs/1601.04811" target="_blank" rel="noopener">link2</a><br></td></tr><tr><td style="text-align:left">05.27</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/P16-1154" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">05.27</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Convolutional Sequence to Sequence Learning</td><td style="text-align:left"><a href="https://arxiv.org/abs/1705.03122" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">05.27</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">多语神经机器翻译</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">06.17</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Context Gates for Neural Machine Translation</td><td style="text-align:left"><a href="https://arxiv.org/abs/1608.06043" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">06.17</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Massive Exploration of Neural Machine Translation Architectures</td><td style="text-align:left"><a href="https://arxiv.org/abs/1703.03906" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">06.17</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">String-based Translation</td><td style="text-align:left"><a href="https://www.isi.edu/natural-language/mt/emnlp16-nmt-grammar.pdf" target="_blank" rel="noopener">link1</a><br><a href="https://arxiv.org/abs/1704.04743" target="_blank" rel="noopener">link2</a><br><a href="https://arxiv.org/abs/1705.01020" target="_blank" rel="noopener">link3</a><br></td></tr><tr><td style="text-align:left">07.22</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Knowledge-Based Semantic Embedding for Machine Translation</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P16-1212" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">07.22</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">What do Neural Machine Translation Models Learn about Morphology?<br>Visualizing and understanding neural machine translation<br></td><td style="text-align:left"><a href="https://arxiv.org/abs/1704.03471" target="_blank" rel="noopener">link1</a><br><a href="http://nlp.csai.tsinghua.edu.cn/~ly/papers/acl2017_dyz.pdf" target="_blank" rel="noopener">link2</a><br></td></tr><tr><td style="text-align:left">07.22</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Dual Supervised Learning</td><td style="text-align:left"><a href="https://arxiv.org/abs/1707.00415" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">08.09</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Pointer Network &amp; It’s application in ACL 16\17</td><td style="text-align:left"><a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">08.16</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Interactive Attention for Neural Machine Translation <br> Neural Machine Translation with Supervised Attention</td><td style="text-align:left"><a href="https://arxiv.org/abs/1610.05011" target="_blank" rel="noopener">link1</a><br><a href="https://arxiv.org/abs/1609.04186" target="_blank" rel="noopener">link2</a></td></tr><tr><td style="text-align:left">09.23</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Plan, Attend, Generate: Char-NMT with Planning</td><td style="text-align:left"><a href="https://arxiv.org/abs/1706.05087" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">09.23</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">AI Challenger</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.23</td><td style="text-align:left">谢玉强</td><td style="text-align:left">RNN</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.23</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Neural Machine Translation with Word Predictions（EMNLP,2017）</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/D17-1013" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">10.14</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">CWMT2017 Review</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">10.14</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Experiments</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">10.14</td><td style="text-align:left">谢玉强</td><td style="text-align:left">A Character-Aware Encoder for Neural Machine Translation（COLING,2016）</td><td style="text-align:left"><a href="http://www.aclweb.org/old_anthology/C/C16/C16-1288.pdf" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">10.14</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Sequence-to-Dependency Neural Machine Translation（EMNLP,2017）</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/P17-1065" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.04</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems (EMNLP,2015)</td><td style="text-align:left"><a href="https://arxiv.org/abs/1508.01745" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.04</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Topic Aware Neural Response Generation (AAAI,2017)</td><td style="text-align:left"><a href="https://arxiv.org/abs/1606.08340" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.04</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</td><td style="text-align:left"><a href="https://arxiv.org/abs/1704.04675" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.04</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Agreement on Target-Bidirectional LSTMs for Sequence-to-Sequence Learning (AAAI,2016) <br> Agreement on Target-Bidirectional Neural Machine Translation（NAACL-HLT,2016）</td><td style="text-align:left"><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12028" target="_blank" rel="noopener">link1</a> <br> <a href="http://www.aclweb.org/anthology/N16-1046" target="_blank" rel="noopener">link2</a></td></tr><tr><td style="text-align:left">11.04</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Gated-Attention Readers for Text Comprehension</td><td style="text-align:left"><a href="https://arxiv.org/abs/1606.01549" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.25</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Unsupervised Machine Translation Using Monolingual Corpora Only</td><td style="text-align:left"><a href="https://arxiv.org/abs/1711.00043" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.25</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Unsupervised Neural Machine Translation</td><td style="text-align:left"><a href="https://arxiv.org/abs/1710.11041" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.25</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Memory Augmented Neural Machine Translation</td><td style="text-align:left">EMNLP,2017<br><a href="https://arxiv.org/abs/1708.02005" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.25</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation</td><td style="text-align:left">ACL,2017 <br> <a href="http://www.aclweb.org/anthology/P17-1140" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">12.16</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Decoding with Value Networks for Neural Machine Translation</td><td style="text-align:left">NIPS,2017 <br> <a href="http://papers.nips.cc/paper/6622-decoding-with-value-networks-for-neural-machine-translation" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">12.16</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Learning to Remember Translation History with Continuous Cache</td><td style="text-align:left">TACL,2018 <br> <a href="https://arxiv.org/abs/1711.09367" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">12.16</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Adversarial Multi-task Learning for Text Classification</td><td style="text-align:left">ACL,2017 <br> <a href="https://arxiv.org/abs/1704.05742" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Deliberation Networks: Sequence Generation Beyond One-Pass Decoding</td><td style="text-align:left">NIPS,2017 <br> <a href="https://papers.nips.cc/paper/6775-deliberation-networks-sequence-generation-beyond-one-pass-decoding.pdf" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">谢玉强</td><td style="text-align:left">词法分析调研</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">孙雅静</td><td style="text-align:left">句法分析调研</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">彭  伟</td><td style="text-align:left">The Colorful World in the Neural Network</td></tr></tbody></table></div><h1 id="相关内容列表"><a href="#相关内容列表" class="headerlink" title="相关内容列表"></a>相关内容列表</h1><ul><li><a href="/2019/01/09/schedule-2018-md/" title="2018年讨论组内容列表">2018年讨论组内容列表</a></li><li><a href="/2019/01/09/schedule-2019/" title="2019年讨论组内容列表">2019年讨论组内容列表</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Group-Discussion </category>
          
      </categories>
      
      
        <tags>
            
            <tag> schedule </tag>
            
            <tag> group </tag>
            
            <tag> discussion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/01/07/hello-world/"/>
      <url>/2019/01/07/hello-world/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>This is a post to record some solutions in blog with hexo<br><a id="more"></a></p><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><h2 id="美化方案"><a href="#美化方案" class="headerlink" title="美化方案"></a>美化方案</h2><p>参考：<a href="https://jerry011235.github.io/2015/05/06/Hexo%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BC%98%E5%8C%96%EF%BC%88%E4%BA%8C%EF%BC%89/" target="_blank" rel="noopener">https://jerry011235.github.io/2015/05/06/Hexo%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BC%98%E5%8C%96%EF%BC%88%E4%BA%8C%EF%BC%89/</a></p><h3 id="用css控制Markdown表格列宽"><a href="#用css控制Markdown表格列宽" class="headerlink" title="用css控制Markdown表格列宽"></a>用css控制Markdown表格列宽</h3><p>参考: <a href="http://blog.echoxu.cn/2018-05-15-Hexo中用CSS控制Markdown各列表格宽度.html" target="_blank" rel="noopener">http://blog.echoxu.cn/2018-05-15-Hexo中用CSS控制Markdown各列表格宽度.html</a></p><h2 id="编辑"><a href="#编辑" class="headerlink" title="编辑"></a>编辑</h2><h3 id="post样式"><a href="#post样式" class="headerlink" title="post样式"></a>post样式</h3><p>参考: <a href="http://dinghongkai.com/2017/12/19/Blog-development-6-Customized-Style-of-Writing/" target="_blank" rel="noopener">添加文章书写样</a></p><p>置顶文章：</p><ol><li>插件<a href="https://blog.csdn.net/COCO56/article/details/103840966" target="_blank" rel="noopener">hexo-generator-topindex</a>;</li><li>置顶标注代码：<a href="https://blog.csdn.net/qwerty200696/article/details/79010629" target="_blank" rel="noopener">https://blog.csdn.net/qwerty200696/article/details/79010629</a></li></ol><h3 id="添加站内文章链接"><a href="#添加站内文章链接" class="headerlink" title="添加站内文章链接"></a>添加站内文章链接</h3><ul><li><code>\{\% post_link post_name_in_source_posts link_show_title \%}</code></li><li><code>[title](/year/month/day/name.md#section)</code></li></ul><h3 id="字体颜色"><a href="#字体颜色" class="headerlink" title="字体颜色"></a>字体颜色</h3><ul><li>可以直接在Markdown 文档编辑中使用html语法<ul><li><code>&lt;font size=4 &gt; 这里输入文字，自定义大小 &lt;/font&gt;</code></li><li><code>&lt;font color=&quot;#FF0000&quot;&gt; 这里输入文字，自定义颜色的字体 &lt;/font&gt;</code></li></ul></li></ul><h3 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h3><ul><li>图片大小控制：<ul><li><code>&lt;div style=&quot;width: 200px; margin: auto&quot;&gt;![image-caption](image-link)&lt;/div&gt;</code></li></ul></li></ul><h3 id="在Hexo中渲染MathJax数学公式"><a href="#在Hexo中渲染MathJax数学公式" class="headerlink" title="在Hexo中渲染MathJax数学公式"></a>在Hexo中渲染MathJax数学公式</h3><ul><li><a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">https://www.jianshu.com/p/7ab21c7f0674</a></li><li><a href="https://www.cnblogs.com/zhyantao/p/10424874.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhyantao/p/10424874.html</a></li></ul><h2 id="bugs"><a href="#bugs" class="headerlink" title="bugs"></a>bugs</h2><ul><li><code>fatal: multiple stage entries for merged file &#39;lib/pace&#39;</code><ul><li><code>cd .deploy_git</code></li><li><code>rm .git/index</code></li><li><code>git add -A</code></li><li><code>git commit -m &quot;&quot;</code></li></ul></li></ul><h2 id="迁移"><a href="#迁移" class="headerlink" title="迁移"></a>迁移</h2><ul><li>重装hexo、配置依赖</li><li>Git clone next主题</li><li>复制 站点配置 文件和 主题文件夹</li><li>复制 source 文件夹</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

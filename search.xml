<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[EMNLP2019 | KagNet - Knowledge-Aware Graph Networks for Commonsense Reasoning]]></title>
    <url>%2F2019%2F09%2F09%2Fpaper-emnlp2019-kagnet%2F</url>
    <content type="text"><![CDATA[Title: Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading ComprehensionAuthor: Bill Yuchen Lin, Xinyue Chen, Jamin Chen, Xiang RenOrg.: University of Southern California, Shanghai Jiao Tong UniversityPublished: EMNLp,2019Code: https://github.com/INK-USC/KagNet MotivationThis workModelExperimentsAnalysis &amp; Summary]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>mrc</tag>
        <tag>note</tag>
        <tag>commonsense</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ACL2019 | Modeling Semantic Compositionality with Sememe Knowledge]]></title>
    <url>%2F2019%2F08%2F27%2Fpaper-acl2019-sc-with-sememe%2F</url>
    <content type="text"><![CDATA[Title: Modeling Semantic Compositionality with Sememe KnowledgeAuthors: Fanchao Qi, Junjie Huang, Chenghao Yang, Zhiyuan Liu, Xiao Chen, Qun Liu, Maosong SunOrg: Tsinghua University, Beihang University, Huawei Noah’s Ark LabPublished: ACL 2018 official code: https://github.com/thunlp/Sememe-SC Task Definition Semantic Compositionality (SC, 语义组合): is deﬁned as the linguistic phenomenon that the meaning of a syntactically complex unit is a function of meanings of the complex unit’s constituents and their combination rule 1 Multiword Expressions (MWE, 多字/词表达): 大多数关于 SC 的工作都是集中在 基于向量的分布式语义模型来学习 多字/词构成的 短语、成分的表示 基本框架（公式1），以两个词为例： \mathbf{p} = f(\mathbf{w}_1, \mathbf{w}_2, R, K) 其中: $f$ 是组合函数 $\mathbf{p}$ 是MWE的embedding $\mathbf{w}_1$ 和 $\mathbf{w}_2$ 表示 MWE 成分的embedding，也就是 组成 MWE 的词的表示 $R$ 是 组合规则（Combination Rule） $K$ 指在构建MWE语义时需要的额外的知识 Sememe (义原)： the minimum Semantic units of human language all the words can be composed of a limited set of sememes, which is similar to the idea of semantic primes (语义启动)2 HowNet：义原知识库 Motivation 之前的工作：利用复杂的组合函数，而很少考虑外部知识 This Work 1、设计了一个简单的 Semantic Compositionality Degree (SCD，语义组合程度) 测量实验： 通过实验发现 MWE 的 SCD 可以通过简单的基于义原的公式计算，并且和人类的判断高度相关 义原可以很好的刻画 MWE 的含义和 MWE 的组成成分，并且可以捕捉这两者之间的语义关联 证实：义原适用于建模 SC，并且可以提高 SC 相关任务的效果，如 MWE 表示学习 2、提出了两个结合义原的SC模型同于学习 MWE embedding，同时将公式1中的组合规则也结合到了两个模型中: 模型1：Semantic Compositionality with Aggregated Sememe model (SCAS) 模型2：Semantic Compositionality with Mutual Sememe Attention model (SCMSA) Note: 在这篇文章中，主要关注两个词组成的中文 MWE 的语义组合建模 Measuring SC Degree with Sememes这一部分的主要工作是通过一个SCD验证实验来证明义原适用于建模语义组合 Sememe-based SCD Computation Formulae基于义原的SCD计算规则 基本原则： 不同的MWE具有不同的 SC degrees 一个词的所有义原可以准确的描述一个词的意思 启发式的设计了SCD的计算规则集合，见下图： 数字越大表示SCD值越高 $S_p$, $S_{w_1}$, $S_{w_2}$ 分别表示 MWE 的义原集合 和 MWE 两个组成成分(词)的义原集合 SCD = 3: MWE 的义原集合为两个组成成分义原集合的的并集（union）相同 也就是说，MWE 的含义正好是两个成分含义的组合 the MWE is fully semantically compositional SCD = 2: MWE 的义原集合为两个组成成分义原集合的的并集（union）的一个适合的子集 组成成分的语义有覆盖 MWE 语义的部分，但是不能准确的推出 MWE 的语义 SCD = 1: MWE 与 组成成分共享一部分义原，但是他们分别具有各自私有的义原 SCD = 0: MWE 的含义与其组成成分的含义完全不同 无法从组成成分的含义中推出 MWE 的语义 the MWE is completely non-compositional Evaluation构建了一个人工标注的SCD数据集 评测人工标注的与规则预测的SCD之间的相关系数 (Pearson 和 Spearman) Sememe-incorporated SC ModelsIncorporating Sememes Only仅结合义原计算 MWE 的情况对应于公式1的一个简化： $\mathbf{p} = f(w_1, w_2, K)$ 使用 $S$ 表示所有的义原集合，词w的义原集合为 $S_w = \{s_1, …, s_{|S_w|}\} \subset S$ $w$ 和 $\mathbf{s}$ 的 embedding 维度都是 $\mathbb{R}^d$ Semantic Compositionality with Aggregated Sememe model (SCAS)SCAS 的结构如下图所示： SCAS 模型仅仅串联了MWE组成成分和他们的义原的embedding $\mathbf{p} = \text{tanh} (W_c [w_1 + w_2; w_1^\prime + w_2^\prime] + b_c)$ $w_1^\prime = \sum_{s_i \in S_{w_1}} \mathbf{s}_i$ $w_2^\prime = \sum_{s_j \in S_{w_2}} \mathbf{s}_j$ Semantic Compositionality with Mutual Sememe Attention model (SCMSA)SCMSA 的结构如下图所示： SCMSA 模型 计算了 一个组成成分的义原的 Mutual Attention 以及 与其他成分的义原集合的 Mutual Attention 动机：组成成分之间的义原互不相同，在进行义原组合的时候，MWE 的义原应对组成成分的义原应该分配不同的权重 $\mathbf{p}$ 的计算与SCAS相同，不同的是 $w^\prime$的计算： $\mathbf{e}_1 = \text{tanh} (W_a w_1 +b_a)$ $a_{2,i} = \frac{\text{exp}(\mathbf{s}_i \cdot \mathbf{e}_1)}{\sum_{s_j \in S_{w_2}} \text{exp}(\mathbf{s}_j \cdot \mathbf{e}_1) }$ $w_2^\prime = \sum_{s_i \in S_{w_2}} a_{2,i} \mathbf{s}_i$ Integrating Combination Rules根据原始的公式1： 使用不同的MWE组合矩阵来表示不同的组合规则，即： $W_c = W_c^r, r\in R_s$ $W_c^r \in \mathbb{R}^{d \times 2d}$ $R_s$ refers to combination rule set containing syntax rules of MWE e.g., adjective-noun, noun-noun 考虑到组合矩阵的稀疏性，以及组合矩阵之间会存在通用的组合信息，将上述矩阵进行拆分： $W_c = U^rV^r + W_c^c$ $U^r \in \mathbb{R}^{d \times h_r}$ $V^r \in \mathbb{R}^{d \times h_r}$ $h_r \in \mathbb{N}_+$ 为超参数，根据组合规则变化 $W_c^c \in \mathbb{R}^{d\times 2d}$ Training Objective training for MWE similarity computation squared Euclidean distance training for MWE sememe prediction weighted cross-entropy loss ExperimentsAnalysis &amp; Summary1.Francis Jeffry Pelletier. 1994. The Principle of Semantic Compositionality. Topoi, 13(1):11–24. ↩2.Anna Wierzbicka. 1996. Semantics: Primes and Universals: Primes and Universals. Oxford University Press, UK. ↩]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>chinese</tag>
        <tag>sememe</tag>
        <tag>hownet</tag>
        <tag>semantic-compositionality</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 | Improving Question Answering with External Knowledge]]></title>
    <url>%2F2019%2F08%2F26%2Fpaper-2019-edl-md%2F</url>
    <content type="text"><![CDATA[Title: Improving Question Answering with External KnowledgeAuthors: Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, Dong YuOrg: Rensselaer Polytechnic Institute/ CMU/ Tencent AI LabPublished: unpublished 之前在组内汇报过的一篇文章，最近翻出来了，直接把之前做的slide分享一下 Introduction Model Experiment Analysis]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>mrc</tag>
        <tag>qa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 | SenseBERT - Driving Some Sense into BERT]]></title>
    <url>%2F2019%2F08%2F18%2Fpaper-2019-sense-bert%2F</url>
    <content type="text"><![CDATA[Title: SenseBERT: Driving Some Sense into BERTAuthors: Yoav Levine, Barak Lenz, Or Dagan, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, Yoav ShohamOrg: AI21 Labs, Tel Aviv, IsraelPublished: unpublished Motivation 现有的工作通过应用自监督学习，使神经网络语言模型在NLU上取得了很大的进步 但是，目前自监督技术主要在 word-form level 上进行操作（即在word-form level上提取自监督信号指导模型进行学习） 这种 word-form 级别的监督只是本质的语义内容信号的一种替代。 从词汇语义 (lexical semantic) 的角度来看，word-form 可以看做是词汇的 surface-level 的表现形式 一词多义是自然语言处理中的一个常见现象，一个词具有多种不同的含义（文中举的例子是 ‘bass’，既可以指一种鲈鱼，低音吉他，还可以指低音歌唱家） 一个词，其本身的形式，仅仅是在给定上下文/特定语境中的实际意义的一个替代。 一词多义现象所带来的一个重要挑战就是自然语言理解中的歧义问题 BERT中的MLM只是对word-form进行的mask，无法捕捉word-sense信息，即缺乏对 lexical semantic 的建模 This Work这篇文章对于BERT的改进，正如其题目所说，drive some sense into BERT： 除了基础的预测 masked word 任务，还引入了一类 explicit word-sense 作为 BERT的 semantic-level 的自监督信号 explicit word-sense 信息指的是 每个词在 WordNet 中所对应的 supersense （共有45个supersense分类，具体参见论文Appendix） 相应的增加了一个预测 masked word-sense 任务，即预测被mask的词所对应的supersense 外部语言学知识的引入，还可以提高模型对于词汇语义（lexical semantics）的归纳偏置。 Experiments本文通过两种实验来证明所提出的SenseBERT的有效性: Lexical Semantics 实验和通用的GLUE评测 Lexical Semantics实验数据集为：SemEval WSD 和 WiC(Word in Context) Analysis &amp; Summary 一词多义是自然语言处理中普遍存在的现象之一 而传统的词向量训练方法得到的 Word Embedding 都是静态的向量表示，无法准确的表示一个 word-form 的多种不同 词义 这两年以来，GPT、ELMO、BERT等一系列预训练语言模型，通过在大规模的无监督语料上进行预训练，从而使PLM产生 dynamic contextual word/token representation ，可以认为间接的缓解了 一词多义 问题。 对于下游任务来说，使用PLM产生了更符合当前语境或上下文的词表示，使词表示更加准确 但还是从 word surface level 进行词义的学习，缺乏直接针对 lexical semantic 的监督信号 相比于过去一些工作，使用 WordNet 中的 lexical semantic 信息作为词级别的特征输入，SenseBERT 使用 lexical semantic 信息作为监督信号参与到PLM中的训练中，还可以使模型具有区分 lexical semantic 信息的能力，增加了PLM对 word-sense 的建模能力。]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>note</tag>
        <tag>wordnet</tag>
        <tag>pre-trained-lm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ACL2019 | Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension]]></title>
    <url>%2F2019%2F07%2F29%2Fpaper-acl2019-kt-net%2F</url>
    <content type="text"><![CDATA[Title: Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading ComprehensionAuthor: An Yang et al.Org.: MOE(PKU), BaiduPublished: ACL,2019 Motivation 预训练语言模型在机器阅读理解任务上取得了突破性进展。通过在海量无标注文本数据上对足够deep的网络结构进行预训练而得到的LM，能够捕捉复杂的语言现象，更好地理解语言。 然而，真正意义上的阅读理解不仅要求机器具备语言理解的能力，还要求机器具备知识以支撑复杂的推理。 例如： 这个例子需要用到下面的知识 world knowledge：Trump is the person who leads US word knowledge：sanctions has a common hypernym with ban BERT在不知道额外知识的情况下无法正确地判断出结果 This Work 为了同时利用强大的PLM捕获的语言规律和外部高质量的知识/事实，提出了语言表示与知识表示的深度融合模型 KT-NET（Knowledge and Text Fusion Net） 外部知识使用的是 包含词汇知识的WordNet 以及 包含实体信息的NELL 没有采取符号化的知识表示，而是使用了KB embedding，文中给出了两点原因： 1、KB embedding 带有整个KB的全局信息 2、易于同时融合多个KBs，而不需要过多的task-specific设计 在ReCoRD（https://sheng-z.github.io/ReCoRD-explorer/）数据集上取得了SOTA Model模型的整体结构如图所示，主要包含4个模块： BERT encoding layer knowledge integration layer: select desired KB embeddings and integrate KB representation with BERT representation self-matching layer: fuse BERT and KB representations output layer: predict the final answer 使用 BERT 进行编码时，将question和passage连在一起输给BERT，question 作为 sentence1，passage 作为 sentence2。使用 BERT 最后一层的输出 $h_i^L \in \mathbb{R}^{d_1}$ 作为 passage 和 question 中每个token的上下文编码。 下面主要介绍一下KT-NET中最重要的两个模块和KB编码与抽取过程： Knowledge Integration Layer对于每个token $s_i$，我们可以得到上下文编码 $h_i^L$，以及检索到的相关KB concepts集合 $C(s_i)$ $C(s_i)$中的每个 $c_j \in \mathbb{R}^{d_2}$ 是预训练好的KB embedding 使用Attention机制自适应地选择最相关的KB Concepts，同时引入了一个sentinel向量$\bar{c} \in \mathbb{R}^{d_2}$，来控制检索出来的KB Concepts中没有相关KB的情况： concept 相关度：$\alpha_{ij} \propto \text{exp}(c_j^\top W h_i^L)$ $W\in \mathbb{R}^{d_2 \times d_1}$ sentinel 相关度：$\beta_i \propto \text{exp}(\bar{c}^\top W h_i^L)$ 经过Attention之后得到该token的 Knowledge State 表示向量： $\mathbf{k}_i = \sum_j \alpha_{ij}c_j + \beta_i \bar{c}$ 由于sentinel向量的加入，需要对attention score进行约束：$\sum_j \alpha_{ij} + \beta_{i} = 1$ 最后，将得到是knowledge state向量与上下文编码表示进行串联，得到本模块的输出： knowledge-enriched 表示：$u_i = [h_i^L,k_i] \in \mathbb{R}^{d_1 + d_2}$ Self-Matching Layer获取了knowledge-enriched表示之后，通过self-attention机制使上下文表示和知识表示进行交互，本文中设计了直接交互和间接交互两种self-attention计算方式 直接（direct）交互：采用的是BIDAF中的trilinear公式计算attention，计算token $s_j$ 和 token $s_i$ : $r_{ij} = w^\top [u_i, u_j, u_i \odot u_j]$ $w \in \mathbb{R}^{3d_1 + 3d_2}$ row-wise softmax: $a_{ij}=\frac{\text{exp}(r_{ij})}{\sum_j \text{exp} (r_{ij})}$ attended vector: $\mathbf{v}_i = \sum_j a_{ij} u_j$ $\mathbf{v}_i$ 表示 每个token j与token i的直接交互程度 间接（indirect）交互： 间接交互指的是：token i和token j可以通过一个中间token k产生间接的关联，计算间接交互的方式很简单 attention matrix $\bar{\mathbf{A}} = \mathbf{A}^2$ $\bar{\mathbf{v}}_i = \sum_j \bar{a}_{ij} u_j$ 最后，将两种交互得到的attended向量表示进行串接，作为本模块的输出： $\mathbf{o}_i = [u_i, v_i, u_i - v_i, u_i \cdot v_i, \bar{v}_i, u_i - v_i] \in \mathbb{R}^{6d_1 + 6d_2}$ Knowledge Embedding and RetrievalKB Embedding： 采用的是 Yang et al. (2015)1 提出的 BILINEAR 算法 KB Concepts Retrieval: WordNet：给定一个词，返回其在WordNet中的额同义词集（synsets）作为候选的KB Concepts NELL：首先对Passage和Question进行NER，通过字符串匹配在NELL找到实体对应的mention，抽取对应的KB作为候选的KB Concepts 每个KB Concept 其实是在KB中对应的尾实体？ 在这一部分最后，文中还列举了自身的几点Advantages： 之前的一些结合knowledge的MRC工作，都可以算作是一种 retrieve-then-encode 范式，这部分工作只对抽取出来的相关知识进行编码（参考另一篇blognote）与整合，获得的是局部的、与文本相关的知识信息 本文的工作首先是在整个KB上预训练出kb embedding，可以捕获全局的信息 基于使用的KB Embedding方法来说，是全局的表示 易于扩展至融合多个KB的信息 对于每个token可以从不同的KB中抽取出不同的KB Concepts集合 $C^j(s_i)$ 通过不同的KB Concepts集合 $C^j(s_i)$ 计算出不同的 Knowledge State 向量 $k_i^j$ 直接将多个 $k^j_i$ 与 $h_i^L$ 串联获得融合多个KB的knowledge-enriched表示：$u_i = [h_i^L, k_i^1, k_i^2,…]$ Experiments KT-NET中的encoder使用的是BERT-Large-Cased版本，输入最大长度限制为 384，学习率 3e-05，batch大小为24 KB embedding是预训练好的，在训练过程中固定，不进行微调 实验数据集为：ReCoRD 和 SQuAD case study 从例子的热力图中，可以明显看出与BERT相比，KT-Net可以使不同的question词对passage中的词赋予不同的相关性，并且具有KB信息的支持，而BERT中，不同的question词对同一个passage中的词的相关性是相近的，也就是说，question中的每个词对于passage中的词的相关性相等， Analysis &amp; Summary KT-NET 仅在 ReCoRD 数据集上取得了SOTA的成绩，印象中在leaderboard上放了将近半年也没有其他模型可以超越它的成绩，足以证明这个模型的性能 如何选择预训练KB embedding的方法？ 从在SQuAD数据集上的实验可以看出，相比$KT-NET_{BOTH}$和$KT-NET_{NELL}$，$KT-NET_{wordnet}$取得了最好的结果，可以间接的说明，SQuAD数据集仅需要文本的信息就可以很好的回答问题，并不依赖外部的世界知识（实体信息） 1.Embedding entities and relations for learning and inference in knowledge bases. ICLR,2015. ↩]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>mrc</tag>
        <tag>note</tag>
        <tag>pre-trained-lm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ACL2019 | Explicit Utilization of General Knowledge in Machine Reading Comprehension]]></title>
    <url>%2F2019%2F07%2F17%2Fpaper-acl2019-kar%2F</url>
    <content type="text"><![CDATA[Title: Explicit Utilization of General Knowledge in Machine Reading ComprehensionAuthor: Chao Wang, Hui JiangOrg.: York UniversityPublished: ACL,2019Old Version: Exploring Machine Reading Comprehension with Explicit Knowledge. arXiv:1809.03449 Motivation MRC模型和人类之间的差距有两方面 1、MRC模型需要大量的训练样例来学校 2、MRC模型对于有意加入噪声数据不鲁棒 造成差距的原因在于目前MRC模型仅利用了给定passage-question对中的信息，而没有像人类一样利用一些general knowledge 如何利用抽取的知识 目前的做法都是隐式地将抽取到的知识的编码用于增强相应词的lexical/contextual表示 缺点：缺乏解释性和控制性 This Work 本文中提出，词之间的语义联系（inter-word semantic connections）可以作为一种general knowledge 例如： 此外，本文 提出了一种 data enrichment 方法，利用 WordNet 作为知识源，为passage-question对抽取 inter-word semantic connections 提出了一个 Knowledge Aided Reader（KAR） 模型，用于显示地将抽取到的 general knowledge 引入到模型中，并辅助 attention 机制 Data Enrichment Method本文介绍的基于 WordNet 的数据富集的方法是一个可控的抽取过程 Semantic Relation ChainWordNet 中的几个要素： synset：同义集合（a set of words expressing the same sense） 一个词可以有多个不同的senses，可以属于多个synset semantic relations：synset 之间的语义关系 在NLTK中，共有16种 hypernyms, hyponyms, holonyms, meronyms, attributes, etc. 本文根据 synset 和 semantic relation 定义了一个新的概念：Semantic Relation Chain 连接一个 synset 与另一个 synset 的一系列 semantic relation（a concatenated sequence of semantic relations） semantic relation chain 中的每个 semantic relation 定义为一跳（a hop） Inter-word Semantic ConnectionData Enrichment 方法的核心问题就是确定两个词之间是否存在 语义的联系（semantically connections）为了解决这个问题，定义了一个新的概念：扩展同义词集（the extended synsets of a word） 定义：通过 semantic relation chain 可以到达的 synset 定义符号：$S_w$ 表示 synset，$S_w^*$ 表示扩展同义词集 理论上来看，如果不加以限制的话，WordNet 中所有的 synset 都将属于 $S_w^*$ 故此，引入了一个超参数 $k \in \mathbb{N}$，表示 semantic relation chain 的最大跳数 即，只有小于 $k$ 的 chains 才用于构建 $S_w^*$： $ S_w^{*}(k) $ $\text{ if } k=0, \text{ we will have }S_w^{*}(0)=S_w$ 构建 inter-word semantic connections，定义： 当且仅当 $S_{w_1}^* (k) \cap S_{w_2} \neq \emptyset$ 时，$w_1$ 和 $w_2$ 具有语义关联 General Knowledge Extraction遵循上述的定义，为给定的 passage-question 对抽取任意词与 passage 中的词的 inter-word semantic relation： 只抽取 positional information 对词 $w$，抽取一个集合 $E_w$，包含所有 passage 中与 $w$ 有语义关联的词的位置，如果 $w$ 本身是 passage 中的词，去除其本身在 passage 中的位置 通过上一节定义的超参数 $k$ 就可以控制抽取的 扩展同义词集 的大小，即抽取出来的 general Knowledge 的数量 超参数 $k$ 通过在验证集上的效果确定 Knowledge Aided ReaderKAR 模型的结构如下图所示： 与现有的一些模型相比，主要的改进集中在右侧部分的输入和引入 general Knowledge 之后， attention 计算的改进 Notation: $P = \{p_1, … , p_n\}$ $Q = \{q_1, … , q_m\}$ Knowledge Aided Mutual Attention 利用提起抽取好的 general Knowledge 为每个词 $w$ 构建 增强的上下文表示 $c_w^*$ 通过 $E_w$ 和 原始上下文向量 $C_p$ 的对应关系 得到 matching context embeddings $Z \in \mathbb{R}^{d\times |E_w|}$ 计算 matching vector $c_w^+$ $t_i = v_c^{\top} tanh(W_c x_i + U_c c_w) \in \mathbb{R}$ $c_w^+ = Z \text{ softmax} ({t_1,…,t_{|E_w|}}) \in \mathbb{R}^{d}$ attention 的计算方式 同 BIDAF Knowledge Aided Self Attention 方法同上 Experiments 超参数 $k$ 的选择 Summary &amp; Analysis 抽取出来的 general knowledge 相当于为 passage-question 对中的词构建了一个隐式的关联矩阵，通过这个关联矩阵，在两次 attention 中抽取出对应的 contextual representation 和 coarse memory （result of mutal attention）来辅助增强 attention 的输入]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>mrc</tag>
        <tag>note</tag>
        <tag>wordnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ACL2019 | Explain Yourself! Leveraging Language Models for Commonsense Reasoning]]></title>
    <url>%2F2019%2F07%2F10%2Fpaper-acl2019-cos-e%2F</url>
    <content type="text"><![CDATA[Title: Explain Yourself! Leveraging Language Models for Commonsense ReasoningAuthors: Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, Richard SocherOrg.: Salesforce ResearchPublished: ACL 2019 official code: https://github.com/salesforce/cos-e Motivation Commonsense reasoning that draws upon world knowledge derived from spatial and temporal relations, laws of physics, causes and effects, and social conventions is a feature of human intelligence.However, it is difficult to instill such commonsense reasoning abilities into artificial intelligence implemented by deep neural networks. While neural networks effectively learn from a large number of examples, commonsense reasoning for humans precisely hits upon the kind of reasoning that is in less need of exemplification.Rather, humans pick up the kind of knowledge required to do commonsense reasoning simply by living in the world and doing everyday things.AI models have limited access to the kind of world knowledge that is necessary for commonsense reasoning. from official blog: https://blog.einstein.ai/leveraging-language-models-for-commonsense/ DL 模型在需要常识推理的任务上表现不佳，原因可能是往往需要某种形式的世界知识，或是推理的信息未在输入中直接显示出来 虽然已经有了一些用于检验模型Commonsense推理能力的数据集，但是 it is still unclear how these models perform reasoning and to what extent that reasoning is based on world knowledge. This Work 构建了一个Common Sense Explanations（CoS-E）数据集：收集了人类对于常识推理的解释（模拟人类的常识推理过程）以及highlight annotation（针对问句的） 提出了一个常识自动生成解释（Commonsense Auto-Generate Explanation，CAGE）框架：使用CoS-E数据集来训练语言模型，是LM可以自动的产生解释（这个解释可以在训练和推理过程中使用），证明可以有效地利用语言模型来进行Commonsense Reasoning 在任务数据集（CommonsenseQA）上取得了10%的提升（Version 1.0） 还进行了跨领域迁移的实验 Background and Related Work Commonsense reasoning Natural language explanations Knowledge Transfer in NLP CoS-E CorpusCommon Sense Explanations 数据集是在基于CommonsenseQA任务数据集上构建的，包含两种形式的人类解释： 1、open-ended 自然语言解释 （CoS-E-open-ended） 2、对于问句，标注了 highlighted span annotations，标注的是对于预测正确答案起到重要作用的词 （CoS-E-selected） 示例： Algorithm and Model（在这篇文章中，进行实验时使用的是CommonsenseQA Version 1.0 的数据集，所以只有三个候选答案，在 Version 1.1 中，每个问题对应5个候选答案） 框架图： Commonsense Auto-Generated Explanations (CAGE)在CAGE框架中，使用GPT进行微调。微调部分的框架如上图中的图（a）所示。 本文提出了两种生成解释（即微调GPT-LM）的方法： 1、explain-and-then-predict（reasoning） 2、predict-and-then-explain（rationalization） 方法一：Reasoning 输入：$C_{RE} = q,c_0, c_1, c_2 \text{ ?}\text{ commonsense says} $ 目标：条件式生成 explanations $e$ $\sum_i \text{log} P(e_i|e_{i-k},…,e_{i-1}, C_{RE}; \Theta)$ 其中，$k$ 是语言模型生成句子的窗口大小，在本文中，设为大于explanation的长度 称为reasoning的原因： 在推理阶段也可以自动的生成explanation，为常识问答提供额外的上下文 Note：在生成explanation的过程中不知道正确答案的标签 方法二：Rationalization 输入：$C_{RA} = q,c_0, c_1, c_2 \text{ ? } a \text{ because} $ 目标：同上 在这个算法中，在训练时，生成explanation的过程中是已知正确答案的标签的 相当于为正确的 q-a 对 产生一个合理的解释 而不是真正的常识推理 CAGE的参数设置： 最大序列长度：20 batch size：36 train epochs：10 learning rate：$1e-6$ warmup linearly with proportion $0.002$ weight decay：0.01 模型选择指标：在验证集上的BLEU和perplexity Commonsense Predictions with Explanations这部分关注的是，给定一个人类生成的解释或是LM推理生成的解释，使模型在CQA任务上进行预测。见上图图（b） 任务模型采用BERT with multiple choice BERT输入：question [SEP] explanation [SEP] candidate answer choice 微调BERT的参数： batch size：24 test batch size：12 train epochs：10 maximum sequence length：175（baseline=50） Experiments这篇文章的实验部分很充分、很完整，包含以下几个方面： 使用CoS-E进行CQA任务，在dev上的效果：Table-2 仅在训练时应用 CoS-E-open-ended BERT baseline：63.8% 在BERT中加入CoS-E-open-ended，提升了2个点（65.5%） CAGE-reasoning：72.6% CQA test set 上的效果见 Table-3 Table-4：在训练和验证时同时应用CoS-E Table-5：在CQA v1.1 测试集上的结果 迁移到域外数据集 Summary &amp; Analysis论文中有意思的几点分析 explanations可以帮助阐明更长更复合的问题 CAGE-reasoning 生成的explanations中，有43%的可能性包含答案选项]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>note</tag>
        <tag>commonsense</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EMNLP2018 | Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text]]></title>
    <url>%2F2019%2F07%2F08%2Fpaper-emnlp2018-graft-net%2F</url>
    <content type="text"><![CDATA[Title: Open Domain Question Answering Using Early Fusion of Knowledge Bases and TextAuthors: Haitian Sun, Bhuwan Dhingra et al.Org: CMUPublished: EMNLP 2018 official code: https://github.com/OceanskySun/GraftNet Motivation 大多数 open domain QA 任务都是使用单信息源（要么是text from encyclopedia，或者是 single KB）来回答问题 判断一个信息源的适用性（suitability），取决于信息源的覆盖度（coverage）和从其中抽取答案的难度（difficulty of extracting answers from it） large text corpus作为信息源：具有很高的覆盖度，但是信息被不同的text pattern表示（这里可以理解为，不同领域/体裁的文本有不同的表现形式），模型还需要学习这些text pattern，导致模型难以泛化到其他领域以及新的推理类型 KB作为信息源：覆盖度低（由于不可避免的不完全性和有限的模式），但是更易于抽取答案 由于有两种信息源存在，有些问题可被text回答，有些问题更适合用KB回答，但是只使用一种信息源不足以回答问题，一个很自然的问题就是如何有效地结合多种类型的信息：有以下两种方式 late fusion：为每种信息源设计SOTA的QA模型，得到他们的预测结果之后，再用一些启发式的方法将得到的答案进行聚合 问题：sub-optimal的解决方法，模型受限于从不同的信息源中聚集证据信息 early fusion：本文所采取的方式 只利用一个模型，训练其从一个问题子图中抽取答案 问题子图，既包含相关的 KB fact 又包含 text 可以灵活地结合多个信息源的知识 This Work这篇文章进行的是开放域的KBQA任务（incomplete KB），结合图表示学习，提出了一个GRAFT-Net模型，可以从同时包含文本、KB实体与关系的 Question-specific 子图中抽取答案 为了实现early fusion，提出了一个 Graphs of Relations Among Facts and Text Network（GRAFT-Net） 基于图卷积神经网络模型，可以在由KB facts和text sentences组成的异构图上进行运算 提出了 heterogeneous update rules 来处理KB节点；LSTM-based update rules来更新text节点 提出了 directed propagation method， 启发自 Personalized PageRank 算法，用于限制图中 embedding 在基于从seed节点链接到question的路径上进行传播 实验数据集：WikiMovies，WebQuestionSP Task SetupTask DescriptionQuestion Subgraph RetrievalGRAFT-Net1.Node Initialization2.Heterogeneous UpdatesEntitiesDocuments 3.Conditioning on the QuestionAttention over RelationsDirected Propagation 4.Answer Selection5.Regularization via Fact DropoutExperimentsSummary &amp; Analysis]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>note</tag>
        <tag>reasoning</tag>
        <tag>graph</tag>
        <tag>kbqa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 | Incorporating Sememes into Chinese Definition Modeling]]></title>
    <url>%2F2019%2F07%2F08%2Fpaper-2019-cdm-with-sememe%2F</url>
    <content type="text"><![CDATA[Incorporating Sememes into Chinese Definition Modeling2019Linear Yang et al. MotivationChinese Definition Modeling(CDM) 任务：为给定的中文词产生词典式的中文定义 This work 为了解决CDM任务，构建了一个CDM数据集，每个example由 &lt;word, sememes, definition&gt; 三元组组成 两个新模型 1、Adaptive-Attention Model（AAM）：利用adaptive注意力机制结合sememes（义原）信息生成 Definition 2、Self- and Adaptive-Attention Model（SAAM ）：进一步使用self-attention替代AAM中的recurrent connection，减少word，sememes，definition之间的路径长度 MethodologyChinese Definition Modeling Corpus： 包含 104,517 个条目 三元组：&lt;a word, the sememes of a speciﬁc word sense, and the deﬁnition in Chinese of the same word sense&gt; Sememes：义原，是描述词义的最小的语义单位 具体信息请参考：HowNet 为什么要使用义原：可以为生成定义提供额外的语义信息 例子： ModelRNN-based Seq-to-Seq Model Adaptive-Attention Model引入 Adaptive-Attention 的原因： vanilla attention 在每一步都会关于sememes 在生成definition的过程中，不是所有词都与sememes有关 Adaptive-Attention 利用 time-varying sememes 信息作为 sememe context LM的信息作为 LM context 首先，由 decoder 的 hidden state 和 上一时刻生成的definition词 通过线性映射和sigmoid运算得到 一个 gate 向量 再，对上一时刻的hidden state 进行tanh激活运算，通过 gated unit，得到 LM context 再根据 context，引入一个新的attention，决定在生成当前时刻词是依赖 sememe context 还是 LM context Self- and Adaptive-Attention Model将 AAM 中的 RNN，换成 transformer]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>note</tag>
        <tag>tg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AAAI2019 | Improving Question Answering by Commonsense-Based Pre-Training]]></title>
    <url>%2F2019%2F07%2F06%2Fpaper-aaai2019-cs-based-pre-train%2F</url>
    <content type="text"><![CDATA[AAAI,2019中山大学，微软亚研 Motivation 现有的神经网络模型不能很好的回答常识问题是由于缺乏concepts之间的常识联系 回答有些问题需要模型有能力在常识知识上进行推理 回答这类的问题既需要词本身的知识又需要世界知识 This Work 利用外部常识知识（ConceptNet）提高QA系统的常识推理能力 根据外部关于世界的常识知识预训练一个模型，对concepts之间的直接关系和间接关系进行预训练 预训练的functions可以轻松地加到神经网络中 concept之间的关系可以分为直接和间接 可以学习两个度量每对concept之间直接和间接关系的functions 好处： 模型具有很大的concept/entity覆盖度 模型常识推理的能力不受限于训练实例的数量和不需要覆盖所有终端任务中需要的推理类型 易于扩展 实验数据集：ARC / MCScripts Approach 候选答案的打分函数由两部分组成 $f(a_i) = \alpha f_{doc}(a_i) + \beta f_{cs}(a_i)$ 分别是 document-based model 给出的打分 和 commonsense-based model 给出的打分 document-based model 同 yuanfudao 的 tri-attention Commonsense-based Model 预训练知识表示的参数： 关系表示： 两个concept之间的关系表示：$f_{cs}(c_1,c_2)=Enc(c_1) \odot Enc(c_2)$ concept encoder $Enc(\cdot)$ 的计算： $h^w(c) = BiLSTM(Emb(c))$ 考虑到邻居节点：$h^n(c) = \sum_{c^\prime\in NBR(c)}(W^{r(c,c^\prime)} h^w(c^\prime) + b^{r(c,c^\prime)})$ $Enc(c) = [h^w(c);h^n(c)]$ 基于排序的损失函数： $l(c_1,c_2,c^\prime) = max(0, f_{cs}(c_1, c^\prime) - f_{cs}(c_1,c_2) + mgn)$ $c_1$和$c_2$是正例 $c_1$和$c^\prime$是负例 根据不同的策略对负例进行采样 直接关系：直接根据kg中的邻接图进行采样 间接关系：拥有共同邻节点的作为正例，没有one-hop或two-hop关系的作为负例 $f_{cs}(a_i)$函数是commonsense-based model 的打分函数： $f_{cs}(a_i) = \frac{1}{|E_1|} \sum_{x\in E_1} max_{y\in E_2}(f_{cs}(x,y))$ max 表示选择$E_1$中最相关的concept 其中$E_1$和$E_2$分别表示从问题句子Q和候选答案抽取出来的常识事实 同样还可以计算从文章和候选答案抽取处理的常识事实 对于P-Q对，为了保证和候选答案的相关性，去除不在候选答案抽取出来的知识集中的concept 每个$E$是从知识库中抽取出的三元组 相关工作Topic：结合外部知识库或结构知识库的相关工作这方面的工作可以分为两类，大部分属于第一类 enhance each basic computational unit (word or noun phrase) Leveraging knowledge bases in lstms for improving machine reading Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge. support external signals at the top layer before the model makes the final decision 分析 cons: 对文本和问题的建模和对知识的建模是分开的，通过最终的打分函数进行关联 pros: 通过区分直接关系和间接关系的采样来训练得到的知识表示向量效果更好]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>mrc</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cognition - Primary Research]]></title>
    <url>%2F2019%2F06%2F11%2Fmrc-research-cognitive%2F</url>
    <content type="text"><![CDATA[To Be Update]]></content>
      <categories>
        <category>MRC-Research</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>research</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recent Advances on Multi-Hop RC - WikiHop]]></title>
    <url>%2F2019%2F06%2F06%2Fpaper-wikihop-1%2F</url>
    <content type="text"><![CDATA[WikiHop in QAngaroo benchmark is a dataset for Multi-hop Reading Comprehension Across Documents.Style: Multiple-Choice Reference papers on WikiHop task: Neural models for reasoning over multiple mentions using coreference. NAACL,2018. Exploring graph-structured passage representation for multihop reading comprehension with graph neural networks. 2018. Exploiting explicit paths for multihop reading comprehension. 2018. BAG: Bi-directional Attention Entity Graph Convolutional Network for Multi-hop Reasoning Question Answering. NAACL,2019. Question Answering by Reasoning Across Documents with Graph Convolutional Networks. NAACL,2019. Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs. ACL,2019. Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question Answering. ICLR,2019. Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension. ACL,2019. Task DefinitionInput: $&lt; q, P, C_q &gt;$ query: $q$ in the form of triple without tail entity $&lt; h_e, r, ? &gt;$ a set of supporting documents: $P = \{P_1, …, P_M\}$ a set of candidate answers (all of which are entities mentioned in $P$): $C_q = \{C_1,…,C_N\}$ Goal: select $a^{\star} \in C_q$, which is the entity that correctly answers the question need to aggregate information from multiple evidences across documents Coref-GRU Neural models for reasoning over multiple mentions using coreference. NAACL,2018. Bhuwan Dhingra. (William W. Cohen) CMU. 1.Motivation existing RNN layer are biased towards short-term dependencies This work: adapt a standard RNN layer by introducing a bias towards coreferent recency 2.Model Details coreference relationships between words (Directed acyclic graph(DAG) style graph) introduce a term in the update equations for GRU which depends on the hidden state of the coreferent antecedent of the current token/word hidden states are propagated along coreference chains and the original sequence in parallel MHQA-GCN/GRN Exploring graph-structured passage representation for multihop reading comprehension with graph neural networks.2018.Linfeng Song. (Yue Zhang)University of Rochester. (Westlake University) 1.Motivation local coreference information is limited in providing information for rich inference and global evidence This work: form more complex graphs to better connecting global evidence considering two more types of edges in addition to coreference same entity mentions (cross-document) window-typed (within-document) two mentions of different entities within a context window 2.Model Details Encoding: evidence integration with graph network graph recurren network graph convolutional network Prediction: summing the probabilities over all occurrences of the same entity mention Pr_{\epsilon}=\frac{\sum_{k\in{C_q}}\alpha_k}{\sum_{k^{\prime} \in {C_q}} \alpha_{k^{\prime}}} \alpha_k = \frac{exp(e^k)}{\sum_{k^{\prime} \in C_q} exp(e^{k^{\prime}})} $e^k$ is the representation of entity mention of $\epsilon_k$ Path-based(Kundu.2018.)1.Motivation graph-based models only implicitly combine knowledge from all the passages unable to provide explicit reasoning paths for the selected answer. This work present a path-based reasoning approach for textual reading comprehension generating potential paths across multiple passages extracting implicit relations along this path composing relations to encode each path 2.Model Details Path Define: only consider two-hop path: eg: $path_{kj}=h_e \rightarrow e_1 \rightarrow c_k$ $e_1$: intermediate entity and can be extended to multi-hop an extracted path is a set of entity sequences Path Extraction: for each candidate step #1: find a passage $P_1$ contains $h_e$ of the query step #2: find intermediate entities: all Named Entities and Noun Phrases that appear in the same sentence with $h_e$ or in the subsequent sentence step #3: find another passage $P_2$ containes any of the intermediate entities found in step#3 for distinguishing the $e_1$ in different passage，use $e_1^{\prime}$ to stand for the same mention in the second passage step #4: check if passage $P_2$ contains any of the candidate answer choices Path Encoding: context-based path encoding use the concatenation of the boundary vectors of the passage encoding as the location encoding vector of entity $g_{e} = [s_{p_1,i_1};s_{p_1,i_2}]$ extract implicit relation with a feed forward layer $r_{h_e,e_1}=FFL(g_{h_e}, g_{e_1})$ as well as $r_{e_1^{\prime}, c_k}$ compose implicit relation vector with a feed forward layer $x_{ctx}=FFL(r_{h_e,e_1}, r_{e_1^{\prime},c_k})$ feed forward layer $FFL(a,b)=tanh(aW_a + bW_b + bias)$ passage-based path encoding question-weighted passage representation query-aware passage representation: $S_p^1$ and $S_p^2$ aggregate passage representation: get single passage vector self-attention $\tilde{s}_{p_1}$ and $\tilde{s}_{p_2}$ $x_{psg} = FFL(\tilde{s}_{p_1}, \tilde{s}_{p_2})$ Path Scorer: context-based path scoring $\tilde{q}=([q_0;q_L])W_q$ $y_{ctx,q}=FFL(x_{ctx},\tilde{q})$ $z_{ctx}=y_{ctx,q}W_{ctx}^T$ passage-based path scoring self attention get single candidate answer choice vector $\tilde{c}_k$ $z_{psg}=\tilde{c}_k x_{psg}^T$ unormalized score $z = z_{ctx} + z_{psg}$ softmax over all the paths and candidates get $score(path_{kj})$ Prediction $prob(c_k)=\sum_j score(path_{kj})$ BAG1.Motivation comprehend the relationships of entities across documents before answering questions 2.Model Details Graph Construction node: all mentions of candidates edge: undirected 1)cross-document edge 2)within-document edge Multi-Level Features: input node embedding concatenation of GLoVe/ELMo(+linear)/NER/POS Query Encoding: BiLSTM + linear Relational Graph Convolutional Network, same with Entity-GCN. Bi-directional Attention Between a Graph and a Query similarity matrix $S = pooling_{mean}(f_a ([h_n; f_q; h_n \circ f_q] ))$ $f_q$: query representation $h_n$: all node representation directional computation is the same with BiDAF Prediction: the probability of each node becoming answer. the probability of each candidate is the sum of all corresponding nodes. Entity-GCN1.MotivationThis work frame question answering as an inference problem on a graph representing the document collection. 2.Model Details Graph Construct node: mentions of candidate choices and head entity in the query edge: co-occurrence in the same document mentions that exactly match across document Encoding ELMo: a concatenation of three 1024-dimensional vectors resulting in 3072-dimensional input vectors Graph Encoding: Relational GCN to model message passing process at layer $l$: aggregation: aggregate information from neighbors of each node z_i^l = \frac{1}{|N_i|} \sum_{j\in N_i} \sum_{r \in R_{ij}} f_r(h_j^l) $N_i$ is the set of indices of nodes neighbouring $i$-th node $R_{ij}$ is the set of edge annotations between $i$ and $j$ combination u_i^l = f_s(h_i^l) + z_i^l updating: how much of the update message propagates to the next step g_i^l = sigmoid(f_g ( [z_i^l; h_i^l] )) h_i^{l+1} = tanh(u_i^l) \odot g_i^l + h_i^l \odot (1-g_i^l) Prediction $Prob(c|q,C_q,P) \varpropto exp(max_{i\in M_c} f_o( [q, h_i^L] ) )$ $M_c$ is the set of node indicate that $i \in M_c$ only if node $i$ is a mention of candidate choice $c$ Heterogeneous Document-Entity1.Motivation mainly compared with Entity-GCN contains different granularity levels of information including candidates, documents and entities in speciﬁc document contexts This work design the Heterogeneous graph contains three kinds of nodes seven types of edges include nodes corresponding to candidates, documents and entities. 2.Model Details Graph Construction node: 1) candidate entity nodes 2) entity nodes extracted from documents 3) document nodes edge between (doc, entity) 1) if the candidate appear in the document at least one time between (doc, entity) 2) if the entity is extracted from the document between (entity, candidate) 3) if the entity is a mention of the candidate between (entity, entity) 4) if they are extracted from the same document 5) if they are mentions of the same candidate or query subject and they are extracted from different documents 7) entity nodes that do not meet previous conditions are connected between (cnadidate, candidate) 6) all candidate nodes connect with each other Graph Encoding Relational GCN, the same with Entity-GCN Prediction $a = f_C(H^C) + ACC_{max}(f_E(H^E)$ $H^C$ : node representations of all candidate nodes $H^E$: node representations of all entity nodes that correspond to candidates $ACC_{max}$: max pooling of entites belong to the same candidate $f(\cdot)$: two-layers MLP with tanh Summary related works can be categorized as follows: graph-based coreference co-occurrence heterogeneous path-based neural network based official leaderboard: http://qangaroo.cs.ucl.ac.uk/leaderboard.html Models UnMaskDev UnMaksTest MaskDev MaksTest BiDAF 49.7 42.9 59.8 - Coref-GRU 56.0 59.3 - - MHQA-GCN 62.6 - - - MHQA-GRN 62.8 65.4 - - Entity-GCN 64.8 67.6 - - CFC 66.4 70.6 - - Kundu.2018 67.1 - - - BAG 66.5 69 70.9 68.9]]></content>
      <categories>
        <category>MRC-Paper-Intro</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>note</tag>
        <tag>multi-hop</tag>
        <tag>gnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recent Advances on Multi-Hop RC - HotpotQA]]></title>
    <url>%2F2019%2F05%2F30%2Fmrc-paper-hotpotqa%2F</url>
    <content type="text"><![CDATA[HotpotQA is a dataset for Diverse, Explainable Multi-hop Question Answering.Style: Extractive-based HotpotQA has 4 features, which can also be seen as challenges: questions require finding and reasoning over multiple supporting documents to answer questions are diverse and not constrained to any pre-existing knowledge contains two types of questions: 1.bridge 2.comparison (factoid comparision questions) provides sentence-level supporting facts required for reasoning introduces the strong supervision for reasoning and make the final predictions explainable (can form a reasoning chain through the support facts) Reference papers on HotpotQA task: Cognitive Graph for Multi-Hop Reading Comprehension at Scale. ACL, 2019. Dynamically Fused Graph Network for Multi-Hop Reasoning. ACL, 2019. Answering while Summarizing: Multi-task Learning for Multi-Hop QA with Evidence Extraction. ACL, 2019. Compositional Questions Do Not Necessitate Multi-hop Reasoning. ACL,2019. (short) Multi-hop Reading Comprehension through Question Decomposition and Rescoring. ACL,2019. Task DefinationInput: query $N_p = 10$ paragraphs Output answer span supprot facts (sentence), which can be regarded as the supervised signal during training Cognitive Graph QA Cognitive Graph for Multi-Hop Reading Comprehension at Scale.ACL, 2019.Ming Ding (Chang Zhou)THU (DAMO, Alibaba) 1.MotivationDual Process Theory from the cognitive process of humans: System1 first retrieve relevant information following attention via an implicit, unconscious, intuitive process efficiently provides resources according to requests System2 conduct explicit, conscious, controllable reasoning process based on the result of System1 enable diving deeper into relational information by performing sequential thinking in the working memory slower but human-unique rationality For complex reasoning, two systems are coordinated to perform fast and slow thinking iteratively This work: System1 extracts question-relevant entities and answer candidates from paragraphs and encodes their semantic information Extracted entities are organized as a cognitive graph —&gt; working memory System2 conducts the reasoning procedure over the graph, and collects clues to guide System 1 to better extract next-hop entities. iterate until all possible answers are found, and then the final answer is chosen based on reasoning results from System 2. 2.Model DetailsAlgorithm of Cognitive Graph QA Model Overview 3.Experimentscase study Dynamically Fused Graph Network Dynamically Fused Graph Network for Multi-Hop Reasoning.ACL,2019.Yunxuan XiaoShanghai Jiao Tong University. (ByteDance AT Lab) 1.Motivation Challenges: 1) filtering out noises from multiple paragraphs and extracting useful information previous work: build entity graph from input paragraphs and apply GNN to aggregate the information shortcomings: static global entity graph of each QA pair —&gt; implicit reasoning, lack of explainability 2) aggregate document information to an entity graph and answers are then directly selected on entities of the entity graph shortcomings: answers may not reside in entities of the extracted entity graph This work: intuition: mimic human reasoning process in multi-hop QA start from an entity of interest in the query focus on the words surrounding the start entities connect to some related entity in the neighborhood guided by the question repeat the step to form a reasoning chain, and lands on some entity or snippets likely to be answer. consturcts dynamic entity graph propagating information on dynamic entity graph under soft-mask constraint bidirectional fusion: aggregate information from document to the entity graph and entity graph to document 2.Model Details2.1 Graph Constructingnode: POL (Persion, Organization, Location) entitiesedge: 1) sentence-level: co-occurence 2) context-level: coreference 3) paragraph-level: link with central entities extracted from title sentence of each paragraph 2.2 Model Paragraph Selector use BERT sentence classification: [Q , Pi] concat all selected Pi —&gt; C Encoder use BERT encoding concatenating of query and context performs better output: query $Q_0 \in \mathbb{R}^{L\times d_2}$ context $C_0 \in \mathbb{R}^{M\times d_2}$ Fusion Block document to graph binary Matrix $M \in \mathbb{R}^{M \times N}$ : get text span associated with an entity entity embedding: mean-max pooling $E_{t-1} = [e_{t-1,1},…,e_{t-1,N}] \in \mathbb{R}^{2d_2 \times N}$ dynamic graph attention soft-mask: signify the start entities in the t-th reasoning step query vector $\tilde{q}^{t-1} = MeanPooling(Q^{t-1})$ masked entity $\tilde{E}^{t-1} = [m_1^t e_1^{t-1},…]$ \gamma_i^t = \tilde{q}^{t-1} V^t e_t^{t-1} / \sqrt{d_2} m^t = \sigma([\gamma_1^t,...,\gamma_N^t]) propagate information in graph by GAT (the more relevant to the query, the neighbor nodes receive more information from nearby) $e^t_i = ReLu( \sum_{j\in B_i} \alpha_{j,i}^t h_j^t)$ h_i^t = U_t \tilde{e}_i^{t-1} + b_t \beta_{i,j}^t = LeakyReLu(W_t^T [h_i^t, h_j^t]) \alpha_{i,j}^t = \frac{exp(\beta_{i,j}^t)}{\sum_k exp(\beta_{i,k}^t)} $E_t = [e_1^t,…,e_N^t]$ updating query $Q^t = BiAttention(Q^{t-1}, E^t)$ graph to document issue: the unrestricted answer still cannot be back traced keep information flowing from entity back to tokens in the context the same matrix $M$ update the context embedding $C^t = LSTM( [C^{t-1}, {ME^t}^T] ) \in \mathbb{R}^{M\times d_2}$ Prediction Layer 4 targets supporting sentences start position of answer span end position of answer span answer type use four LSTMs to get final representation $O_{sup} = F_0 (C^t)$ $O_{start} = F_1([C^t, O_{sup}])$ $O_{end} = F_2([C^t, O_{sup}, O_{start}])$ $O_{type} = F_3([C^t, O_{sup}, O_{end}])$ $L = L_{start} + L_{end} + \lambda_s L_{sup} + \lambda_t L_{type}$ 3.ExperimentsQuery Focused Extractor Answering while Summarizaing: Multi-task Learning for Multi-hop QA with Evidence ExtractionACL,2019.Kosuke Nishida.NTT Media Intelligence Laboratories. 1.Motivation2.Model Details3.ExperimentsDecompRC Multi-hop Reading Comprehension through Question Decomposition and RescoringACL,2019.Sewon Min.University of Washington. AI2. 1.Motivation decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models inspired by the idea of compositionality from semantic parsing 2.Model Details3 step process: decomposes the original, multi-hop question into several single-hop sub-questions according to a few reasoning types in parallel, based on span predictions. for every reasoning types D ECOMP RC leverages a single-hop reading comprehension model to answer each sub-question, and combines the answers according to the reasoning type. leverages a decomposition scorer to judge which decomposition is the most suitable, and outputs the answer from that decomposition as the ﬁnal answer. Summary]]></content>
      <categories>
        <category>MRC-Paper-Intro</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>note</tag>
        <tag>multi-hop</tag>
        <tag>gnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Commonsense Reasoning for Natural Language Understanding - A Survey of Benchmarks, Resources, and Approachs]]></title>
    <url>%2F2019%2F04%2F18%2Fmrc-cs-reasoning-for-nlu-survey%2F</url>
    <content type="text"><![CDATA[Authors: Shane Storks, Qianzi Gao, Joyce Y. ChaiOrg.: Department of Computers Science and Engineering, Michigan State UniversityYear: 2019 AbstractCommonsense Knowledge (CS Know.) 和 Commonsense Reasoning 是机器智能的两大重要瓶颈。 现有的NLP研究中，已经提出了一些需要常识推理的benchmarks和tasks，旨在评估机器获得和学习常识知识的能力。 这篇文章的主要目的是针对NLU的常识推理，提供关于以下四个方面的一个综述： 现有的任务和benchmarks Knowledge Resources Learning and Inference Approachs 关于本篇文章的思维导图： 1.IntroductionDavis and Marcus (2015)davis 指出常识推理的挑战：spans from difficulties in understanding and formulating commonsense knowledge for specific or general domains to complexities in various forms of reasoning and their integration for problem solving. 现有的研究主要关注如下图所示的几个方面：(在这篇文章中主要关注文本数据源) 2.Benchmarks and Tasks这章主要介绍一些需要常识推理的benchmarks，以及对构建这类benchmarks的重要要求进行一个总结。 benchmarks数据集的发展： 2.1 Overivew of ExistingBenchmarks很多常识benchmark数据集都是基于classic language processing问题建立起的，从focused task (共指消解、命名实体识别) 到更理解性的任务和应用。 Benchmarks不应该局限于需要 language processing 能力提升性能的类型，应该更有针对性，更关注某类的常识知识和推理 (或是某几类的混合) 。 将Benchmarks分为6类，分别展开介绍 2.1.1 Coreference Resolution共指消解是NLU中的一个基本任务，在句子中出现多个代名词或明显复杂的过程时，需要常识知识确定决策。 代表数据集：Winograd Schema Challenge (link) 2.1.2 Question Answering相比于只关注某些特定的语言处理或是推理的任务，QA提供了一种在单个任务中更全面地混合语言处理和推理技巧的benchmark。 contain questions requiring commonsense knowledge alongside question requiring comprehension of a given text. 代表数据集：ARC、MCScript、ProPara、MultiRC、SQuADv2、CoQA、OpenBookQA、CommonsenseQA ProPara：面向过程性文本，旨在学习目标的追踪和状态变化 2.1.3 Textual Entailment文本推理任务旨在推理两个句子之间的关系，需要多种语言处理能力（paraphrase）以及object tracking、causal reasoning和常识知识。 代表数据集：RTE challenges、SICK、SNLI、SciTail RTE knowledge resources: https://aclweb.org/aclwiki/RTE_Knowledge_Resources 2.1.4 Plausible Inference似然推理：require hypothetical, intermediate or uncertain conclusions defined as plausible inference. 这类数据集关注的是everyday events和interactions，包含各种的实际的常识关系。 代表数据集：COPA、CBT、ROCStories、JOCI、CLOTH、SWAG、ReCoRD 2.1.5 Psychological Reasoning心理推理：关于情绪（情感）和意图的推理，需要社会心理学的常识知识 代表数据集：Triangle-COPA、Story Commonsense、Event2Mind StoryCommonsenserashkin-2018a：要求预测Motivation和Emotions，以及Maslow (human need)、Reiss (human motives)、Plutchik (emotions) link：http://uwnlp.github.io/storycommonsense Theories of Motivation (Maslow and Reiss) and Emotional Reaction (Plutchik): Event2Mindrashkin-2018b：推理关于事件的intentions和reactions，每个事件都有1到2个参与者，三个任务：预测主要参与者的意图和反应，并预测其他人的反应 link：http://uwnlp.github.io/event2mind 2.1.6 Multiple Tasksconsist of several focused language processing or reasoning tasks so that reading comprehension skills can be learned one by one in a consistent format 代表数据集：bAbI、IIE、GLUE、DNC IIE：Inference is Everything，RTE的形式 DNC poliak：Diverse Natural Language Inference Collection，包含9个NLI任务需要7中不同类型的推理 recast from： Event Factuality, recast from UW (Lee, Artzi, Choi, &amp; Zettlemoyer, 2015), MEANTIME (Minard, Speranza, Urizar, Altuna, van Erp, Schoen, &amp; van Son, 2016), and (Rudinger, White, &amp; Van Durme, 2018b) Named Entity Recognition, recast from the Groningen Meaning Bank (Bos, Basile, Evang, Venhuizen, &amp; Bjerva, 2017) and the ConLL-2003 shared task (Tjong Kim Sang &amp; De Meulder, 2003) Gendered Anaphora Resolution, recast from the Winogender dataset (Rudinger et al., 2018a) Lexicosyntactic Inference, recast from MegaVeridicality (White &amp; Rawlins, 2018), VerbNet (Schuler, 2005), and VerbCorner (Hartshorne, Bonial, &amp; Palmer, 2013) Figurative Language, recast from puns by Yang, Lavie, Dyer, and Hovy (2015) and Miller, Hempelmann, and Gurevych (2017) Relation Extraction, partially from FACC1 (Gabrilovich, Ringgaard, &amp; Subramanya, 2013) Subjectivity, recast from Kotzias, Denil, De Freitas, and Smyth (2015) link：http://github.com/decompositional-semantics-initiative/DNC 2.2 Criteria and Consideration for Creating Benchmarks2.2.1 Task Format决定任务形式对于Benchmarks的创建是重要的一步，现有的任务形式有三类： Classification Task：有三种形式 Textual Entailment Task Cloze Task Traditional Multiple-Choice Task Open-ended Task：开放式任务 Span Subset of category labels：Story Commonsense Purely open-ended：Event2Mind、bAbI Generative 2.2.2 Evaluation Schemes评测形式 现有的评测结果都是直接给出是否通过(pass or fail grade)，没有任何反馈 理想的评测形式应该考虑有信息的指标，可以比较不同的方法，比较机器和人之间性能表现的差异 Evaluation Metrics：Precision、Recall、F-Measure、Exact-Match、Recall@k、BLEU、ROUGE Comparison of Approaches Human Performance Measurement 2.2.3 Data Biases数据分布的平衡 Label Distribution Bias Question Type Bias Superficial Correlation Bias：gender bias 2.2.4 Collection Methods[Not Focus] Manual versus Automatic Generation Automatic Generation versus Text Mining Crowsourcing Considerations 3.Knowledge Resources3.1 Overview of Knowledge Resources for NLU为了理解自然语言，通常需要语言学知识来却确定文本的句法、语义结构，再进一步使用通识、常识知识来增强对结构的理解，以达到更全面的理解 3.1.1 Linguistic Knowledge Resources带标记的句法、语义、篇章结构资源 Annotated Linguistic Corpora Penn TreeBank：POS tags &amp; syntactic structures based on context-free grammar PropBank：predicate-argument structures Penn Discourse TreeBank Abstract Meaning Representation (AMR) Lexical Resources WordNet VerbNet：hierarchical English Verb lexicon FrameNet：frame semantics for a set of verbs 3.1.2 Common Knowledge Resources Common knowledge refers to speciﬁc facts about the world that are often explicitly stated. 与Commonsense Knowledge的不同是cambria-2011：CS Know. required to achieve a deep understanding of both the low- and high-level concepts found in language. Yet Another Great Ontology (YAGO): with common knowledge facts extracted from Wikipedia, converting WordNet from a primarily linguistic resource to a common knowledge base. DBpedia: Wikipedia-based knowledge base originally consisting of structured knowledge from more than 1.95 million Wikipedia articles. WikiTaxonomy: consists of about 105,000 well-evaluated semantic links between categories in Wikipedia articles. Categories and relationships are labeled using the connectivity of the conceptual network formed by the categories. Freebase NELL Probase 3.1.3 Commonsense Knowledge Resources Commonsense knowledge, on the other hand, is considered obvious to most humans, and not so likely to be explicitly stated Cambria, E., Song, Y., Wang, H., &amp; Hussain, A. (2011). Isanette: A Common and Common Sense Knowledge Base for Opinion Mining. In 2011 IEEE 11th International Conference on Data Mining Workshops, pp. 315–322, Vancouver, BC, Canada. IEEE. Cyc ConceptNet AnalogySpace is an algorithm for reducing the dimensionality of commonsense knowledge so that knowledge bases can be more efﬁciently and accurately reasoned over. SenticNet: intended for sentiment analysis IsaCore: a set of “is a” relationships and conﬁdences. http://sentic.net/downloads COGBASE WebChild LocatedNear Atlas of Machine Commonsense (ATOMIC) about 300,000 nodes corresponding to short textual descriptions of events, and about 877,000 “if-event-then” triples representing if-then relationships between everyday events. 3.2 Approaches to Creating Knowledge Resources The goal is to create general knowledge bases to provide inductive bias for a variety of learning and reasoning tasks Manual Encoding Text Mining Crowdsourcing 4.Learning and Inference Approaches4.1 Symbolic and Statistical Approaches4.2 Neural Approaches common components in neural models: 4.2.1 Memory Augmentation针对需要理解状态变化或是具有多个支撑事实来进行文本理解的任务 Memory Network add a long-term memory component to track the world state and context MemNet can efficiently leverage a wider context in making inferences outperform primarily RNN and LSTM based models Recurrent Entity Networks (ENTENT) Henaff, M., Weston, J., Szlam, A., Bordes, A., &amp; LeCun, Y. (2017).Tracking the World State with Recurrent Entity Networks. In Proceedings of the 5th International Conference on Learning Representations.ICLR,2017. composed of several dynamic memory cell each cell learns to represent the state or properties concerning entities mentioned in the input. each cell is a Gated-RNN, only updates its content when new information relevant to the particular entity is received run in parallel, allow multiple locations of memory to be updated at the same time. unlike MemNet: MemNet only preform reasoning when the entire supporting text and the question are processed and loaded to the memory. when given a supporting text with multiple questions: ENTENT do not need to process the input text multiple times to answer these question. MemNet need to re-process the whole input for each question. drawbacks perform well in bAbI, but not in ProPara maintain memory registers for entities, it has no separate embedding for individual states of entities over time do not explicitly update coreferences in memory KG-MRC Das, R., Munkhdalai, T., Yuan, X., Trischler, A., &amp; McCallum, A.Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension.ICLR, 2019 Knowledge Graph Machine Reading Comprehension maintain a dynamic memory memory is in the form of knowledge graphs generated after every sentence of procedural text. generated knowledge graphs: are bipartite, connecting entities in the paragraph with their locations (currently, only capture the location relation) connections between entities and locations are updated to generate a new graph after each sentence KG-MRC learns some commonsense constraints automatically. recurrent graph representations help. 4.2.2 Attention Mechanism automatically provides an alignment between inputs and outputs have limitations when the alignment between inputs and outputs is not straightforward. sequentail attention self-attention multi-head comparison score function 4.2.3 Pre-Trained Models and Representations ELMO GPT BERT still far from human: SciTail、ReCoRD、OpenBookQA When to fine-tune sentence pair tasks 4.3 Incorporating External Knowledge WordNet in Textual Entailment ConceptNet in Commonsense Task Main Problems： how to incorporate external knowledge in modern neural approaches how to acquire relevant external knowledge 5.Other Related Benchmarks language-related tasks visual benchmarks perception 6.Discussion and Conclusion two types of commonsense knowledge are considered fundamental for human reasoning and decision making: intuitive psychology：心理 intuitive physics：物理 Challenges relation with humans: understanding how much Commonsense Knowledge is developed and acquire in humans and how they related to human Language Production and Comprehension may shed light on computational models for NLP difficult to identify and formalize Commonsense Knowledge disconnect between Commonsense Knowledge resources and approaches to tackle these benchmarks One likely reason is that these knowledge bases do not cover the kind of knowledge that is required to solve those tasks To address this problem, several methods have been proposed for leveraging incomplete knowledge bases Eg1 AnalogySpace：uses principle component analysis to make analogies to smooth missing commonsense axioms Eg2 Memory Comparison Networks：allow machines to generalize over existing temporal relations in Knowledge Sources in order to acquire new relations jointly develop benchmark tasks and construct knowledge bases Event2Mind &amp; ATOMIC CommonsenseQA &amp; ConceptNet only learning superficial artifacts from the dataset obscure statistical biases — high preformance, but not actual reasoning davis. Commonsense reasoning and commonsense knowledge in artiﬁcial intelligence. Commun. ACM, 58(9), 92–103. &#8617; rashkin-2018a. Modeling Naive Psychology of Characters in Simple Commonsense Stories. ACL,2018. &#8617; rashkin-2018b. Event2Mind: Commonsense Inference on Events, Intents, and Reactions. ACL,2018. &#8617; poliak. Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation. EMNLP, 2018. &#8617; cambria-2011. Isanette: A Common and Common Sense Knowledge Base for Opinion Mining. In 2011 IEEE 11th International Conference on Data Mining Workshops, pp. 315–322, Vancouver, BC, Canada. IEEE. &#8617;]]></content>
      <categories>
        <category>MRC-Research</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>mrc</tag>
        <tag>reasoning</tag>
        <tag>commonsense</tag>
        <tag>survey</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Memory Networks]]></title>
    <url>%2F2019%2F04%2F17%2Fpaper-memory-network%2F</url>
    <content type="text"><![CDATA[Memory Networks 是一种框架，在这个框架内部的每个module都可以根据特定任务的需要用不同的方式来实现（task-specific）。 本篇主要以 《End-to-End Memory Networks》 (2015, MemN2N) 对 Memory Network 的框架进行介绍。 Architecture 记忆网络的核心是记忆模块，可以看做是一个知识存储器。 在学习的过程中，首先需要对这个存储器的内容进行插入或更新，然后在测试的时候依靠这个存储器中的信息对于答案进行推理判断，具体包含以下四个主要模块： I：输入特征映射 将输入转换为内部特征的表示？ 将输入映射到特征空间 G：泛化 得到新的输入时，对过去的记忆进行更新； 称为泛化的原因是：在整个过程中网络能够根据未来的某些特定需要压缩、泛化本身的记忆； O：输出特征映射 根据当前的输入和记忆状态得到输出，输出的是内部特征表示的形式（以内部特征表示作为输出） R：响应 将上一步中的输出转换为指定的响应格式； 具体过程：对于一个特定的输入：one-hop 1、将转换为内部特征表示的形式； 2、根据输入更新记忆； 3、根据输入和记忆计算输出特征； 4、解码得到响应的结果； MemN2N 的模型结构图，左侧是单层结构，右侧是多层（3层）结构 Multi-hop的计算过程： Memory Module: Approach Details 模型的输入输出： 输入：inputs $x_1,…,x_n$ (会被存储到memory中，$x_i$是一个句子) 和 query $q$，词典大小为 $|V|$ 输出：answer $a$ Single Layer input memory representation，将输入映射到特征/memory空间 将 $x_i$ 通过 input embedding $A \in \mathbb{R}^{d\times |V|}$ 映射为 memory vector $m_i$ 将 $q$ 通过 question embedding $B \in \mathbb{R}^{d\times |V|}$ 映射为 internal state $u$ 计算每个 memory 和 query 之间的attention，得到匹配程度： $p_i = softmax(u^T m_i)$ 有多少个 memory vector 就有多少个 $p$ output memroy representation： 再将 $x_i$ 通过 output embedding $C \in \mathbb{R}^{d\times |V|}$ 映射为相应的 output vector $c_i$ 计算 response context vector $o$: $o = \sum_i p_i c_i$ generating final prediction: 使用 $o$ 和 $u$ 一起预测答案标签（可以是一个词） $\hat{a} = softmax(W(o+u))$ 模型中的主要训练参数为：$A$、$B$、$C$、$W$ Multiple Layers多层的结构就是对memory进行多次寻址（addressing/attention），每次关注不同的memory，主要的几点不同是： 第一层之后的每层/每个hop的 query vector 是前一层的 response context vector 和 query vector 的结合，可以用不同的结合方式计算： $u^{k+1} = u^k + o^k$ 每层之间的embedding矩阵$A^k$和$C^k$不是共享的，具体有两种 权重初始化方式，参考下面的weight typing。 Weight Typing每个embedding A 和 embedding C 都是与词典大小相等的词向量矩阵，在multiple layers的结构中引入这两个参数矩阵会带来很大的参数开销 Adjacent方式 使第$k+1$层的input embedding $A^{k+1}$ 等于 第$k$ 层的output embedding $C^{k}$：$A{k+1} = C^k$ 还是增加其他的约束： (a) 用最后一层的output embedding $C^{K}$ 去对 answer prediction中的参数矩阵 $W$ 进行赋值：$W^T = C^K$ (b) 使 question embedding 等于 第一层的input embedding $A^1$：$B = A^1$ Layer-wise（RNN-like）方式 不同的层之间使用相同的embedding参数，在层间加入一个线性映射 $H$ 来更新 $u$：$u^{k+1} = H u^k + o^k$ 在这种方式下，整体模型可以看成一个传统的rnn，将rnn的输出分为 internal 和 external 两类，$u$ 是rnn的hidden state Related Works Memory Networks Ask Me Anything: Dynamic Memory Networks for Natural Language Processing]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>note</tag>
        <tag>network</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ICLR2019 | Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question Answering]]></title>
    <url>%2F2019%2F04%2F11%2Fpaper-iclr2019-cfc%2F</url>
    <content type="text"><![CDATA[Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question AnsweringICLR 2019Victor Zhong, Caiming Xiong, Nitish Shirish Keskar, Richard SocherUniversity of Washington, Salesforce ResearchDatasets: Qangaroo-WikiHop MotivationModelExperimentSummary &amp; Analysis]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Analysis of Multi-Passage RC Task]]></title>
    <url>%2F2019%2F04%2F11%2Fmrc-analysis-multi-passage%2F</url>
    <content type="text"><![CDATA[Works of multi-passage MRC taskTarget Datasets: MS MARCO, Dureader Reference papers on this track: Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification. ACL,2018. A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension. EMNLP,2018. A Deep Cascade Model for Multi-Document Reading Comprehension. AAAI,2019. Multi-Mention Learning for Reading Comprehension with Neural Cascades. ICLR,2018. Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question Answering. ICLR,2019. Multi-style Generative Reading Comprehension. 2019. TO BE Updated]]></content>
      <categories>
        <category>MRC-Paper-Intro</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>research</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Analysis of Multi-Choice RC Task]]></title>
    <url>%2F2019%2F03%2F28%2Fmrc-analysis-multichoice%2F</url>
    <content type="text"><![CDATA[focus on the strategy of matching processing between (P, Q, Ans) target datasets: RACE, MCScripts Reference papers on multi-choice MRC task, especially toward matching processing. Hierarchical Attention Flow for Multiple-Choice Reading Comprehension. AAAI,2018. Dynamic Fusion Networks for Machine Reading Comprehension. 2017. A Co-Matching Model for Multi-choice Reading Comprehension. ACL,2018. Dual Co-Matching Network for Multi-choice Reading Comprehension. 2019. Convolutional Spatial Attention Model for Reading Comprehension with Multiple-Choice Questions. AAAI,2019. Option Comparison Network for Multiple-choice Reading Comprehension. 2019 Yuanfudao at SemEval-2018 Task 11: Three-way Attention and Relational Knowledge for Commonsense Machine Comprehension. 2018. HFL-RC System at SemEval-2018 Task 11: Hybrid Multi-Aspects Model for Commonsense Reading Comprehension. 2018. Co-Match Network (HCM)Motivation previous works: 之前的MRC的工作通常是基于句对的序列匹配（Pair-Wise Sequence Matching)，有如下情况： passage 与 question 和 candidate answer 的串联进行比较； passage 先与 question 进行比较，计算出 matching 结果，再使结果与 candidate answer 进行比较； 这样的计算方式不适用于多选型RC任务，具体存在以下几点问题： 1、仅将 passage 和 question 进行匹配，得到的结果可能没有意义并且会导致原始 passage 的信息丢失； 例如：问题 Which statement of the following is true？ 若将 question 和 candidate answer 串联成为一个序列，损失了 question 和 candidate answer 的交互信息； 基于此，多选RC任务需要解决匹配序列三元组 (matching sequence triplets)的问题； 本文的方法： match a question-answer pair to a given passage； explicitly treat the question and the candidate answer as two sequences and jointly match them to the given passage； 对P中的每个位置，都计算两个attention权重，构成两个匹配表示，形成一个co-match状态（同时计算P和Q/A的匹配），然后再用一个层次LSTM框架（2个LSTM）对passage进行编码； 层次汇聚信息： 在passage中的每个句子内部，信息从word-level汇聚在sentence-level 在passage中的句子序列维度上，再从sentence-level汇聚到document-level； 可以更好的处理，问题需要的信息分散在passage中不同句子，的情况 Model DetailsNotation:&nbsp;&nbsp;&nbsp;&nbsp;(one sentence in) Passage: $P\in \mathbb{R}^{d\times P}$&nbsp;&nbsp;&nbsp;&nbsp;Question: $Q \in \mathbb{R}^{d\times Q}$&nbsp;&nbsp;&nbsp;&nbsp;(one candidate answer in) Answer: $A \in \mathbb{R}^{d\times A}$ architecture co-matching encoding: the same BiLSTM $H^p\in \mathbb{R}^{l\times P}$, 每个句子分别计算 $H^q\in \mathbb{R}^{l\times Q}$, $H^a\in \mathbb{R}^{l\times A}$, 每个候选分别计算 attention: $G^q = softmax( (W^gH^q + b^g \otimes e_Q)^T H^p ) \in \mathbb{R}^{Q\times P}$ $G^a = softmax( (W^gH^a + b^g \otimes e_Q)^T H^p ) \in \mathbb{R}^{A\times P}$ aggregation: attentive passage representation $\bar{H}^q = H^q G^q \in \mathbb{R}^{l \times P}$ $\bar{H}^a = H^q G^a \in \mathbb{R}^{l \times P}$ co-match passage state: concurrently matches a passage state with both the question and the candidate answer. It represent how each P state can be matched with the Q and Candidate A. $M^q = ReLU(W^m[\bar{H}^q \ominus H^p; \bar{H}^q \otimes H^p]) + b^m \in \mathbb{R}^{l\times P}$ $M^a = ReLU(W^m[\bar{H}^a \ominus H^p; \bar{H}^a \otimes H^p]) + b^m \in \mathbb{R}^{l\times P}$ $W^m \in \mathbb{R}^{l\times 2l}$ $C = [M^q; M^a] \in \mathbb{R}^{2l \times P}$ hierarchical aggregation for each triplet $\{P_n, Q, A\}, n\in [1,N]$, get $C_n$ through co-match sentence-level aggregation of the co-matching states: sentence sequence representation merge into a single vector $h_n^s = MaxPooling(BiLSTM(C_n)) \in \mathbb{R}^l$ $MaxPooling$： row-wise max pooling final triplet matching representation: $H^s=[h_1^s, h_2^s,…,h_N^s]$ $h^t = MaxPooling (BiLSTM (H^s)) \in \mathbb{R}^{l}$ Output Layer for each candidate answer $A_i$, get $h_i^t \in \mathbb{R}^{l} $ $L(A_i|P,Q) = -log \frac{exp(w^Th_i^t)}{\sum_{j=1}^4 exp(w^T h_j^t)}$ Model Parameters word emb dim: 300 rnn hidden dim: 150 optimizer: Adamax, lr=0.002 batch：10 epochs：30 dropout：0.2 Dual Co-Matching NetworkMotivation previous work: 只计算了question-aware P表示和 option-aware P表示； 一些pretrainLM的做法是将P和Q串联成为一个句子，A单独作为另一个句子； 本文： model the relationship among passage，question and answer bidirectionally 在计算question-aware P表示和 option-aware P表示的同时，计算passage-aware Q表示和passage-aware O表示 Model Details Encoding $H^p = Bert(P) \in \mathbb{R}^{P\times l}$ $H^q = Bert(Q) \in \mathbb{R}^{Q\times l}$ $H^a = Bert(A) \in \mathbb{R}^{A\times l}$ $l$: Bert hidden state dimension Matching Layer attention between P and A: $W = softmax(H^p(H^a G+b)^T) \in \mathbb{R}^{P\times A}$ $G \in \mathbb{R}^{l\times l}$ $M^p = WH^a \in \mathbb{R}^{P\times l}$ $M^a = W^TH^p \in \mathbb{R}^{A\times l}$ $W \in \mathbb{R}^{P\times A}$ attention between P and Q in the same method, get: $M^q \in\mathbb{R}^{Q\times l}$ $W^\prime \in \mathbb{R}^{P\times Q}$ 问题：为什么P和Q进行attention，不计算question-aware的passage表示？ integration original contextual representation $S^a = F([M^a - H^a;M^a \cdot H^a]W_1 + b_1) \in \mathbb{R}^{P \times l}$ $S^p = F([M^p - H^p;M^p \cdot H^p]W_2 + b_2)\in \mathbb{R}^{A \times l}$ $F()$ is activation function $ReLU$ in the question side: $S^{p^\prime} \in \mathbb{R}^{P\times l}$ $S^q \in \mathbb{R}^{Q\times l}$ Aggregation Layer get final representation for each candidate answer row-wise max pooling $C^p = Pooling(S^p) \in \mathbb{R}^{l}$ $C^a = Pooling(S^a) \in \mathbb{R}^{l}$ $C^{p^\prime} = Pooling(S^{p^\prime}) \in \mathbb{R}^{l}$ $C^q = Pooling(S^q) \in \mathbb{R}^{l}$ $C = [C^p;C^a;C^{p^\prime};C^q]$ Output Layer $L(A_i|P,Q)=-log\frac{exp(V^TC_i)}{\sum_{j=1}^N exp(V^TC_j)}$ Model Parameters No description Option Comparison Network (OCN)Motivation previous work: read each option independently. compute a fixed-length representation for each option before comparing them. ideas: humans typically compare the options at multiple-granularity level before reading the article in detail and make reasoning more efficient. 人解决多选RC任务的策略，通常在仔细阅读文章之前会在不同粒度上比较候选答案。 通过比较候选答案，可以定位答案选项间的相互关系，在读文章时只关注与选项相互关系有关的文章信息。（更高效？more efﬁciently and effectively） 本文： explicitly compare options at word-level to better identify their correlations to help reasoning 首先使用一个skimmer network对每个option进行独立编码； 然后对每个option，将其与其他的options使用attention进行word-level的比较，来建立option之间的相互比较； 最后，带着聚集之后的option间的相关性，重读文章，进行推理和答案选择 Analysis: 这篇文章的主要更新的是option的表示 Model DetailsNotation:&nbsp;&nbsp;&nbsp;&nbsp;Passage: $P=\{w_1^p,…,w_m^p\}$&nbsp;&nbsp;&nbsp;&nbsp;Question: $Q= \{w_1^q,…,w_n^q\}$&nbsp;&nbsp;&nbsp;&nbsp;Answer set: $O=\{O_1,…,O_K\}$&nbsp;&nbsp;&nbsp;&nbsp;Each option: $O_k = \{w_1^o,…,w_{n_k}^o\}$ Overall: 4 stages concatenate each (article, question, option) triple into a sequence and use a skimmer to encode them into vector sequences. attention-based mechanism is leveraged to compare the options. the article is reread with the correlation information gathered in last stage as extra input. compute the probabilities for each option. Option Feature Extraction skimmer encoding: 将每个option与P和Q串联构成一个句子，使用BERT进行编码 $[P^{enc};Q^{enc};O^{enc}_k] = BERT()$ $P^{enc} \in \mathbb{R}^{d\times m}$ $Q^{enc} \in \mathbb{R}^{d\times n}$ $O^{enc}_k \in \mathbb{R}^{d\times n_k}$ 由于Q和option的关联紧密，将两者串联，作为option的特征 $O_k^q=[Q^{enc}|O^{enc}_k] \in \mathbb{R}^{d\times n_k^\prime}$ $n_k^\prime = n+n_k$ Option Correlation Features Extraction $Att(\cdot)$的计算方式：假设输入为$U\in \mathbb{R}^{d\times N}$和 $V\in \mathbb{R}^{d\times M}$ $v \in \mathbb{R}^{3d}$ 是参数 $s_{ij}=v^T[U_{:i};V_{:j};U_{:i}\circ V_{:j}]$ $A= Att(U,V;v)=[\frac{exp(s_{ij})}{\sum_i exp(s_{ij})}]_{ij} \in \mathbb{R}^{N\times M}$ option correlation feature extraction 分3步进行 option $O_k$ 与其他options进行one-by-one比较，收集 pair-wise correlation信息 $\bar{O}_k^{(l)}=O^q_l Att(O^q_l,O_k^q;v_o)$ $\tilde{O}_k^{(l)}=[O_k^q-\bar{O}_k^{(l)};O_k^q \circ \bar{O}_k^{(l)}] \in \mathbb{R}^{2d\times n_k^\prime}$ gather pair-wise correlation information $\tilde{O}_k^c=tanh(W_c [O_k^q;\{\tilde{O}_k^{(l)}\}_{l\neq k} ])$ $W_c \in \mathbb{R}^{d\times (d+2d(|O|-1))}$ element-wise gating 机制控制option feature和option-wise correlation information的融合，以产生option correlation features $O_k^c$ $g_k \in \mathbb{R}^{d\times n_k^\prime}$ $g_{k,:i}=\sigma (W_g [Q_{K,:i}^q; \tilde{O}_{k,:i}^c; \tilde{O}]+b_g)$ $g_{k,:i}$ 表示 g 向量的第i列 $\tilde{O}$ 的计算：关于 Q 的attention pooling $A_q = softmax(v_a^T Q^{enc})^T, v_a \in \mathbb{R}^d$ $\tilde{O}=Q^{enc}A^q \in \mathbb{R}^{d}$ option correlation features: $O_k^c\in \mathbb{R}^{d\times n_k^\prime}$ $O_{k,:i}^c = g_{k,:i} \circ O_{k,:i}^q + (1-g_{k,:i}) \circ \tilde{O}_{k,:i}^c$ Note: $O_k^c$ 不被压缩成fixed-length向量，文中的解释为-这样可以使我们的模型更灵活的使用correlation信息。 Article ReReading co-attention + self-attention 对于每个option $O_k$ 计算 co-attention: $A_k^c = Att(O_k^c,P^{enc};v_p) \in \mathbb{R}^{n_k^\prime \times m}$ $A_k^p = Att(P^{enc},O_k^c;v_p) \in \mathbb{R}^{m\times n_k^\prime}$ $\hat{O}_k^p = [P^{enc};O_k^c A_k^c]A_k^p \in \mathbb{R}^{2d\times n_k^\prime}$ fused with correlation information $\tilde{O}_k^p = ReLU(W_p[O_k^c;\hat{O}_k^p]+b_p) \in \mathbb{R}^{d\times n_k^\prime}$ self-attention to get full-info option representation $O_k^f\in \mathbb{R}^{d\times n_k^\prime}$ $\tilde{O}_k^s = \tilde{O}_k^p Att(\tilde{O}_k^p, \tilde{O}_k^p;v_r)$ $\tilde{O}_k^f = [\tilde{O}_k^p;\tilde{O}_k^s;\tilde{O}_k^p-\tilde{O}_k^s;\tilde{O}_k^p \circ \tilde{O}_k^s]$ $O_k^f = ReLU(W_f\tilde{O}_k^f +b_f)$ Answer prediciton score $s_k = v_s^T MaxPooling(O_k^f)$ MaxPooling: row-wise $v_s \in \mathbb{R}^d$ probability： $P(K|Q,P,O)=\frac{exp(s_k)}{\sum_i exp(s_i)}$ loss: $J(\theta)=-\frac{1}{N}\sum_i log(P(\hat{k}_i | Q_i,P_i,O_i)) + \lambda||\theta||_2^2$ Model Parameters for BERT base: batch:12 epochs:3 lr: $3\times 10^{-5}$ for BERT large: batch:24 epochs:5 lr: $1.5\times 10^{-5}$ $\lambda$: 0.01 lengths: P: 400 Q: 30 A: 16]]></content>
      <categories>
        <category>MRC-Paper-Intro</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>research</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Practicable Course List (continually updated)]]></title>
    <url>%2F2019%2F03%2F12%2Fcourse-list%2F</url>
    <content type="text"><![CDATA[The main areas of concern are: Natural Language ProcessingMachine Learning and Deep LearningReinforcement LearningFoundation of Mathematics NLP Stanford，CS224n http://web.stanford.edu/class/cs224n/index.html video：Winter 2019 CMU，CS 11-731 Machine Translation and Seq-to-Seq Models，2018 http://www.phontron.com/class/mtandseq2seq2018/ CMU，CS 11-747 Neural Networks for NLP，Spring 2018 http://www.phontron.com/class/nn4nlp2018/schedule.html video：2019 Oxford https://github.com/oxford-cs-deepnlp-2017/lectures Berkeley Applied Natural Language Processing http://people.ischool.berkeley.edu/~dbamman/info256.html Penn，CIS 700 Advanced Machine Learning for Natural Language Processing，Dan Roth http://www.cis.upenn.edu/~danroth/Teaching/CIS-700-006/index.html CS 4650 and 7650 https://github.com/jacobeisenstein/gt-nlp-class University of Washington CSE 447/547M: Natural Language Processing https://courses.cs.washington.edu/courses/cse447/19wi/ MLandDL Stanford，CS229 http://cs229.stanford.edu/index.html#info 李宏毅 http://speech.ee.ntu.edu.tw/~tlkagk/courses.html Berkeley，STAT 157 Introduction to Deep Learning http://courses.d2l.ai/berkeley-stat-157/index.html Berkeley，2019，无监督学习 CS294-158 Deep Unsupervised Learning Spring 2019 https://sites.google.com/view/berkeley-cs294-158-sp19/home Stanford，CS 236: Deep Generative Models https://deepgenerativemodels.github.io/ MIT，6.883 Science of Deep Learning：Bridging Theory and Practice https://people.csail.mit.edu/madry/6.883/ Maching Learning Summer School http://mlss.cc/ RL UCL，David Sliver http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html Berkeley S294-112 Deep Reinforcement Learning http://rail.eecs.berkeley.edu/deeprlcourse/ Stanford CS234: Reinforcement Learning Winter 2019 http://web.stanford.edu/class/cs234/ Berkeley CS188 Introduction to Artificial Intelligence https://inst.eecs.berkeley.edu/~cs188/sp19/ MathBasic Stanford，CS229T/STATS231 Statistical Learning Theory https://web.stanford.edu/class/cs229t/ CMU，10-708，Probabilistic Graphical Models Eric Xing， Spring 2014 http://www.cs.cmu.edu/~epxing/Class/10708/lecture.html NYU，MathsDL-spring18 Topics course Mathematics of Deep Learning, NYU, Spring 18. CSCI-GA 3033. https://github.com/joanbruna/MathsDL-spring18 Recommend Archive Links https://deep-learning-drizzle.github.io/index.html#contents]]></content>
      <categories>
        <category>ForStudy</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>study</tag>
        <tag>resource</tag>
        <tag>course</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Knowledge-based MRC Papers]]></title>
    <url>%2F2019%2F03%2F10%2Fmrc-knowedge-paper-info%2F</url>
    <content type="text"><![CDATA[A list of recent papers with respect to Knowledge-based Machine Reading Comprehension. table th:nth-of-type(2){ width: 60%; } Works on Knowledge-aware MRC Conf. Title Authors/Org. Note ACL2017 Leveraging knowledge bases in lstms for improving machine reading Yang, et al.CMU ACL2017 Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension Hongyu Lin, et al. ACL2017 World knowledge for reading comprehension: Rare entity prediction with hierarchical lstms using external descriptions Long, et al.McGill University ACL2018 Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge Mihaylov, et al.Heidelberg University knreader-note 2018 Dynamic Integration of Background Knowledge in Neural NLU Systems Dirk Weissenborn, et al. note EMNLP2018 Commonsense for Generative Multi-Hop Question Answering Tasks Lisa Bauer mhpgm-note EMNLP2018 Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text CMU (2018v1)ACL2019 Exploring Machine Reading Comprehension with Explicit KnowledgeExplicit Utilization of General Knowledge in Machine Reading Comprehension York University note ACL2018 Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text CMU note AAAI2018 Incorporating Structured Commonsense Knowledge in Story Completion 2019 Improving Question Answering with External Knowledge ACL2019 Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension PKUBaidu note MRC with Knowledge how to let the machine obtain Knowledge？ how to extract external knowledge? how to represent knowledge？ in which kind of format？ how to fuse the external knowledge? how to let the machine to learn Knowledge incrementally？ how to make the machine can automatically use Knowledge it already knows or it has been told？]]></content>
      <categories>
        <category>MRC-Paper-Intro</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>mrc</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018 | Dynamic Integration of Background Knowledge in Neural NLU Systems]]></title>
    <url>%2F2019%2F03%2F06%2Fpaper-2018-refinewordemb%2F</url>
    <content type="text"><![CDATA[Dynamic Integration of Background Knowledge in Neural NLU SystemsICLR,2018. RejectDirk Weissenborn, et.al.Datasets: SQuAD, TriviaQA, SNLI, MNLI Motivation the requisite background knowledge is indirectly acquired from static corpora. background knowledge learned from task supervision and also by pre-training word embeddings. 从静态的训练数据中获取背景知识有两点缺陷: 1/不是所有的对解决NLU任务重要的背景知识都可以从有限量的训练数据中抽取出来； 2/随着时间的变化，对于理解文本有帮助的事实也会发生变化； This work:（不同于仅依赖于从训练数据中获取静态知识） develop a new reading architecture for the dynamic integration of explicit background knowledge in NLU models. a new task-agnostic(任务无关) reading module provides reﬁned word representations to a task-speciﬁc NLU architecture by processing background knowledge in the form of free-text statements, together with the task-speciﬁc inputs. Model输入是：待理解的文本，即context，和抽取出的相关知识的assertions.然后，使用 word embedding refinement 的策略，增量式地读入context和assertions，最初使用上下文无关的词向量仅初始化.这种 contextually refined word embedding 可以看成是一种动态记忆，用来存储新结合的知识. External Knowledge as Supplementary Text Inputs 结合知识的形式： 本文中并不限制外部信息的形式：无结构/结构化知识都可以作为补充信息 结合何种知识： 从知识源中抽取上下文相关的信息本身就是个复杂的研究，并且依赖于知识库的形式 全面抽取所有潜在的assertions，然后依赖于我们的阅读结构来学习抽取相关的信息 Assertion Retrieval 抽取知识是为了获得句子之间的关联 抽取出连接头/尾实体在text中，尾/头实体在question中的知识 由于抽取出的assertion过多，使用排序分数对assertions进行打分（类似于tf-idf的打分方式，针对的是罕见但是重要的知识，选择top-k个） Refine Word Embeddings by Reading将词向量看做一种记忆，不仅包含通用的知识，还包含上下文信息和抽取的知识信息. 本文提出的增量式 refinement 过程编码输入文本，然后使用多个阅读步得到的编码输入来更新词向量矩阵.过程如图： Notations: $E^0$: 初始的词向量 $E^\ell$: 第$\ell$步更新的词向量 $X^\ell$: 第$\ell$步的上下文信息 $FC(z)=W z + b, W\in \mathbb{R}^{n \times m}, b\in \mathbb{R}^{n}, z\in \mathbb{R}^m$ 1.Unrefined Word Embeddings这一步的目标是根据预训练词向量$e_w^p \in \mathbb{R}^{n^\prime}$得到初始的non-contextual词表示，计算如下： $e_w^{p^\prime} = ReLU(FC(e_w^p))$ $g_w = \sigma(FC([e_w^{p^\prime} ; e_w^{char}]))$ $e_w^0 = g_w \cdot e_w^{p^\prime} + (1-g_w) \cdot e_w^{char}$其中，$e_w^{char}$是通过cnn编码（n convolutional ﬁlters w of width 5 followed by a max-pooling operation over time.）得到 2.Contextually Refined Word Representation在编码输入文本时， 给每个词concatenate一个长度为L(即进行refienment处理的次数)的one-hot向量表示对应的位($\ell$)置1， 得到输入文本$X_i^{\ell} \in \mathbb{R}^{d\times |x_i^l|}$ 经过lstm进行context编码: $\hat{X}_i^{\ell} = ReLU(FC(BiLSTM(X_i^{\ell})))$ 在任务中：$X^1$相当于是Passage(Premise)文本的表示，$X^2$相当于是Question(Hypothesis)的表示，额外的知识assertions是$X^3$ 在实验中，p\q的顺序对最终的结果没有显著的影响 更新词向量： 首先对所有在文本中与此词的lemma相同的词进行一个maxpool: $\hat{e}_w^{\ell} = max\{\hat{x}_k^{\ell} | x^{\ell} \in X^{\ell}, lemma(x_k^{\ell}) = lemma(w) \}$ 然后，用context-independent的表示去计算一个context-sensitive的表示 通过门控机制，是模型决定利用多少新读入的信息来改写词向量 $u_w^{\ell} = \sigma(FC( [e_w^{\ell -1}; \hat{e}_w^{\ell}] ))$ $e_w^{\ell} = u_w^{\ell} \cdot e_w^{\ell -1} + (1- u_w^{\ell})\cdot \hat{e}_w^{\ell}$ 关于pooling操作：在具有相同lemma的词上面进行pooling操作 有效的联系可以缓解长距离依赖问题 更充分的利用输入作为相关背景知识 Experiments这篇文章的实验是在NLI（SNLI）和DQA（SQuAD）的任务上进行。 对NLI任务上： 使用全部的数据进行训练时的提升不是很大 但是使用部分数据进行训练时的提升相对较多]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>mrc</tag>
        <tag>note</tag>
        <tag>nli</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ACL2018 | Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering]]></title>
    <url>%2F2019%2F03%2F03%2Fpaper-acl2018-slqa%2F</url>
    <content type="text"><![CDATA[Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question AnsweringACL 2018Wei Wang, Ming Yan, Chen Wu.Alibaba Group.Multi-Granularity; Hierarchical Attention Fusion; ArchitectureDatasets: SQuAD; TriviaQA 看到题目，首先就产生了三个关注点： multi-granularity, 代表哪些粒度？ (word level 和 sentence level) hierarchical, 有哪些层次？（co-attention 和 self-attention） fusion, 怎样进行融合？对粒度的融合（global level） Motivation 启发于人类通常的阅读模式： 浏览全篇文章，大致了解文章内容（通读全文） 浏览问题，记住问题，找到P和Q之间的联系，理解Q的意图 定位一个大致/粗略的潜在的答案区域，使注意力聚集到定位的上下文（重点阅读上下文） 再次回顾问题，确定一个最优答案 This Work 建模问题和文章中特定区域关联的同时，借助分层策略逐步集中注意力，是答案边界清晰 提出 hierarchical attention network： 逐步定位答案边界 建模P和Q之间的不同粒度层级间的关系 在Encoder层： 为了更好的建模Q和P的多个aspects：同时使用预训练的glove表示和ELMo表示作为一个词的表示 针对ELMo的融合，设计了一个 Representation-aware fusion 方法来结合ELMo输出向量和BiLSTM建模的上下文表示 在本文提出的Hierarchical Attention Fusion Network中： 利用co-attention和self-attention机制，逐步的将注意力聚集到最优的答案span co-attention with shallow semantic fusion self-attention with deep semantic fusion memory-wise bilinear alignment function 特点： fine-grained fusion 结合attention vector 更好的建模P和Q的关系 multi-granularity attention 应用于 word-level 和 sentence-level 与其他方法不同的是： 利用全局表示（原始的上下文表示）构建attention表示 利用fusion layer来对attention表示进行进一步的微调 Model 本文提出模型 SLQA+ (Semantic Learning for Question Answering) 的整体架构： encoder、attention、matching、output 1.Encoder Layer 输入是 P和Q的每个词的glove向量和ELMo向量： 问题：$\{e_t^Q\}_{t=1}^m$，$\{c_t^Q\}_{t=1}^m$ 文章：$\{e_t^P\}_{t=1}^n$，$\{c_t^P\}_{t=1}^n$ 输出：original context representation BiLSTM 的输出结果，再次和ELMo 串联concat 问题：Q = $u_t^Q = [BiLSTM_Q(e_t^Q, c_t^Q), c_t^Q]$ 文章：P = $u_t^P = [BiLSTM_P(e_t^P, c_t^P), c_t^P]$ 2.Hierarchical Attention Fusion Layer original context representation和通过attention得到的aligned representation可以反映上下文不同粒度的语义 2.1 Co-attention &amp; Fusion co-attention 过程： 计算一个soft-alignment 矩阵： S_{ij} = Att(u_t^Q, u_t^P) = ReLU(W^T_{lin}u_t^Q)^T \cdot ReLU(W^T_{lin}u_t^P) 计算 P2Q attention \alpha_{j} = softmax(S_{:j}) \tilde{Q}_{:t} = \sum_j \alpha_{ij} \cdot Q_{:j}, \forall j \in [1,...,m] 计算 Q2P attention \beta_j = softmax(S_{i:}) \tilde{P}_{k:} = \sum_i \beta_{ik} \cdot P_{i:}, \forall i \in [1,..,n] Fusion：本文重点 定义fusion kernel $P^\prime=Fuse(P,\tilde{Q})$ $Q^\prime=Fuse(Q,\tilde{P})$ simple fusion 过程： m(P,\tilde{Q}) = tanh(W_f[P;\tilde{Q};P\circ\tilde{Q};P-\tilde{Q}] + b_f) 输出：利用gate机制，融合/refine 注意力表示和original contextual 表示 希望利用 original context representation 提供的重要的 global level 的语义信息提供指导，进一步引入gate机制控制不同层次表示的融合 文档：$P^\prime = g(P,\tilde{Q})\cdot m(P,\tilde{Q}) + (1-g(P, \tilde{Q})）\cdot P$ 问题：$Q^\prime = g(Q,\tilde{P})\cdot m(Q,\tilde{P}) + (1-g(Q, \tilde{P})）\cdot Q$ $g(\cdot)$的定义见2.3 2.2 Self-attention &amp; Fusion文档的self-attention fusion过程： 首先将manual feature引入，与refined question-aware passage表示进行串接 $D = BiLSTM([P^\prime; feat_{man}])$ self-alignment fusion process $L = softmax(D \cdot W_1 \cdot D^T)$ $\tilde{D} = L \cdot D$ $D^\prime = Fuse(D,\tilde{D})$ 双向LSTM获得最终的上下文文档表示： $D^{\prime\prime} = BiLSTM(D^\prime)$ 问题端的self-attention fusion过程 获得新的问题上下文表示：$Q^{\prime\prime} = BiLSTM(Q^\prime)$ $Q^\prime$ 来自于co-attention+fusion的结果 linear self-alignment, 使用 linear transformation (linear sequence attention，同drqa中的问题编码)，将问题编码为向量 $\gamma = softmax(w_q^T Q^{\prime\prime})$ $q = \sum_{j} \gamma_j \cdot Q^{\prime\prime}, \forall j \in [1,…,m]$ 2.3 Fusion Functions simple concat：简单讲两个channel的输入进行串联（计算m(·)时） Full Projection：heuristic matching，形如 $[P;\tilde{Q};P\circ\tilde{Q};P-\tilde{Q}]$ Scalar-based fusion：训练一个标量参数 $g(P,\tilde{Q}) = g_p$ Vector-based Fusion: 效果最好的 $g(P,\tilde{Q}) = \sigma(w_g^T \cdot [P;\tilde{Q};P\circ\tilde{Q};P-\tilde{Q}] + b_g)$ $w_g$是待训练的权重向量 Matrix-based Fusion： $g(P,\tilde{Q}) = \sigma(W_g^T \cdot [P;\tilde{Q};P\circ\tilde{Q};P-\tilde{Q}] + b_g)$ $W_g$是待训练的权重矩阵 3.Output Layer bilinear match function $P_{start} = softmax(q \cdot W_s^T D^{\prime\prime})$ $P_{end} = softmax(q \cdot W_e^T D^{\prime\prime})$ 可以看做是对问题的回顾 ExperimentsSummary &amp; Analysis ELMo 与上下文表示融合的新思路 原始的context representation可以看做是global level的信息，在fusion中的作用很大(每一步的fusion都是将attention representation与original context representation进行融合)，利用original context representation对attention之后的表示进行融合，微调带有注意力的表示 计算attention时，采取 linear + relu 的bilinear的效果最优 输出层的 bilinear match function 对结果的提升很大]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>note</tag>
        <tag>model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EMNLP2018 | Commonsense for Generative Multi-Hop Question Answering Tasks]]></title>
    <url>%2F2019%2F02%2F21%2Fpaper-emnlp2018-mhpgm%2F</url>
    <content type="text"><![CDATA[Commonsense for Generative Multi-Hop Question Answering TasksEMNLP 2018Lisa Bauer et al. UNC.Multi-Hop reasoning; generative-mrc; commmonsense knowledgeDatasets: NarrativeQA; QAngaroo-WikiHopSource code: https://github.com/yicheng-w/CommonSenseMultiHopQA Motivation Multi-Hop Generative Task (e.g. NarrativeQA) requires models to reasong, gather and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning requires understanding implicit relations (external, background commonsense knowledge). Related work: some fact-based datasets (e.g. SQuAD) do not need to place heavy emphasis on multi-step reasoning capabilities. some multi-hop datasets (e.g. QAngaroo) prompt a strong focus on multi-hop reasoning in very long texts. QAngaroo is an extractive dataset where answers are guaranteed to be spans within the context, thus, it more focuse on fact finding and linking. This work: a. a strong generative baseline, Multi-Hop Pointer-Generator Model, uses a multi-attention to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. b. a novel system for selecting grounded multi-hop relational commonsense information from ConceptNet via a pointwise mutual information and term-frequency based scoring function. c. a selectively gated attention mechanism to insert selected commonsense paths between the hops of document-context reasoning. multi-hop commonsense paths: multiple connected edges within ConceptNet graph that give us information beyond a single relationship triple. different aspects of the commonsense relationship path at each hop to bridge different inference gaps in the multi-hop task. Model Model Overview: 1.Multi-Hop Pointer-Generator Baseline模型的输入: context: $X^C={w_1^C, w_2^C,…,w_n^C}$ query: $X^Q = {w_q^Q, w_2^Q,…,w_m^Q}$ 模型的输出: answer tokens: $X^a = {w_1^a,w_2^a,…,w_p^a}$ 1.1.Embedding Layerpretrained word embeddings with ELMO: $e_i^Q$/$e_i^C$ $\in \mathbb{R}^{d+1024}$ 1.2.Reasoning Layer$k$ 个 reasoning cell 增量式地更新 context representation. 第 $t$ 个reasoning cell的计算过程: 输入: 前一时刻的输出 $\lbrace c_i^{t-1} \rbrace _{i=1}^n$ 和 query 的向量 $\lbrace e_i^Q \rbrace _{i=1}^m$ a.通过 cell-specific Bi-LSTM 计算一个 step-specific 的 context 和 query 的编码 $u^t=BiLSTM(c^{t-1})$; $v_t=BiLSTM(e^Q)$ b.通过bi-attention 计算 context 中的相关方面 来模拟一个hop的推理: context-to-query attention: $S_{ij}^t = W_1^t u_i^t + W_2^t v_j^t + W_3^t (u_i^t \odot v_j^t)$ p_{ij}^t = \frac{exp(S_{ij}^t)}{\sum_{k=1}^m exp(S_{ik}^t)} (c_q)^t_i = \sum_{j=1}^m p_{ij}^t v_j^t \in \mathbb{R}^{n \times dim} query-to-context attention vector: $m_i^t = \max_{1 \leq j\leq m} S_{ij}^t$ p_i^t = \frac{exp(m_i^t)}{\sum_{j=1}^n exp(m_j^t)} q_c^t = \sum_{i=1}^n p_i^t u_i^t \in \mathbb{R}^{dim} 更新的context representation: $c_i^t = [u_i^t; (c_q)_i^t; u_i^t \odot (c_q)_i^t; q_c^t \odot (c_q)_i^t]$ $c^0 = e^C$ 最后一时刻的输出是 $c^k$ 1.3.Self-Attention Layer 帮助模型处理long context中的长期依赖 输入是 reasoning 层最后一时刻的输出 经过 一个BiLSTM之后的表示 $c^{SA}$ 计算流程： $S_{ij}^{SA} = W_4 c_i^{SA} + W_5 c_j^{SA} + W_6(c_i^{SA}\odot c_j^{SA})$ $p_{ij}^{SA} = exp(S_{ij}^{SA}) / \left( \sum_{k=1}^n exp(S_{ij}^{SA}) \right)$ $c^\prime = \sum_{j=1}^n p_{ij}^{SA} c_j^{SA}$ $c^{\prime\prime} = BiLSTM([c^\prime;c^{SA};c^\prime \odot c^{SA}])$ $c = c^k + c^{\prime\prime}$ 1.4.Pointer-Generator Decoding Layer 输入: $x_t$, 前一时刻解码出的词向量表示 $s_{t-1}$, 前一时刻的隐藏层状态 $a_{t-1}$, 上下文向量 计算: 当前时刻的隐藏层状态: $s_t = LSTM([x_t; a_{t-1}], s_{t-1})$ 计算在生成词典上的概率分布: $P_{gen} = softmax(W_{gen}s_t + b_{gen})$ 计算 attention (使用 Bahdanau 的attention计算过程): \alpha_i = v^\intercal tanh(W_c c_i + W_s s_t + b_{attn}) \hat{\alpha}_i = \frac{exp(\alpha_i)}{\sum_{j=1}^n exp(\alpha_j)} \mathbf{a}_i = \sum_{i=1}^n \hat{\alpha}_i c_i 计算选择生成还是复制的概率: $\mathbf{o} = \sigma(W_a \mathbf{a}_t + W_x x_{t} + W_s s_t + b_{ptr})$ $\mathbf{p}^{sel} = softmax(\mathbf{o}) \in \mathbb{R}^2$ 最终 $t$ 时刻输出的分布为: P_t(w) = p_1^{sel} P_{gen}(w) + p_2^{sel} \sum_{i:w_i^C=w} \hat{\alpha}_i 2.Commonsense Selection and Representation为什么需要常识知识：知识关系有时候没有直接在文本中指出 由于语义网络/知识图谱的规模较大，包含很多无关信息，需要设计有效的选择算法来确定需要的信息 (有用的知识且可以在context-query对中落地(grounded:在context-query中出现) ) 常识知识选择策略：包含两方面 1.通过树结构，收集潜在的相关知识，目的是选择出具有high recall的候选推理路径； 2.通过三步打分策略对候选路径进行排序和过滤，以确保加入信息的质量和多样性； Initial Node Scoring, Cumulative Node Scoring, Path Selection 图: Commonsense selection approach 2.1.Tree Construction树的根节点为query中的一个词, 通过一些分支操作来构建多步推理路径.针对问题中的一个词/concept $c_1$, 进行如下操作: Direct Interaction:方向交互 从ConceptNet中选择与$c_1$和文本上下文中concept( $c_2 \in C$ )有直接链接的关系$r_1$(多个) 例如图中的第一列 Multi-Hop 继续在ConceptNet中选择文本中另外的concept($c_3 \in C$)与$c_2$有链接的关系$r_2$ Outside Knowledge 无文本上下文的约束,寻找收集外部知识 在ConceptNet中通过关系$r_3$寻找$c_3$的邻居(one-hop)得到 $c_4 \in nbh(c_3)$ Context-Grounding 再次利用context进行约束, 来确保3中额外的知识是对任务有帮助的 即, 使$c_3$通过$c_4$可找到的二阶邻居$c_5 \in C$是在文本中出现的 这构建路径的过程中，有几点疑问: a. 如果针对一个query中的concept无法查找到这么长的路径如何处理？ 2.2.Rank and Filter在构建树的过程中, 会收集大量的潜在相关路径, 相应地会引入很多噪声以及与问题无关的路径, 所以需要对2.1中收集到的知识路径进行排序过滤掉噪音. Initial Node Scoring: 初始化节点分数 选择有节点可以提供对context的理解的重要信息的路径 使用 tf 来估计context中concept的重要度和显著度: $score(c) = \frac{count(c)}{|C|}, c \in \lbrace c_2,c_3,c_5 \rbrace$ 对于outside Knowledge选择出来的节点$c_4$, 希望它与推理路径中的$c_1$到$c_3$保持逻辑一致性 利用点互信息 PMI(c_4,c_{1-3}) = log(\frac{\mathbb{P}(c_4,c_{1-3})}{\mathbb{P}(c_4) \mathbb{P}(c_{1-3})}) \mathbb{P}(c_4,c_{1-3}) = \frac{Num.\ of\ paths\ connecting\ c_1,c_2,c_3,c_4}{Num.\ of\ distinct\ paths\ of\ length\ 4} \mathbb{P}(c_4) = \frac{Num.\ of\ nodes\ that\ can\ reach\ c_4}{|ConceptNet|} \mathbb{P}(c_{1-3}) = \frac{Num.\ of\ paths\ connecting\ c_1,c_2,c_3}{Num.\ of\ paths\ of\ length\ 3} 由于PMI对low-frequency敏感, 改进为 normalized PMI $score(c_4) = PMI(c_4, c_{1-3}) / (-log\mathbb{P}(c_4,c_{1-3}))$ 由于每个连接分支代表了one-hop，所以不同层级的hops或者是拥有不同父节点的nodes无法与其他节点相比较, 最终对每个节点的打分进行归一化: 在同级(siblings)节点中进行归一化 $n-socre(c) = softmax_{siblings(c)}(score(c))$ Cumulative Node Scoring 累积节点打分 选择多跳的Commonsense路径中有相关信息的,根据节点以其子节点的相关性和显著性对节点进行再次打分 采取bottom-up的累积计算方式 对于叶子节点(leaf node): $c-socre = n-score$ 对于非叶子节点 $c-score(c_l) = n-socre(c_l) + f(c_l)$ $f(c_l)$是该节点的$c-socre$打分top-2的子节点的$c-score$平均分值 Path Selection 路径选择 采取top-down breath-first(广度优先)的方式选择路径 从根节点(回顾: query中的一个concept)开始, 递归的选择其两个具有最高累计得分的子节点, 直到选择到叶子节点. 选择路径数: $2^4=16 paths$ 路径直接以token序列的方式传给模型 3.Commonsense Model Incorporation有选择性的结合需要的知识，使用常识知识来弥补推理步之间的gaps.提出 Necessary and Optional Information Cell (NOIC) 一种 selectively gated 注意力机制。 输入:以词序列的形式表示给定的 list of commonsense logic paths: $X^{CS} = {w_1^{CS}, w_2^{CS},…,w_l^{CS}}$ $w_l^{CS}$表示构成一条路径的token序列 使用词向量表示: $e_{ij}^{CS}\in \mathbb{R}^d$ NOIC 是基于 Baseline Reasoning Cell 的扩展: 在第 $t$ 个推理步, 得到 baseline reasoning cell 的输出之后, 为Commonsense信息计算cell-specific表示: 将Commonsense paths上的所有向量串接, 每条路径得到一个表示向量$u_i^{CS}$ 通过映射进行降维, 使维度与 $c_i^t$ 的维度相等 $v_i^{CS} = ReLU(W u_i^{CS} + b)$ 利用attention机制使Commonsense与context之间进行交互: S_i^{CS}=W_1^{CS}c_i^t + W_2^{CS} v_j^{CS} + W_3^{CS} (c_i^t \odot v_j^{CS}) p_{ij}^{CS} = \frac{exp(S_{ij}^{CS})}{\sum_{k=1}^l exp(S_{ij}^{CS})} c_i^{CS} = \sum_{j=1}^l p_{ij}^{CS} v_j^{CS} 最后, 将Commonsense-aware的context表示和baseline reasoning cell的结果进行组合得到NOIC的输出 z_i = \sigma(W_z [c_i^{CS}; c_i^t] + b_z) (c_o)_i^t = z_i \odot c_i^t + (1-z_i) \odot c_i^{CS} 通过这种方式做到在每一个推理步对知识进行有选择的结合 ExperimentsResultsAblations Model Ablations Commonsense Ablations Commonsense Selection Summary &amp; AnalysisTBA]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>note</tag>
        <tag>multi-hop</tag>
        <tag>reasoning</tag>
        <tag>commonsense</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Commonsense to Controllable TG]]></title>
    <url>%2F2019%2F01%2F23%2Fpaper-tg-with-commonsense%2F</url>
    <content type="text"><![CDATA[本文将介绍 两篇 关于利用常识知识进行可控地文本生成的论文，都是清华大学黄民烈老师组的研究工作： Commonsense Knowledge Aware Conversation Generation with Graph Attention. IJCAI,2018.Story Ending Generation with Incremental Encoding and Commonsense Knowledge. AAAI,2019. 相比于文本生成问题，我更关注怎样将常识知识引入到文本的理解，即编码过程。 1.CS Know. to Dialogue with GAT这篇文章的主要目的是利用大规模的常识知识帮助对话的理解与生产。在对话任务中，给定一个用户 post： 在编码端：采取静态策略 从(常识)知识库中抽取出与 post 相关的知识子图 在编码端知识子图是静态的，包含与 post 相关的信息 用 Graph Attention(GAT) 机制进行编码 在解码端：采取动态策略 有关注的读取 Knowledge Graph 以及 Knowledge Graph 中的知识三元组 文章中强调了一点：与已有的其他模型(分开、独立使用 knowledge triples/entities)不同，此模型将每个 knowledge graph 作为一个整体进行处理，可以编码更多的结构信息以及有关联的语义信息。 模型的整体结构: Task Definition and Overview user post(input): $X=x_1 x_2 … x_n$ some commonsense knowledge graphs: $G={g_1, g_2, …, g_{N_G}}$ desired response(output): $Y=y_1 y_2 … y_m$ 使用 post 中的每个词作为查询，去常识知识库中 为每个词抽取出一个对应的子图: eg: 对于 post $X=x_1 x_2 … x_n$, 其对应的抽取出来的图是 $G={g_1, g_2,…,g_{N_G}}$ 每个子图由一个三元组集合构成: $g_i = {\tau_1, \tau_2, …, \tau_{N_{g_i}}}$ 每个三元组(head entity, relation, tail entity): $\tau = (h,r,t)$ 使用 TransE 来表示 KG 中的实体和关系; 最终每个三元组 $\tau$ 表示为: $k = (h,r,t) = MLP(TransE(h,r,t))$ $h/r/t$ 为各自的 TransE Embedding 使用 $MLP$ 是为了缩小 知识库 和 无结构对话文本 之间的 表示差距(bridge the representation gap) 对于在知识库中没有检索到匹配的词，使用一个特殊的 Not_A_Fact 来表示 Knowledge InterpreterKnowledge Interpreter overview: 知识感知的词表示: $e(x_t) = [w(x_t);g_i]$ Knowledge Interpreter 的主要目的是用每个词对应的知识子图向量来增强词表示 使用 post 中的每个词 $w_t$ 作为 key entity (红色点)去抽取出一个子图 $g_i$ (上图中黄色的部分) knowledge interpreter 的输出为静态图注意力机制计算得到的 graph vector $g_i$ Static Graph Attention 使用功能 GAT 的好处是：不仅可以编码结构语义信息还可以考虑到图中节点间的关系 SGA 的输入: knowledge triple vectors K(g_i) = {k_1, k_2, ..., k_{N_{g_i}}} SGA 的输出: knowledge graph vector g_i = \sum_{n=1}^{N_{g_i}} \alpha_n^s [h_n;h_t] \alpha_n^s = \frac{exp(\beta_n^s)}{\sum_{j=1}^{N_{g_i}} exp(\beta_j^s)} \beta_n^s = (W_r r_n)^T tanh(W_h h_n + W_t t_n) 其中, $(h_n, r_n, t_n) = k_n$ (Note: 在原始的GAT中没有引入relation向量计算attention的score) 2.CS Know. to Story Ending Generation这篇文章的主要目的是生成连贯、合理且有逻辑的故事结尾。 故事例子：故事与知识的关联 动机： 为故事生成一个合理的结尾需要对故事有很强的理解，需要以下两个方面 Context Clues：故事的逻辑、因果关系以及时序依赖（logic/causality/chronological order）通常跨越多个句子，通过句子中的事件/实体序列来获取； Implicit/Commonsense Knowledge：超越文本表明信息的隐含知识 针对以上的需求/目的，本文提出了： Incremental Encoding Schema：表示文本线索 逐句编码 Multi-Source Attention：帮助故事理解，充分利用常识知识 为每个词抽取一个 one-hop Knowledge graph，计算graph的表示 在编码当前句子时，不仅对前一个句子中的每个词的上下文表示进行Attention，还对每个词的KG进行Attention Overview模型的整体结构: Incremental Encoding Scheme最直接的对故事进行上下文编码的方法有： 将所有的句子进行串联，构成一个长句子，然后用LSTM进行计算 使用带有 Hierarchical-Attention 的层次LSTM 但是这两种都不能够有效的表示上下文的线索信息，相邻句子中事件/实体的时序顺序(chronological order)和因果关系(causal relationship) 本文提出的增量编码模式：编码当前句子 $X_i$ 中的第j个位置的隐状态计算如下 $h_j^{(i)} = LSTM(h_{j-1}^{(i)}, e(x_j^{(i)}), c_{lj}^{(i)}), i&gt;2$ 其中，$e(\cdot)是词向量$， $c_{lj}^{(i)}$ 是关于前一个句子的上下文向量（基于隐状态$h_{j-1}^{(i)}$） Multi-Source Attention通过多源Attention来获取上下文向量表示文本线索信息，由两部分组成： $c_{lj}^{(i)} = W_l( [c_{hj}^{(i)} ; c_{xj}^{(i)}] ) + b_l$ $c_{hj}^{(i)}$ 是 state context vector，即前一个句子隐状态的权重和 c_{hj}^{(i)} = \sum_{k=1}^{l_{i-1}} \alpha_{hk}^{(i)} h_k^{(i-1)} \alpha_{hk}^{(i)} = \frac{ exp( \beta_{h_{k,j}}^{(i)} ) }{ \sum_{m=1}^{l_{i-1}} exp( \beta_{h_{m,j}}^{(i)} ) } \beta_{h_{k,j}}^{(i)} = h_{j-1}^{(i)} W_s h_k^{(i-1)} $h_k^{(i-1)}$ 前一个句子第k时刻的隐状态 $c_{xj}^{(i)}$ 是 knowledge context vector， 即前一个句子的知识表示权重和 c_{xj}^{(i)} = \sum_{k=1}^{l_{i-1}} \alpha_{xk}^{(i)} g(x_k^{(i-1)}) \alpha_{xk}^{(i)} = \frac{ exp( \beta_{x_{k,j}}^{(i)} ) }{ \sum_{m=1}^{l_{i-1}} exp( \beta_{x_{m,j}}^{(i)} ) } \beta_{x_{k,j}}^{(i)} = h_{j-1}^{(i)} W_k g(x_k^{(i-1)}) $g(x_k^{(i-1)})$ 是前一个句子中第k个词的graph vector $h_{j-1}^{(i)}$ 是第i个句子中的第j个位置的隐状态，即当前句子中的前一时刻的隐状态 Knowledge Graph Representation对知识图编码有两种方式： graph attention contextual attention (knowledgeable-reader) 针对词$x$抽取（以$x$为头实体）出的图: $G(x) = {R_1,R_2, …,R_{N_x}}$ $R_i$ 为知识三元组 $(h,r,t)$， 用词向量表示h/t， r向量作为参数进行学习；$N_x$ 为 $x$ 的邻居数量； Graph Attention 的图表示方式 g(x) = \sum_{i=1}^{N_x} \alpha_{R_i} [h_i；t_i] \alpha_{R_i} = \frac{ exp(\beta_{R_i}) }{ \sum_{j}^{N_x} exp(\beta_{R_j})} \beta_{R_i} = (W_r r_i)^T tanh( W_h h_i + W_t t_i ) Contextual Attention 的图表示方式 g(x) = \sum_{i=1}^{N_x} \alpha_{R_i} M_{R_i} M_{R_i} = BiGRU(h_i, r_i, t_i) \alpha_{R_i} = \frac{ exp(\beta_{R_i}) }{ \sum_{j}^{N_x} exp(\beta_{R_j})} \beta_{R_i} =h_{(x)}^T W_c M_{R_i} $h_{(x)}$ 是词$x$的隐状态 CS Know. Process对于常识知识的抽取方式以及和数据的融合方式 CCM中： 实体和关系的向量通过transE进行学习，维度均为100 去除了由多个词构成的头/尾实体 最终的数据量： 三元组：120850 实体数：21471 关系数：44 StoryEndGen中： 只抽取出one-hop的三元组 为每个词抽取最多10个三元组; 头/尾实体为名词(noun)或动词(verb) 最终的数据量： 关系数：45 三元组：16652 两篇论文都是直接给出了处理好的数据，并没有给出数据的预处理过程，github链接分别为： https://github.com/tuxchow/ccm https://github.com/JianGuanTHU/StoryEndGen]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>note</tag>
        <tag>text-generation</tag>
        <tag>commonsense</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Survey about Works of MOSAIC]]></title>
    <url>%2F2019%2F01%2F21%2Fnote-research-ai2-mosaic%2F</url>
    <content type="text"><![CDATA[MOSAIC 是 AI2 (ALLEN Institute for Artificial Intelligence) 研究院中进行机器常识智能研究的小组1. 现有的研究项目：(Update at July, 2019) Commonsense Knowledge Graphs: Exploring semi-structured representations of commonsense.2 Visual Commonsense Reasoning: 视觉常识推理项目3 Mosaic Commonsense Benchmarks: Measuring progress on Machine Common Sense.4 对应的主要Papers： ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. AAAI,2019. From Recognition to Cognition: Visual Commonsense Reasoning SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. EMNLP,2018. 其他Papers: Event2Mind: Commonsense Inference on Events, Intents, and Reactions. ACL,2018. Modeling Naive Psychology of Characters in Simple Commonsense Stories. ACL,2018. Reasoning about Actions and State Changes by Injecting Commonsense Knowledge. EMNLP,2018. 重点关注 Commonsense Knowledge Graphs 这部分的工作 Commonsense Knowledge Graphs Event2Mind: Commonsense Inference on Events, Intents, and Reactions. ACL,2018. ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. AAAI,2019. Event2Mind常识推理任务 推理类型：2类（语用推理 pragmatic inference） intent 意图 可以解释为什么施事者导致事件发生，是行动或事件的心理前提 emotional reaction 情绪反应 反应定义为解释施事者和参与事件的其他人的心理状态将如何因此改变 反应可以被认为是一种行为或事件的心理后置条件 任务目标：给定一个 event phrase，预测 X intent / X reaction / Y reaction pre-condition: X intent post-condition: X/Y reaction 例子： ATOMIC关于 if-then 推理类型的推理知识库（Knowledge base about inferential knowledge on if-then reasoning types） 例子： nine if-then relation types to distinguish: （不同上不全面的刻画维度） cause vs. effect [Xintent，Xneed] vs. [Xwant，Xreaction，Effect on X，Effect on Y，Ywant，Yreaction] agents vs. themes：agent（X，who cause the event），themes（Other） [Xintent，Xneed，Xwant，Xreaction，Effect on X， Xattribute] vs. [Effect on Y，Ywant， Yreaction] voluntary vs. involuntary events [Xintent，Xneed，Xwant，Ywant] vs. [Xreaction，Effect on X，Effect on Y，Yreaction，Xattribute] actions vs. mental states [] vs. [] dynamic vs. static [Xintent，Xneed，Xwant，Xreaction，Effect on X，Effect on Y，Ywant，Yreaction] vs. [Xattribute] taxonomy of if-then reasoning types: based on the content being predicted: **if-event-then-mental-state** * mental pre-/post- conditions of an event * 3 relations: **X inent/ X reaction/ Other reaction** **if event-then event** * events that constitute probable pre- and postconditions of a given event * 5 relations: **X need/ X want/ Effect on X/ Other want/ Effect on Other** **if event-then-person** * a stative relation that describes how the subject of an event is described or perceived * 1 relations: **X attribute** based on their causal relations causes（因） effect（果） stative（状态） 图例： 与现有knowledge base的对比： DARPA LeaderboardsAbductive Natural Language inference (alpha NLI)HellaSwag: Can a Machine Really Finish Your Sentence?Physical IQa: Pyhsical Interaction QASocial IQA: Social Interaction QA1.https://mosaic.allenai.org/ ↩2.https://mosaic.allenai.org/projects/commonsense-knowledge-graphs ↩3.https://visualcommonsense.com/ ↩4.https://mosaic.allenai.org/projects/mosaic-commonsense-benchmarks ↩]]></content>
      <categories>
        <category>MRC-Research</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>research</tag>
        <tag>note</tag>
        <tag>cs-know</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20190121 | 清华大学AI研究院知识智能研究中心发布会的记录]]></title>
    <url>%2F2019%2F01%2F21%2Fnote-20190121-thukc%2F</url>
    <content type="text"><![CDATA[Minute and thoughts of the launch conference of THUKC (清华大学人工智能研究院知识智能研究中心).Mainly about the part of Commonsense-Awared Natural Language Generation (常识知识感知的语言生成, 黄民烈) Commonsense Extraction what is Commonsense Knowledge (CS Know.) what is the Boundary of commonsense Commonsense Extraction From Embedding Extracting Commonsense Properties from Embeddings with Limited Human Guidance. 2018 Commonsense Knowledge base completion Commonsense Knowledge base Completion. 2018 From RAW data(text, image) Automatic Extraction of Commonsense LocatedNear Knowledge. 2018 Commonsense Knowledge in RC 代表工作：Knowledgeable Reader Commonsense Knowledge to Intent, Reaction, Emotion, etc 代表工作： Event2Mind: Commonsense Inference on Events, Intents and Reaction Modeling Naive Psychology of Characters in Simple Commonsense Stories AI2 实验室做了大量这方面的工作，相关调研：link Commonsense to Controllable TG 黄民烈老师组的一些很好的工作（黄老师称这些工作是常识知识感知的语言生成的初步探索）： Commonsense Knowledge Aware Conversation Generation with Graph Attention. IJCAI,2018.(distinguished paper)1 Story Ending Generation with Incremental Encoding Commonsense Knowledge. AAAI,2019. 学习笔记：note_link Three Fundamental problems in current Neural Language Generation Models semantics(real understanding) Consistency(long text generation) Logic(reasonable and making sense) 在评价方面，关于如何评价生成的文本具有逻辑性还是个挑战（相关研究点：文本生成评价指标(BLEU、ROUGE等)的研究） New Architecture: symbolic knowledge + planning + neural computing 发布的重要资源知识智能研究中心：http://ai.tsinghua.edu.cn/kirc/# 中英文跨语言百科知识图谱XLORE — 世界知识 特点： 大规模跨语言百科知识图谱 通过融合维基百科和百度百科，并对百科知识进行结构化和跨语言链接构建而成。 以结构化形式描述客观世界中的概念、实例、属性及其丰富语义关系。 XLORE目前包含约247万概念、44.6万属性/关系、1628万实例和260万跨语言链接。 项目地址：https://xlore.org/ 基于义原的开放语言知识库 OpenHowNet — 语言知识、常识知识 ！HowNet 核心数据首次开源 项目地址：https://openhownet.thunlp.org/ 1.https://juejin.im/post/5b6a9e085188251a8d37136d ↩]]></content>
      <categories>
        <category>MRC-Research</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>research</tag>
        <tag>report</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Natural Language Inference | Analysis and Paper List]]></title>
    <url>%2F2019%2F01%2F16%2Fnli-paper-info%2F</url>
    <content type="text"><![CDATA[NLI IntroductionChallenges 词语匹配的多元性（同义词、一词多义）lexical gap、lexical variant； 短语匹配的结构性、多粒度计算语义； 文本匹配的层次性； 利用不相似部分的信息； 自然语言的基础现象：Variability of Semantic expression 是语言歧义的对偶问题； 语义表达的多变性、语言结构的多样性； Approachsmodels on NLI(Natural Language Infernce), concluded as: sentence-encoding-based vs. interaction-based 基于encoding的基本框架： input encoding —&gt;concat vector —&gt;vector &amp; element-wise difference/product—&gt; prediction 基于交互的基本框架： input encoding —&gt; inference(interaction) —&gt; composition —&gt; prediction Measure of SimilarityModels of Sentence Encoding Based BiLSTM-Max NSE Deep Gated Attn. BiLSTM Residual Stacked Encoder Reinforced Self-Attention Network Distance-based Self-Attention Network Hierarchical BiLSTM with Max Pooling Dynamic Self-Attention Model Models of Interaction Based Decomposable Attention ESIM KIM Densely Interactive Inference Network (DIIN) BIMPM Multi-Way Attention DR-BiLSTM CAFE Densely-Connected Recurrent and Co-Attentive Network DMAN SLRC AFN]]></content>
      <categories>
        <category>NLI</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>paper</tag>
        <tag>research</tag>
        <tag>nli</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WIP| Analysis of FEVER]]></title>
    <url>%2F2019%2F01%2F15%2Fmrc-analysis-fever%2F</url>
    <content type="text"><![CDATA[FEVER Introduction FEVER data: link 包含 185,445 条断言(claims) FEVER shared Task(with NAACL 2018) info: details here FEVER official baseline code: github repo FEVER(Fact Extraction and VERification) 任务中，给定一个未经验证的断言(claim)，即一个句子，要求模型/系统从Wikipedia中找到对应的证据句(evidence)，来验证这个断言，是否可以被证实(SUPPORTED)、反驳(REFUTED)或是没有足够的信息判断(NOT ENOUGH INFO)。对于SUPPORTED和REFUTED的判断，需要给出证据句子。其中 16.82%的例子中，需要多个证据句子来进行判断，12.15%的情况下，证据句来源于多篇文档。 Data Statistics split SUPPORTED REFUTED NEI Train 80035 29775 35639 Dev 6666 6666 6666 Test 6666 6666 6666 Baseline Systembaseline 系统是 pipelined 形式，由三部分组成: 1.document retrieval, 2.sentence-level evidence selection, 3.textual entailment. 其中文本蕴含识别(recognizing textual entailment)部分采用的是 谷歌的 Decomposable Attention 模型 Score Metricsofficial scorer: https://github.com/sheffieldnlp/fever-scorer 判断 claim 的类型(SUPPORTED/REFUTED/NOT ENOUGH INFO)是个三分类问题，对此使用 accuracy 来评价。对于 SUPPORTED 和 REFUTED 的类别，还需要提供证据片段，对此使用 F1 来评价。 FEVER Shared Task Top-3 Systems Solutions The Fact Extraction and VERification (FEVER) Shared Task Top-1: UNC-NLP Combining Fact Extraction and Verification with Neural Semantic Matching NetworksAAAI 2019 Top-2: UCL Machine Reading Group UCL Machine Reading Group: Four Factor Framework For Fact Finding (HexaF) Top-3: Athene UKP TU Darmstadt Multi-Sentence Textual Entailment for Claim Verification Shared Task OverviewFEVER Workshop Notes FEVER workshop info: details here]]></content>
      <categories>
        <category>MRC-Paper-Intro</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>paper</tag>
        <tag>research</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MRC 的评测与所需能力]]></title>
    <url>%2F2019%2F01%2F14%2Fmrc-measure-skills%2F</url>
    <content type="text"><![CDATA[Reading Comprehension Skills An Analysis of Prerequisite Skills for Reading Comprehension.1Prerequisite skills for reading comprehension: Multi-perspective analysis of mctest datasets and systems. AAAI,2017.2Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability. ACL,2017.3 (从数据的角度)将阅读理解所需要的能力分为两类: 认知能力(prerequisite skill) 和 语言能力(readability) 1.prerequisite skills认知能力: measure different types of reasoning and knowledge required to answer the question 定义了 13 种认知能力4, 如下: object tracking 目标跟踪 同时锁定和跟踪多个目标，如集合或个体，也被称为列举或枚举 mathematical reasoning 数学推理 能够完成统计或者量化操作 coreference resolution 指代消解 将指代词映射到相应的实体上 logical reasoning 逻辑推理 逻辑操作，例如对量词、否定、条件以及转移推理 analogy 类比 能够了解一些隐喻，如转喻和提喻 causal relation 因果关系 理解文本中的因果关系 spatiotemporal relation 空间关系 理解空间或者时间上的关系 ellipsis 省略 识别出文章中隐含或者忽略的信息，如参数、谓词、量词、时间等等 bridging 间接引用 能够根据词法或者句法的信息进行推理 elaboration 阐述 能够根据已有事实/常识进行推理 meta-knowledge 元知识 理解读者、作者或者文体信息（如：谁是这个故事的主人公） schematic clause relation 短语关系 理解包含有并列、从句或者关系子句的复杂句子 punctuation 标点符号 理解文章中标点符号代表的意义 备注： 认知能力与RC的一个联系是：当回答一个问题时，需要用到的认知能力越多，该问题越难回答 9和10的区别在于：9利用词项/句法信息还是10通用的常识信息 8到11是对Commonsense reasoning的细致分类 1-11是涉及到多句的，12-13涉及单句 2.readability语言能力: 如何将低层次的文本符号（字、词）组合为高层次的含义的能力measures the text ease of processing and a wide range of linguistic features/human readability measurements are used. 词义辨析：正确理解和区分词的意思。 句法识别：识别出文本的句法信息，使推理过程不受句法 变化的影响。 语义组合：根据句法信息将基本单元（字、词）的语义组 合成高等单元（句子，篇章）的语义。 Metrics/Evaluation如何评价一个example的难度？ 1.https://aclweb.org/anthology/W16-6001 ↩2.http://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/14-Sugawara-14614.pdf ↩3.https://aclanthology.info/papers/P17-1075/p17-1075 ↩4.王炳宁《A_Cognitive_Perspective_of_Machine_Comprehension_And_Recent_Advances》 ↩]]></content>
      <categories>
        <category>MRC-Research</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>research</tag>
        <tag>metric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器阅读理解数据集]]></title>
    <url>%2F2019%2F01%2F12%2Fmrc-dataset-info%2F</url>
    <content type="text"><![CDATA[Contributed by Luxi Xing and Yuqiang Xie. IIE, CAS. IntroductionBased on the style of the ANSWER for each datasets, we split the datasets into several category and we will give details for each datasets. Extractive Multi-Choice Generative Sequential OthersCloze-Style Extractive Dataset Language Domain #Train#Doc #Dev #Test Year Features SQuADv1 English Wikipedia 87599442 1057048 953346 2016 SQuADv2 English Wikipedia 130319442 1187335 886228 2018 have no answer TriviaQA English WebWikipedia 52897961888 686219951 650599509 2017 avg. length is 2895 HotPotQA English Wikipedia 90564 7405 7405 2017 multiple supporting doc. to answer Natural Questions English Wikipedia 307k 8k 8k 2019 whole wikipedia article;long answer Multi-Choice Dataset Language Domain #Train#Doc #Dev #Test Year Features RACE English Multi 8786625137 48871389 49341407 2017 highmiddle MCScript English InScript 97311470 1411219 2797430 2018 ARC English Science 14M/7787 2018 hard OpenBook English 4957 500 500 2018 multi-hop;commonsense;science fact MultiRC English 7 domain 9872871 2018 multi correct answers QAngaroowikihop English 43738 5129 2451 2017 multi evidence pieces;multi options DREAM English 2019 dialogue-based multi-choice GenerativeFree-form answer generation Dataset Language Domain #Train #Dev #Test Year Features MSMARCOv1 English Web 100k 2016 MSMARCOv2 English Web 100k 2018 NarrativeQA English book/moive(wikipedia) 32747 3461 10557 2018 based on summary DuReader Chinese Web 2017 Metric for evaluate the Generative-style QA dataset is: ROUGE-L/BLEU MS MARCO More details about MSMARCO, please reference to this REPO Narrative QA dataset consists of two settings: based on the summary： average 659 tokens based on the full book/movie script: average 62528 tokens Sequential Dataset Language Domain Document Question Size(Train/Dev/Test) Year Features QuAC English Wikipedia 2018 CoQA English Wikipedia 2018 DREAM English Daily life 6444 10197 2019 dialogue-based multi-choice;multi-turn multi-party OthersChinese Datasets People Daily &amp; Children’s Fairy Tale (PD&amp;CFT): https://github.com/ymcui/Chinese-RC-Dataset cloze-style dureader: https://github.com/baidu/DuReader generative cmrc2018: https://github.com/ymcui/cmrc2018 extractive Multi-Documents Datasets TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. SearchQA: A new Q&amp;A dataset augmented with context from a search engine. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. Commonsense Reasoning Winograd Schema Challenge. https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html COPA: Choice of Plausible Alternatives. https://www.cs.york.ac.uk/semeval-2012/task7/index.html ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension CommonsenseQA FEVER: Fact Extraction and VERification FEVER data: link FEVER shared Task(with NAACL 2018) info: details here FEVER official baseline code: github repo my note and analysis of FEVER: fever-note Google Natural Questionsofficial details: https://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.htmlgithub: https://github.com/google-research-datasets/natural-questions 自然问题数据集(NQ)是一个公开的自然发生问题(即由寻求信息的人提出的问题) 用于训练和评估开放领域问答系统的新的、大规模语料库; 也是第一个复制人类查找问题答案的端到端流程的语料库; 专注于通过阅读整个页面来查找答案，而不是从一个短段落中提取答案; 来源于Wikipedia; 人工注释答案: 要求注释者通过通读整个维基百科页面来找到答案，就好像这个问题是他们自己提出的一样。 注释者需要找到一个长答案和一个短答案，长答案涵盖推断问题所需的所有信息，短答案需要用一个或多个实体的名称简洁地回答问题 自然语言理解挑战： NQ的目的是使QA系统能够阅读和理解完整的维基百科文章，其中可能包含问题的答案，也可能不包含问题的答案。 系统首先需要确定这个问题的定义是否足够充分，是否可以回答 许多问题本身基于错误的假设，或者过于模糊，无法简明扼要地回答。 然后，系统需要确定维基百科页面中是否包含推断答案所需的所有信息。 作者认为，相比在知道长答案后在寻找短答案，长答案识别任务(找到推断答案所需的所有信息)需要更深层次的语言理解。 human upper bound: long answer selection task: 87% F1 short answer selection task: 76% F1 dataset statistics: train: 307k dev: 8k test: 8k]]></content>
      <categories>
        <category>MRC-Research</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>paper</tag>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension]]></title>
    <url>%2F2019%2F01%2F12%2Fpaper-emnlp2018-dcu%2F</url>
    <content type="text"><![CDATA[Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading ComprehensionEMNLP 2018Yi Tay et al.多粒度/尺度序列编码; 膨胀组合单元 Motivation Sequence encoder是MRC中的重要部件: helps to model compositionality of words, capturing rich and complex linguistic and syntactic structure in language Sequence encoder的问题: 文档较长，文档数多时计算开销大; 限制获取长距离上下文; 限制了多句和文档内部的推理; 本文工作: 采取组合式编码方式 主要思路是将多个尺度的信息组合在一起进行编码，利用多尺度 n 元语法信息来实现语义融合，得到更好的文档表达，用于后续的推理和Attention操作. 设计了一种 dilated compositions 机制来建模多个尺度之间的关系，相当于通过门控的方式决定要保留多少信息 多尺度 包括：word-level、phrase-level、sentence-level、paragraph-level etc. 一种 divide-and-conquer 的序列编码方式 本文贡献: 提出了一个 compositional encoder DCU (Dilated Compositional Units), DCU 既可以进行独立编码，也可以以RNN-style的方式进行更有表示能力的编码； DCU 可以加速序列编码速度，并且保持相邻词之间的交互关系； 建模长句子时，模型可以获得前方更多的信息，是对所有上下文的全局概览； 相当于一个门控单元对不同粒度的关系进行建模，有利于捕获文档内部细粒度关系； Model DCU用于MRC时的整体结构，左侧为DCU Encoder，右侧为DCU分别用于span prediction model和Multiple choice model的结构: 本节只介绍 DCU 的操作和 encoding 操作，不对整体的mrc模型进行介绍 Dilated Compositional MechanismNotations: Input Sequence: $S=[w_1, w_2, …, w_l]$ Range list: $R={r_1, r_2, …, r_k}$ $k$ 表示进行 $k$ 次 fold/unfold 操作 1.Fold对于每个 $r_j$: 将$S$中的每 $r_j$ 个词进行串接(concat, neural bag-of-words 表示), 原输入长度缩减为 $l/r_j$ 对于新得到的、包含 $l/r_j$ 个 tokens/blocks 的序列中的每个表示进行如下计算: $\bar{w}_t = \sigma_r(W_a(w_t)) + b_a$ $W_a \in \mathbb{R}^{d\times d}, b\in \mathbb{R}^d$ Fold 的操作次数等于 range list 的大小 对于 range list 中不同的 $r$ 值, 参数 $W_a$ 和 $b_a$ 不共享 2.Unfold将transformed之后的序列展开为原长 下图中为 $r_j=2$ 时的 Fold-Unfold 操作: 3.Multi-Granular Reasoning多尺度推理 将不同尺度的unfold之后的token表示进行串接，然后通过两层前馈神经网络得到一个门向量 $g_t = F_2(F_1([w_1^t,w_2^t,…,w_t^k]))$ $F(\cdot) = ReLU(Wx+b)$ $g_t$ 相当于一个从多尺度中学习的门控向量，尺度值最低的那些词会拥有相同的 $g_t$ 值 Encoding Operation1.Simple Encoding $z_t = tanh(W_p w_t) + b_p$ $y_t = \sigma(g_t) \ast w_t + (1-\sigma(g_t))z_t$ 2.Recurrent EncodingDCU 相当于循环神经网络中的 cell $c_t = g_t \odot c_{t-1} + (1-g_t)\odot z_t$ $o_t = W_o(w_t) + b_o$ $h_t = o_t \odot c_t$ 问题：此处有一点疑问是，为什么通过DCU得到的门控向量 $g_t$ 没有参与到后续的编码过程, 而只是作为了控制初始 $w_t$ 词向量的门控输入 ExperimentImplementation Details Multi-Choice 模型: 数据输入: include the standard EM (exact match) binary feature to each word. In this case, we use a three-way EM adaptation, i.e., EM(P, Q), EM(Q, A) and EM(P, A). The projected embeddings are then passed into a single layered highway network 输出(答案选择层): 将每个候选答案的答案向量转化为标量 $a_j^f=softmax(W_2(\sigma_r(W_1([a_j])+b_1)+b_2))$ range valuse: ${1,2,4,10,25}$ 最大序列长度 (RACE/SearchQA/NarrativeQA): $500/200/1100$ Results只关注了一下RACE和NarrativeQA上的结果，在没有使用RNN-based编码器的情况下，取得了不错的效果，在论文完成时(2018.03)是RACE上的top-1 RACE NarrativeQA Summary &amp; Analysis 对于长文本的编码问题是MRC中的重点问题之一【—&gt;表示问题】 长文本和多篇文本 本文提供了一种跨尺度的交互，或是融合跨尺度的信息的有效方式]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>note</tag>
        <tag>reasoning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WIP| On the Capabilities and Limitations of Reasoning for Natural Language Understanding]]></title>
    <url>%2F2019%2F01%2F11%2Fpaper-1901-02522%2F</url>
    <content type="text"><![CDATA[[Under reading] On the Capabilities and Limitations of Reasoning for Natural Language UnderstandingKhashabi et al.作者来自 University of Pennsylvania, Indiana University 和 AllennAI题目: 论自然语言理解推理的能力与局限 Abstract 一些NLU系统在克服查找 Style Reasoning 的语言可变性(linguistic variability)方面具有很强的实力, 但是他们的准确率会随着推理步数的增加而下降. key: style reasoning; linguistic variability; reasoning step; 本文基于上述观察第一次提出了一个正式框架, 旨在解决使用语言表示隐藏概念空间时引入的: 1.模糊性(ambiguity) 2.冗余性(redundancy) 3.不完整性(incompleteness) 4.不准确性(inaccuracy) 模型使用了两个相互关联的(interrelated)空间: conceptual meaning space: unambiguous and complete but hidden. linguistic symbol space: captures a noisy grounding of the meaning space in the symbols or words of a language. 本文引用此框架来研究无向图中的连通性(connectivity)问题: 是构成更复杂的多步推理的基础核心推理问题 证明了构建高质量算法来检测 latent meaning graph 中的连通性是可能的, 前提: 基于一个可观察的 noisy symbol graph, 并且这些噪声低于我们定量的噪声等级. 此外, 证明了一个不可能的结果: 如果一个query需要大量的推理步数, no reasoning system operating over the symbol graph is likely to recover any useful property of the meaning graph. 这一点同时强调了对于推理问题和系统，需要的是限制两个空间的距离，而不是投入更多的推理步数(hops) Introduction Reasoning的定义: the process of combining facts(事实) and beliefs(信念), in order to make decisions.]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>mrc</tag>
        <tag>nlu</tag>
        <tag>reasoning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Text Summarization - Main Problems]]></title>
    <url>%2F2019%2F01%2F09%2Fnlp-challenge-ts%2F</url>
    <content type="text"><![CDATA[文本摘要中的主要问题与挑战 问题：区分是挑战还是技术问题 1.关注重点的词、句； Attention、pos、tf-idf intra-temporal attention encoder-less self-attention 【generating Wikipedia by Summarizing long Sequences】 2.不准确的复制\生成事实细节； 不准确的复制\生成事实细节； copy 3.重复性短语、句子 重复性短语、句子； temporal Attention coverage intra-attention 4.处理长文档 处理长文档； Sentence-level Attention selective gate self-attention：缓解长距离依赖 5.生成可读性好的摘要 生成可读性好的摘要； RL 6.生成新的词（基于理解的基础上）； 生成新的词（基于理解的基础上）； 7.词汇问题 罕见词（rare but important）、未登录词； add n-gram match term to loss【A neural Attention model for Sentence Summarization】 pointer 使用大规模词典；candidate sampling 关注要解决的问题： 如何在生成过程中使decoder的注意力更集中，使Attention更聚焦，由于输入序列的长度 比较长？即便使用Attention模型，也不能很好的聚焦到对应的源端token（loss focus）； encoder的输出在用于Attention计算时包含噪声 使重点更突出；而不是过滤？区别？ copy 机制的贡献程度？ 以及coverage 如何判定信息冗余与信息丢失]]></content>
      <categories>
        <category>Text Summarization</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>research</tag>
        <tag>challenge</tag>
        <tag>ts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Machine Translation - Main Problems]]></title>
    <url>%2F2019%2F01%2F09%2Fnlp-challenge-nmt%2F</url>
    <content type="text"><![CDATA[神经机器翻译中的主要问题与挑战 1.难以跨领域神经机器翻译（NMT）在处理领域之外的数据时的表现很糟：当前的机器翻译系统会生成非常流畅的输出，这些输出与领域外数据的输入无关。因此像Google翻译这样的通用机器翻译系统在法律或金融等专业领域的表现尤其糟糕。与基于短语的系统等传统方法相比，NMT系统的效果更差。 2.小数据集上表现不佳NMT在小数据集上表现不佳：一般而言，大多数机器学习都是这样，但这个问题在NMT上尤为突出。 NMT的优点在于，随着数据量的增加，它的表现要（比基于短语的机器翻译）更好，但在数据量很低的情况下，NMT的表现确实更差。事实上，正如作者所说，“在资源条件较差的情况下，NMT会产生与输入内容无关的流畅输出。”这可能是Motherboard的文章探讨的一些关于NMT表现奇怪的另一个原因。 3.罕见词汇NMT在罕见词汇上的表现不佳：尽管比基于短语的翻译的表现更好，但NMT对于罕见或未见过的词语翻译的表现不佳。对于存在大量变形词的语言及大量命名实体的领域，这可能成为一个问题，因为变形词和命名实体一般非常罕见。 4.长句翻译长句的翻译问题：对长句编码及生成长句仍然是一个没有解决的问题。 机器翻译系统随句子长度的增加，其表现会越来越糟，NMT系统尤其如此。使用注意力有帮助，但问题远未“解决”。在许多领域，如法律领域，冗长复杂的句子是很常见的。 5.注意力机制不等于简单对齐注意力（Attention）机制不等于简单对齐：这是一个非常微妙但重要的问题。在传统的SMT系统（如基于短语的MT）中，对齐翻译为模型的检测提供了有用的调试信息。但是注意机制不能被视为传统意义上的对齐，即使论文经常将注意力机制作为“软对齐”引起注意。在NMT系统中，除了源语言中的动词之外，目标语言中的动词也可以作为主语和宾语成分。 6.翻译质量 难以控制翻译质量：每个单词都有多种翻译，典型的机器翻译系统在源句的翻译结构上表现很好。为了保持句子结构的大小合理，会使用集束搜索（beam search）。通过改变集束宽度，可以找到低概率但正确的平移。而对于NMT系统，调整集束的宽度似乎没有任何影响，甚至可能会有不良影响。 翻译的忠实度（adequacy）与流畅度（fluency） 信达雅 参考 参考链接：http://deliprao.com/archives/301 论文地址：http://www.aclweb.org/anthology/W/W17/W17-3204.pdf]]></content>
      <categories>
        <category>NMT</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>research</tag>
        <tag>challenge</tag>
        <tag>nmt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年讨论组日程记录]]></title>
    <url>%2F2019%2F01%2F09%2Fschedule-2019%2F</url>
    <content type="text"><![CDATA[2019年讨论组论文报告会 table th:nth-of-type(1){ width: 90px; } table th:nth-of-type(2){ width: 90px; } table th:nth-of-type(3){ width: 50%; } table th:nth-of-type(4){ width: 100px; } Date Reporter Title/Papers Appx. 01.13 邢璐茜 Multi-Granular Sequence Encoding via Dilated Compositional Units for RC EMNLP2018note link 01.13 孙雅静 The Design and Implementation of XiaoIce, an Empathetic Social Chatbot link 01.13 谢玉强 Analysis on MS MARCOv1 Leaderboard Top 3:1.S-Net2.V-Net3.MARSMulti-task Learning details-1details-2 01.18 雷扬帆 Self-Supervised Learning Introduction 01.18 魏相鹏 Meta-Learning Introduction blog 01.18 邢璐茜 Meta-Learning a Dynamic Language Model ICLR workshop 01.18 孙雅静 Continual Leanring Introduction 01.24 孙雅静 1. A Dual-Attention Hierarchical RNN for Dialogue Act Classification2.LifeLong Learning with Dynamically Expandable Networks 01.24 谢玉强 A Survey to Self-Supervised Learning 01.24 雷扬帆 Self-Supervised LearningUnsupervised Learning by Cross-Channel Prediction 01.24 彭伟 Attention-over-Attention NN for RC ACL,2017 02.20 邢璐茜 Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability ACL,2017 02.20 谢玉强 Multi-Style Generative Reading Comprehension 02.20 彭伟 Match-LSTM 02.20 孙雅静 个性化对话:1.Persona-Chat对话数据集2.Personalizing a Dialogue Systems with Transfer Reinforcement Learning 02.20 于静 Lifelong Learning Cross Media Search ACM-MM 03.01 邢璐茜 TG with Commonsense tg-with-cs-note 03.01 谢玉强 Incorporating Structured Commonsense Knowledge in Story Completion AAAI,2019 03.01 孙雅静 What makes a good conversation? How controllable attributes affect human judgments 03.08 孙雅静 Learning Personalized End-to-End Goal-Oriented Dialog AAAI,2019 03.08 于静 1.Compositional Attention Networks for Machine Reasoning2.Visual Dialog ICLR,2018CVPR,2017 03.08 邢璐茜 Commonsense for Generative Multi-Hop Question Answering Tasks details 03.09 魏相鹏 1.Cross-Lingual Language Model Pretraining2.An Effective Approach to Unsupervised Machine Translation 03.09 谢玉强 Self-supervised learning in video and multi-modal 03.09 彭伟 Dynamic Coattention Networks For Question Answering ICLR,2017 03.15 谢玉强 Improving Machine Reading Comprehension with General Reading Strategies 03.15 邢璐茜 Improving Question Answering with External Knowledge 03.15 孙雅静 Learning to Select Knowledge for Response Generation in Dialog System 03.22 邢璐茜 Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering ACL, 2018 03.22 谢玉强 Tackling the Story Encding Biases in The Story Cloze Test ACL,2018 03.22 于静 文本匹配模型介绍 03.29 邢璐茜 Multi-Choice Machine Reading Comprehension Task - PartI details 03.29 孙雅静 Sentence embedding Alignment for LifeLong Relation Extraction NAACL,2019 04.12 谢玉强 Does it care what you asked? Understanding Importance of Verbs in QA Model EMNLP,2018 workshop 04.12 孙雅静 1.Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems2.Global-to-Local Memory Pointer Networks for Task-Oriented Dialogue ACL,2018ICLR,2019 04.12 李云鹏 Relation Extraction Survey - 1 04.19 邢璐茜 Commonsense Reasoning for Natural Language Understanding detail 05.24 彭伟 1.A Deep Cascade Model for Multi-Document Reading Comprehension2.Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering 05.24 魏相鹏 MASS: Masked Sequence to Sequence Pre-training for Language Generation ICML,2019 05.31 邢璐茜 HotpotQA at ACL 2019 details 05.31 孙雅静 Challenges in Building Intelligent Open-domain Dialog Systems 2019 往期内容列表 2017年讨论组内容列表 2018年讨论组内容列表]]></content>
      <categories>
        <category>Group-Discussion</category>
      </categories>
      <tags>
        <tag>schedule</tag>
        <tag>group</tag>
        <tag>discussion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KIM]]></title>
    <url>%2F2019%2F01%2F09%2Fpaper-kim%2F</url>
    <content type="text"><![CDATA[Neural Natural Language Inference Models Enhanced with External KnowledgeACL,2018.Qian Chen, et al.offical code link: here. Motivation作者针对NLI任务提出了一个问题： 是否可以从数据中学习NLI所需要的所有知识？ 如果不能，如何使外部知识帮助神经网络模型？如何构建NLI模型利用外部知识？ 本文的工作是作者基于其ESIM模型之上完成的 Model Framework 模型整体结构分为四部分: 1、representing 2、collecting local inference information 3、aggregating and composing local information 4、make global decision at sentence level 其中，将外部知识加入到了: co-attention、local inference collection、inference composition 模块中 External Knowledge首先确定需要哪些知识来帮助NLI任务 external, inference-related knowledge intuitively Knowledge: synonymy(同义)、antonymy(反义)、hypernymy(上位)、hyponymy(下位) 上下位的关系可以帮助捕捉 entailment 信息 反义和co-hyponymy(共享相同上位词)的关系可以更好的建模 contradiction 关系 这篇文章主要是引入了基础的词汇级的语义知识 建模词$w_i$和$w_j$知识为 $r_{ij}$ 重点是：如何构建 $r_ij$ M1.Encoding Premise and Hypothesis premise: $a=(a_1,…,a_m)$ hypothesis: $b=(b_1,…,b_n)$ 通过BiLSTM进行编码，得到context-dependent 隐藏状态： $a_i^s = Encoder(E(a),i)$ $b_j^s = Encoder(E(b),j)$ M2.Knowledge-Enriched Co-Attention 知识关系特征：$r_{ij} \in \mathbb{R}^{d_r}$ Co-attention: $e_{ij} = (a^s_i)^T b_j^s + F(r_{ij})$ $F(r_{ij}) = \lambda \mathbb{1}(r_{ij})$ $\mathbb{1}(r_{ij})$ 判断 $r_{ij}$ 是否为0向量 得到的 co-attention 矩阵 $e \in \mathbb{R}^{m\times n}$ 根据co-attention对premise和hypothesis的表示进行更新： a_i^c=\sum_{j=1}^n \alpha _{ij} b_j^s \alpha_{ij} = exp(e_{ij}) / \sum_{k=1}^n exp(e_{ik}) b_j^c=\sum_{i=1}^m \beta _{ij} a_i^s \beta_{ij} = exp(e_{ij}) / \sum_{k=1}^m exp(e_{kj}) $\alpha \in \mathbb{R}^{m\times n}$、$\beta \in \mathbb{R}^{m\times n}$ M3.Local inference Collection with External Knowledge 通过比较$a_i^s$、$a_i^c$ 和 他们的关系（从外部知识获得），可以获得词级的推理信息 Knowledge-enriched local inference: a_i^m = G([a_i^s; a_i^c; a_i^s - a_i^c; a_i^s \circ a_i^c; \sum_{j=1}^n \alpha_{ij}r_{ij}]) b_j^m = G([b_j^s; b_j^c; b_j^s - b_j^c; b_j^s \circ b_j^c; \sum_{i=1}^m \beta_{ij}r_{ji}]) 最后一项的目的是：收集对齐词之间的关系特征，是从外部知识获得的word-level inference information $G$ 是非线性映射，用于降维，relu + shortcut connection a_i^m + \sum_{j=1}^n \alpha_{ij}r_{ij} b_j^m + \sum_{i=1}^m \beta_{ij}r_{ij} M4.Knowledge-Enhanced Inference Composition 决定总体的推理关系：BiLSTM —&gt; mean;max;weighted Pooling，得到定长向量 Composition = BiLSTM $a_i^v = Composition(a^m,i)$ $b_j^v = Composition(b^m,j)$ Weighted Pooling a^w = \sum_{i=1}^m \left( \frac{exp(H(\sum_{j=1}^n \alpha_{ij}r_{ij}))}{\sum_{i=1}^m exp(H(\sum_{j=1}^n\alpha_{ij}r_{ij}))} \right) a_i^v b^w = \sum_{j=1}^n \left( \frac{exp(H(\sum_{i=1}^m \beta_{ij}r_{ji}))}{\sum_{j=1}^n exp(H(\sum_{i=1}^m\beta_{ij}r_{ji}))} \right) b_j^v $H$ 函数是 1层的前馈神经网络，激活函数是relu 最后得到定长向量之后，再过一层MLP，激活函数为tanh，和一层 softmax，得到分类结果 Experiment SettingsRepresentation of External KnowledgeLexical Semantic Relations relations of lexical pairs: 检索范围是wordnet synonymy: 如果词对是同义词，使值为1，否则为0 antonymy: 如果词对是反义词，使值为1，否则为0 hypernymy: 如果一个词是另一个词的上位词，取值 $1-\frac{n}{8}$，否则为0 其中 $n$ 是两个词在层次上的边数 忽略边数超过8的 hyponymy: inverse of the hypernymy feature co-hyponymys: 如果两个词有相同的上位词且不是同义词，取值为1，否则为0 向量 $r$ 的维度为 $d_r = 5$ 在 wordnet 中还有额外的一些（15种）关系，但是对结果的提升没有贡献 relation embeddings pretrain based on wordnet TransE]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>note</tag>
        <tag>nli</tag>
        <tag>wordnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Knowledgeable Reader Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge]]></title>
    <url>%2F2019%2F01%2F09%2Fpaper-knreader%2F</url>
    <content type="text"><![CDATA[Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge ACL,2018. Todor Mihaylov, et al. MotivationModel Framework]]></content>
      <categories>
        <category>Paper-Note</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>mrc</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Knowledge-based NLU-Papers]]></title>
    <url>%2F2019%2F01%2F09%2Fkmrc-paper-info%2F</url>
    <content type="text"><![CDATA[A list of recent papers on knowledge-based Natural Language Understand. Contributed by Luxi Xing and Yuqiang Xie, National Engineering Laboratory for Information Security Technologies, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China. table th:nth-of-type(2){ width: 60%; } NLI with Knowledge Conf. Title Authors/Org. Note ACL2018 Neural Natural Language Inference Models Enhanced with External Knowledge Qian Chen KIM-note 2018 Improving Natural Language Inference Using External Knowledge in the Science Questions Domain Xiaoyan Wang MRC with Knowledge Conf. Title Authors/Org. Note ACL2017 Leveraging knowledge bases in lstms for improving machine reading Yang, et al.CMU ACL2017 World knowledge for reading comprehension: Rare entity prediction with hierarchical lstms using external descriptions Long, et al.McGill University ACL2018 Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge Mihaylov, et al.Heidelberg University knreader-note EMNLP2018 Commonsense for Generative Multi-Hop Question Answering Tasks Lisa Bauer mhpgm-note Dialog with Knowledge Conf. Title Authors/Org. Note ACL2017 Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings He, et al.Stanford IJCAI18 Commonsense Knowledge Aware Conversation Generation with Graph Attention Zhou, et al.THU note AAAI2018 A Knowledge-Grounded Neural Conversation Model Ghazvininejad, et al.Microsoft Research AAAI2018 Flexible End-to-End Dialogue System for Knowledge Grounded Conversation Zhu, et al.HKUST 2019 Learning to Select Knowledge for Response Generation in Dialog Systems Baidu Text Generation with Knowledge Conf. Title Authors/Org. Note AAAI2019 Story Ending Generation with Incremental Encoding and Commonsense Knowledge Jian Guan, et al.THU note Representation with Knowledge Conf. Title Authors/Org. Note ICLR2017 A Neural Knowledge Language Model Ahn.et al.Université de Montréal ACL2017 Improved Word Representation Learning with Sememes Niu, et al.THU ACL2018 Cross-lingual Lexical Sememe Prediction Qi, et al.THU AAAI2018 Improving Neural Fine-Grained Entity Typing with Knowledge Attention Xin, et al.THU ACL2019 Modeling Semantic Compositionality with Sememe Knowledge Qi, et al.THU note 2019 SenseBERT: Driving Some Sense into BERT AI21 Labs note Level: KMRC area. Relevant research area. Heuristic. Review. Field: KMRC: Knowledge-based Machine Reading Comprehension; KDS: Knowledge-based Dialogue System; KIR: Knowledge-based Information Retrieval; SC: Sememe Computation; NKLM: Neural Knowledge Language Model.]]></content>
      <categories>
        <category>MRC-Paper-Intro</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>knowledge</tag>
        <tag>paper</tag>
        <tag>text-generation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年讨论组日程表]]></title>
    <url>%2F2019%2F01%2F09%2Fschedule-2018-md%2F</url>
    <content type="text"><![CDATA[2018年讨论组论文报告会 /* 第一列表格宽度 */ table th:nth-of-type(1){ width: 90px; } /* 第二列表格宽度 */ table th:nth-of-type(2){ width: 90px; } /* 第三列表格宽度 */ table th:nth-of-type(3){ width: 50%; } /* 第四列表格宽度 */ table th:nth-of-type(4){ width: 100px; } Date Reporter Title/Papers Confere. 01.13 邢璐茜 Deliberation Networks: Sequence Generation Beyond One-Pass Decoding NIPS,2017 01.13 谢玉强 词法分析调研 01.13 孙雅静 句法分析调研 01.13 彭 伟 The Colorful World in the Neural Network 03.17 邢璐茜 SIX Challenges In the Text Summarization 03.17 魏相鹏 深度学习与文本分类 03.17 孙雅静 Attention is all you need NIPS,2017 03.17 谢玉强 Neural Word Segmentation Learning for Chinese 03.17 彭 伟 Neural Network + CRF 04.15 邢璐茜 NLG survey(partly) 04.15 魏相鹏 NLG survey(partly) 04.15 雷扬帆 开题报告 04.15 谢玉强 Neural Word Segmentation Learning for Chinese 04.15 孙雅静 1. Non autoregressive neural machine translation 2. Deterministic Non autoregressive neural sequence modeling by iterative refinement ICLR,2018 arXiv,2018 04.15 彭 伟 毕设报告 + LSTM-CRF 05.12 邢璐茜 Inter-Weighted Alignment Network for Sentence Pair Modeling EMNLP,2017 05.12 魏相鹏 知识库问答系统-综述 05.12 雷扬帆 Low Resource NMT 开题预讲 05.12 孙雅静 Recursive Neural Network Architecture 05.12 谢玉强 Inner Attention based Recurrent Neural Networks for Answer Selection ACL,2016 06.09 邢璐茜 MRC 综述(Part I) 06.09 魏相鹏 Deep contextualized word representations NAACL,2018best paper 06.09 雷扬帆 Learning Visually Grounded Sentence Representations NAACL-HLT,2018 06.09 孙雅静 Generative Adversarial Network 06.09 谢玉强 Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference EMNLP,2018 06.30 邢璐茜 MRC 综述(Part II) 06.30 魏相鹏 Deep Reinforcement Learning for Sequence to Sequence Models arXiv,2018 06.30 雷扬帆 Bilateral Multi-Perspective Matching for Natural Language Sentences IJCAI,2017 06.30 孙雅静 Transformation networks for target-oriented sentiment classification ACL,2018 06.30 谢玉强 Factorization Machines ICDM,2010 08.03 魏相鹏 1. A Simple and Effective Approach to Coverage-Aware NMT2. Bag-of-Words as Traget for NMT ACL,2018short paper 08.03 孙雅静 对话系统综述 08.03 谢玉强 Syntax-aware Entity Embedding for Neural Relation Extraction AAAI,2018 08.03 彭 伟 语言表示 08.12 邢璐茜 MRC 综述(Part III) R-NetFastQA 08.12 雷扬帆 Multiway Attention Networks for Modeling Sentence Pairs IJCAI,2018 08.12 孙雅静 对话系统综述-safe response 08.12 谢玉强 MRC 模型介绍 08.12 彭伟 Learned in Translation: Contextualized Word Vectors NIPS,2017 08.20 邢璐茜 MRC: J-Net Exploring Question understanding &amp; Adaptation in NN-based QA 08.20 魏相鹏 VAE与NMT1. Variational Neural Machine Translation EMNLP,2016 08.20 孙雅静 对话系统: Safe response1. Neural Response Generation via GAN with an Approxiamte Embedding Layer2. Generating Informative Responses with Controlled Sentence Function ACL,2018 09.04 邢璐茜 MRC: Adversarial Evaluation &amp; Unanswerable Question for SQuAD 09.04 魏相鹏 鲁棒性神经机器翻译 09.04 雷扬帆 文本匹配 09.04 孙雅静 对话: 对话一致性 1. A Persona-Based Neural Conversation Model2. Learning Symmetric collaborateve Dialogue with Dynamic Knowledge Graph Embedding3.CoQA: A Conversational Question Answering Challenge arXiv,2016 ACL,2017 arXiv,2018 09.14 邢璐茜 MRC: Co-Match for Multi-Choice Reading Comprehension ACL,2018 09.14 魏相鹏 文档级神经机器翻译1. Document-Level Neural Machine Translation with Hierachical Attention Networks EMNLP,2018 09.14 谢玉强 Match-LSTM ICLR,2017 09.14 孙雅静 文本改写1. Generating Sentences by Editing Prototypes TACL,2018 09.22 邢璐茜 CopyNet1. Pointer Network2. Incorporating Copying mechanism in Sequence-to-Sequence3. Get to The Point: Summarization with Pointer-Generator Networks NIPS,2015 ACL,2016arXiv,2017 09.22 魏相鹏 Dynamic Sentence Sampling for Efficient Training of NMT ACL,2018 09.22 雷扬帆 Modeling Sentence Tutorial 09.22 谢玉强 Two Methods for Training Deeper Networks1. Highway Network2. Residual Network ICML,2015CVPR,2016 09.22 孙雅静 GAN回顾 09.22 彭伟 Neural Machine Translation by Jointly Learning to Align and Translate ICLR,2015 10.20 邢璐茜 The Pre-Training Language Model1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding arXiv,2018 10.20 魏相鹏 Machine Translation with Weakly Paired Bilingual Documents ICLR,2019.OpenReview 10.20 谢玉强 Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information AAAI,2019 10.20 孙雅静 Reading Wikipedia to Answer Open-domain Questions ACL,2017 10.20 彭 伟 Attention is all you need NIPS,2017 11.17 邢璐茜 MRC: Multi-Step Reasoning &amp; Open-Domain QA 1. Stochastic Answer Networks for Machine Reading Comprehension2. Denoising Distantly Supervised Open-Domain Question Answering ACL,2018ACL,2018 11.17 孙雅静 1. Sequential Matching Network：A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots2. FlowQA: Grasping Flow in History for Conversational Machine Comprehension ACL,2017ICLR,2019.OR 11.17 彭 伟 文本分类1. Recurrent convolutional neural networks for text classification 2. Hierarchical Attention Networks for Document Classification AAAI,2015NAACL,2016 12.09 邢璐茜 MRC: external knowledgeKnowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge ACL,2018 12.09 魏相鹏 Understanding Back-Translation at Scale EMNLP,2018 12.09 彭 伟 End-to-End Memory Networks NIPS,2015 相关内容列表 2017年讨论组内容列表 2019年讨论组内容列表]]></content>
      <categories>
        <category>Group-Discussion</category>
      </categories>
      <tags>
        <tag>schedule</tag>
        <tag>group</tag>
        <tag>discussion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017年讨论组日程表]]></title>
    <url>%2F2019%2F01%2F09%2Fschedule-2017-md%2F</url>
    <content type="text"><![CDATA[2017年讨论组论文报告会 /* 第一列表格宽度 */ table th:nth-of-type(1){ width: 90px; } /* 第二列表格宽度 */ table th:nth-of-type(2){ width: 90px; } /* 第三列表格宽度 */ table th:nth-of-type(3){ width: 50%; } /* 第四列表格宽度 */ table th:nth-of-type(4){ width: 100px; } 日期 报告人 报告主题 简介 03.25 邢璐茜 Generative Adversarial Nets link 03.25 魏相鹏 Neural Architectures for Named Entity Recognition link 03.25 雷扬帆 使用字符级解码器的机器翻译 04.15 邢璐茜 CSGAN for Machine Translation link 04.15 魏相鹏 Hybrid Word-Character Models for NMT link 04.15 雷扬帆 Character-based Neural Machine Translation 05.06 邢璐茜 GAN4NLP 05.06 魏相鹏 Neural Machine Translation with Reconstruction link 05.06 雷扬帆 Minimum Risk Training &amp; Modeling Coverage link1link2 05.27 邢璐茜 Incorporating Copying Mechanism in Sequence-to-Sequence Learning link 05.27 魏相鹏 Convolutional Sequence to Sequence Learning link 05.27 雷扬帆 多语神经机器翻译 06.17 邢璐茜 Context Gates for Neural Machine Translation link 06.17 魏相鹏 Massive Exploration of Neural Machine Translation Architectures link 06.17 雷扬帆 String-based Translation link1link2link3 07.22 邢璐茜 Knowledge-Based Semantic Embedding for Machine Translation link 07.22 雷扬帆 What do Neural Machine Translation Models Learn about Morphology?Visualizing and understanding neural machine translation link1link2 07.22 魏相鹏 Dual Supervised Learning link 08.09 邢璐茜 Pointer Network &amp; It’s application in ACL 16\17 link 08.16 雷扬帆 Interactive Attention for Neural Machine Translation Neural Machine Translation with Supervised Attention link1link2 09.23 邢璐茜 Plan, Attend, Generate: Char-NMT with Planning link 09.23 魏相鹏 AI Challenger 09.23 谢玉强 RNN 09.23 孙雅静 Neural Machine Translation with Word Predictions（EMNLP,2017） link 10.14 邢璐茜 CWMT2017 Review 10.14 魏相鹏 Experiments 10.14 谢玉强 A Character-Aware Encoder for Neural Machine Translation（COLING,2016） link 10.14 孙雅静 Sequence-to-Dependency Neural Machine Translation（EMNLP,2017） link 11.04 邢璐茜 Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems (EMNLP,2015) link 11.04 魏相鹏 Topic Aware Neural Response Generation (AAAI,2017) link 11.04 雷扬帆 Graph Convolutional Encoders for Syntax-aware Neural Machine Translation link 11.04 谢玉强 Agreement on Target-Bidirectional LSTMs for Sequence-to-Sequence Learning (AAAI,2016) Agreement on Target-Bidirectional Neural Machine Translation（NAACL-HLT,2016） link1 link2 11.04 孙雅静 Gated-Attention Readers for Text Comprehension link 11.25 邢璐茜 Unsupervised Machine Translation Using Monolingual Corpora Only link 11.25 魏相鹏 Unsupervised Neural Machine Translation link 11.25 孙雅静 Memory Augmented Neural Machine Translation EMNLP,2017link 11.25 谢玉强 Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation ACL,2017 link 12.16 魏相鹏 Decoding with Value Networks for Neural Machine Translation NIPS,2017 link 12.16 孙雅静 Learning to Remember Translation History with Continuous Cache TACL,2018 link 12.16 谢玉强 Adversarial Multi-task Learning for Text Classification ACL,2017 link 01.13 邢璐茜 Deliberation Networks: Sequence Generation Beyond One-Pass Decoding NIPS,2017 link 01.13 谢玉强 词法分析调研 01.13 孙雅静 句法分析调研 01.13 彭 伟 The Colorful World in the Neural Network 相关内容列表 2018年讨论组内容列表 2019年讨论组内容列表]]></content>
      <categories>
        <category>Group-Discussion</category>
      </categories>
      <tags>
        <tag>schedule</tag>
        <tag>group</tag>
        <tag>discussion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F01%2F07%2Fhello-world%2F</url>
    <content type="text"><![CDATA[This is a post to record some solutions in blog with hexo Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 美化方案参考：https://jerry011235.github.io/2015/05/06/Hexo%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BC%98%E5%8C%96%EF%BC%88%E4%BA%8C%EF%BC%89/ 用css控制Markdown表格列宽参考: http://blog.echoxu.cn/2018-05-15-Hexo中用CSS控制Markdown各列表格宽度.html 编辑post样式参考: 添加文章书写样 添加站内文章链接 \{\% post_link post_name_in_source_posts link_show_title \%} [title](/year/month/day/name.md#section) 字体颜色 可以直接在Markdown 文档编辑中使用html语法 &lt;font size=4 &gt; 这里输入文字，自定义大小 &lt;/font&gt; &lt;font color=&quot;#FF0000&quot;&gt; 这里输入文字，自定义颜色的字体 &lt;/font&gt; 插入图片 图片大小控制： &lt;div style=&quot;width: 200px; margin: auto&quot;&gt;![image-caption](image-link)&lt;/div&gt; 在Hexo中渲染MathJax数学公式 https://www.jianshu.com/p/7ab21c7f0674 bugs fatal: multiple stage entries for merged file &#39;lib/pace&#39; cd .deploy_git rm .git/index git add -A git commit -m &quot;&quot; 迁移 重装hexo、配置依赖 Git clone next主题 复制 站点配置 文件和 主题文件夹 复制 source 文件夹]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
</search>

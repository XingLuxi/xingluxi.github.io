<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>2019 | Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering</title>
      <link href="/2019/09/17/paper-csqa-1909-05311/"/>
      <url>/2019/09/17/paper-csqa-1909-05311/</url>
      
        <content type="html"><![CDATA[<blockquote><p><em>Title</em>: <a href="https://arxiv.org/abs/1909.05311" target="_blank" rel="noopener">Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering</a><br><em>Author</em>: Shangwen Lv, Daya Guo , Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Songlin Hu<br><em>Org.</em>: CAS, Sun Yat-sen University, PKU, Microsoft<br><em>Published</em>: NULL<br><em>Code</em>: NULL</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>常识问答任务(Commonsense Question Answering, CSQA) 要求回答需要背景知识的，且背景知识未在问题中提及的一些问题</p><p>CSQA 的主要挑战是：<font color="blue"><strong>如何从外部的知识中抽取证据信息，并给予证据做出预测</strong></font>。</p><p>近期的工作主要集中在两个方面：</p><ol><li>根据带有人工标注出证据的数据集，学习生成证据 <sup><a href="#fn_cose" id="reffn_cose">cose</a></sup>.<ul><li>问题是：标注代价昂贵</li></ul></li><li>仅从结构化知识库或是非结构化知识库，即同构知识源，中抽取证据信息，并没有<strong>同时利用</strong>不同来源的知识<ul><li><strong>为什么要同时利用结构化和非结构化的知识库?</strong><ul><li><strong>结构化</strong>知识 (Structured Knowledge Source): 包含大量的三元组信息（概念 及其之间的关系），利于推理，但是存在覆盖度低的问题</li><li><strong>非结构化</strong>知识 (Unstructured Knowledge Source): 即 Plain-Text，包含大量冗余的、覆盖范围广的信息，可以辅助/补充结构化知识</li></ul></li></ul></li></ol><p>融合异构知识源必要性的例证：<img src="/../images/paper-csqa-1909-05311/example.png" alt="example"></p><ul><li>根据结构化知识库ConceptNet，可以挑选出候选 A 和 C</li><li>根据Wikipedia文本，可以挑选出候选 C 和 E</li><li>结合两类来源的证据，即可得到最终的正确答案 C</li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><p>本文针对的数据集是：<strong>CommonsenseQA</strong></p><p>本文的主要工作：</p><ul><li>自动地从<strong>异构知识源</strong>中抽取证据，即同时从ConceptNet和Wikipedia文章中抽取知识<ul><li>为每个知识源构建图，来获取证据间的关系结构</li></ul></li><li>提出了一个基于图的模型，由两个模块组成:<ul><li>基于图的上下文词表示学习模块<ul><li>为每个知识源都构建图结构，CN中使用自身的三元组，Wiki中通过语义角色标注来抽取出句子的三元组（谓词，论元）</li><li>utilize graph structural information to re-define the distance between words for learning better contextual word contextual word representations<ul><li>(利用图结构化信息来 重新定义词之间的距离以学习更好的上下文词表示)</li></ul></li></ul></li><li>基于图的推理模块<ul><li>使用GCN编码图信息</li><li>使用图注意力机制聚集证据信息</li></ul></li></ul></li><li>在CommonsenseQA上取得了当前最好的效果</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>CommonsenseQA 的任务形式：<br>输入：问题 $Q = {q_1, …, q_m}$ 和包含五个选项的候选答案集合 $A = {a_1, a_2, …, a_5}$<br>目标：从候选集合中选出正确答案<br>评价指标：准确率</p><p>本文提出的框架：<img src="/../images/paper-csqa-1909-05311/overview.png" alt="overview"></p><h3 id="Knowledge-Extraction"><a href="#Knowledge-Extraction" class="headerlink" title="Knowledge Extraction"></a>Knowledge Extraction</h3><p>知识抽取，主要是对数据进行预处理的阶段</p><h4 id="Knowledge-Extraction-from-ConceptNet"><a href="#Knowledge-Extraction-from-ConceptNet" class="headerlink" title="Knowledge Extraction from ConceptNet"></a>Knowledge Extraction from ConceptNet</h4><p>构建 Concept-Graph :</p><p>1、对于每个问题和选项，在CN中确定其中出现的实体；</p><p>2、构建从问题中的实体到候选中的实体的路径：</p><ul><li>小于 3 hops</li></ul><p>3、对于从CN中抽取出来的路径拆分为三元组，将<strong>每个三元组看做一个节点</strong>，融合到图中：</p><ul><li>如果两个三元组中包含相同的实体，就在图中相应的两点之间增加一条边</li><li>问题：为什么将三元组对应为图中的一个点？而不是之间利用CN原生的节点-关系结构</li></ul><p>4、为了获取CN中节点的上下文词表示，将三元组根据<strong>关系模板</strong>转化为<strong>自然语言语句</strong></p><h4 id="Knowledge-Extraction-from-Wikipedia"><a href="#Knowledge-Extraction-from-Wikipedia" class="headerlink" title="Knowledge Extraction from Wikipedia"></a>Knowledge Extraction from Wikipedia</h4><p>使用的 Wikipedia 版本信息：version enwiki-20190301</p><p>Wikipedia 文本处理:</p><p>1、使用 Spacy 从中抽取出 107M 个句子， 并用 Elastic Search 工具构建句子索引<br>2、对于每个训练样例，去除问句和候选中的停用词，然后将所有词串联，作为检索查询<br>3、使用 Elastic 搜索引擎 在检索查询和所有句子之间进行排序，选择出 top-K 个句子作为 Wikipedia 提供的证据信息，在实验中 K=10</p><p>构建 Wiki-Graph :</p><p>1、使用SRL抽取出 句子中每个谓词的论元 (主语和宾语)<br>2、将论元和谓词都作为图中的节点，谓词和论元之间的关系作为边<br>3、为了增强图的连通性，基于下述两条规则来在 节点 a 和 节点 b 之间加入边 (首先去除停用词)：</p><ul><li>b 中包含 a 且 a 的词数大于3</li><li>a 与 b 仅有一个不同的词，并且 a 和 b 包含的词数都大于3</li></ul><h3 id="Graph-Based-Reasoning"><a href="#Graph-Based-Reasoning" class="headerlink" title="Graph-Based Reasoning"></a>Graph-Based Reasoning</h3><p>基于图的推理模块：<img src="/../images/paper-csqa-1909-05311/module.png" alt="gr"></p><p>由两部分组成，分别是 1. 基于图的上下文表示学习模块 和 2. 基于图的推理模块</p><h4 id="1-Graph-based-Contextual-Representation-Learning-Module"><a href="#1-Graph-based-Contextual-Representation-Learning-Module" class="headerlink" title="1.Graph-based Contextual Representation Learning Module"></a>1.Graph-based Contextual Representation Learning Module</h4><p>本文使用 <strong>XLNet</strong> 作为基本编码器。</p><p>将所有证据信息进行串联，作为 raw input 输入给 XLNet，获得每个词的表示。</p><p>这种方式构成的编码器输入存在一个问题：</p><ul><li>使在不同的知识源中提及的词的距离变远，即便是语义相关的。</li><li>（个人理解，这里是想说同一个词，在不同的证据句中出现的时候，由于仅仅将证据句进行简单串联，而且编码中依然存在长期依赖的问题，所以会造成，在多个证据句中出现的相同词的编码表示是存在较大差异的）</li></ul><p>针对这个问题，提出根据 graph 结构，来re-define证据词之间的相对位置。</p><ul><li>使语义相关的词的相对位置更近一些；</li><li>并用证据的内部关系结构获得更好的 CWR (contextual word representation)；</li><li>采用的方法是：利用排序算法，根据知识抽取部分得到的图结构，对证据句的顺序进行re-order；</li></ul><p>对于 Wikipedia Sentences :</p><ul><li>构建一个句子图（sentence-graph），证据句是图中的节点</li><li>当满足以下条件时，在两个句子 $s_i$ 和 $s_j$ 间建立边：<ul><li>如果在 Wiki-Graph 中的 节点 $p$ 和 $q$ 之间存在一条边，且 $p$ 和 $q$ 分别在句子 $s_i$ 和 $s_j$中。</li></ul></li><li>然后根据 下图（<strong>算法1</strong>） 对证据句进行重排序<ul><li><img src="/../images/paper-csqa-1909-05311/sort-algo.png" alt="algo"></li></ul></li></ul><p>对于结构化知识CN :</p><ul><li>根据关系模板，将 CN 中的三元组转化为自然语句，作为 CN 提供的证据句：<ul><li>E.g.: <code>(mammals, HasA, hair)</code> —&gt; <code>mammals has hair</code></li></ul></li><li>也是根据上图的 <strong>算法1</strong> 对证据句进行重排序</li></ul><p>注：从两个知识源中抽取出的证据句分别排序</p><p>最终，XLNet 的输入格式为：</p><ul><li>CN Evidence sentences $S^{\prime}_T$ ; Wiki Evidence sentences ; question $q$ ; choice $c$</li><li>四个部分的串接，用 <code>[SEP]</code> 进行分隔</li><li>在实验部分，对choice还增加了一个头部：<code>The answer is</code></li><li>最大长度设为 <code>256</code></li><li>得到 word-level clue</li></ul><h4 id="2-Graph-based-Inference-Module"><a href="#2-Graph-based-Inference-Module" class="headerlink" title="2.Graph-based Inference Module"></a>2.Graph-based Inference Module</h4><p>主要目的：在图级别上聚集、传播证据信息，并在图上进行推理以预测最终的答案</p><p>1、将 两个证据图 CN-graph 和 Wiki-Graph 看做一个图，使用 GCN 进行编码，获得节点的表示</p><ul><li>在使用 GCN 的时候，将图看为无向图进行设置</li><li>节点表示：$i$-th node<ul><li>$h_i^0$ 由证据句的 hidden state 的平均得到<ul><li>$h_i^0 = \sigma(W \sum_{w_j \in s_i} \frac{1}{|s_i|} h_{w_j})$</li><li>$s_j = \{w_0, …, w_t\}$</li><li>$W \in \mathbb{R}^{d \times k}$</li></ul></li></ul></li></ul><p>2、证据传播过程，分为两步：聚集 和 组合<br>（1）从邻居节点聚集信息</p><ul><li>$z_i^l = \sum_{j\in N_i} \frac{1}{|N_i|} V^l h_j^l$<br>（2）组合，更新节点表示</li><li>$h_i^{l+1} = \sigma(W^l h_i^l + z_i^l)$</li></ul><p>（$l$ 表示层数，$L$ 表示最终层）</p><p>3、利用图注意力，聚集图级别表示，进行最终的预测</p><ul><li>使用 multiplicative attention，$h^c$ 表示 XLNet 中对应的 <code>[CLS]</code> 位置的表示</li><li>第 $i$ 个节点的重要程度：<ul><li>$\alpha_i = \frac{h^c \sigma(W_1 h_i^L)}{\sum_{j\in N h^c \sigma(W_1 h_j^L)}}$</li></ul></li><li>最终的图表示<ul><li>$h^g = \sum_{h\in N} \alpha_j^L h_j^L$</li></ul></li></ul><p>4、最终的预测打分</p><ul><li>$score(q,a) = \text{MLP}(h^g)$</li><li>$p(q,a) = \frac{e^{score(q,a)}}{\sum_{a^{\prime}\in A} e^{score(q,a^{\prime})}}$</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>实验设置：</p><ul><li>batch size = 4</li><li>学习率 = 5e-6</li><li>训练轮数 = 1 epoch (2800 steps)</li></ul><p>主实验结果：<br><img src="/../images/paper-csqa-1909-05311/exp-all.png" alt="exp-all"></p><ul><li>给leaderboard上公布出的模型划分了四个组，对比的很全面</li></ul><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>1、对模型组成结构的验证</p><ul><li><img src="/../images/paper-csqa-1909-05311/exp-ablation.png" alt="exp-module"></li></ul><p>2、对使用知识源的验证，证实了加入异构来源的知识对最终的性能提升有很大的帮助</p><ul><li><img src="/../images/paper-csqa-1909-05311/exp-kn.png" alt="exp-kn"></li></ul><h3 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h3><p>在验证集中随机选择了50个错误样例，错误类型大致分为三种：</p><ul><li>lack of evidence</li><li>similar evidence</li><li>dataset noise</li></ul><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><ul><li>论文中涉及到了多个图，在叙述上有些混乱。</li><li>对于 topology 排序算法的验证，还需要加入一些例子来证明这部分性能提升的缘由，排序之前和排序之后，对于evidence的表示，产生了什么样的影响。</li></ul><p>疑问：</p><ol><li>在CN知识抽取部分，为什么将三元组对应为图中的一个点？而不是之间利用CN原生的节点-关系结构？</li></ol><blockquote id="fn_cose"><sup>cose</sup>. Explain Yourself! Leveraging Language Models for Commonsense Reasoning. ACL,2019. <a href="/2019/07/10/paper-acl2019-cos-e/" title="note">note</a><a href="#reffn_cose" title="Jump back to footnote [cose] in the text."> &#8617;</a></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> commonsense </tag>
            
            <tag> graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EMNLP2019 | KagNet - Knowledge-Aware Graph Networks for Commonsense Reasoning</title>
      <link href="/2019/09/09/paper-emnlp2019-kagnet/"/>
      <url>/2019/09/09/paper-emnlp2019-kagnet/</url>
      
        <content type="html"><![CDATA[<blockquote><p><em>Title</em>: <a href="https://arxiv.org/abs/1909.02151" target="_blank" rel="noopener">KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning</a><br><em>Author</em>: Bill Yuchen Lin, Xinyue Chen, Jamin Chen, Xiang Ren<br><em>Org.</em>: University of Southern California, Shanghai Jiao Tong University<br><em>Published</em>: EMNLP,2019<br><em>Code</em>: <a href="https://github.com/INK-USC/KagNet" target="_blank" rel="noopener">https://github.com/INK-USC/KagNet</a></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>本文针对的数据集是：<strong>CommonsenseQA</strong></p><p>目标：</p><ul><li><strong>empowering</strong> machines with the ability to perform commonsense reasoning/inferences<ul><li>关于推理的定义：<ul><li>reasoning is the process of combining facts and beliefs to make new decisions <sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>.</li><li>reasoning is the ability to manipulate knowledge to draw inferences <sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>.</li></ul></li><li>关于常识推理的定义：<ul><li>commonsense reasoning utilizes the basic knowledge that reflects our natural understanding of the world and human behaviors, which is common to all humans.</li></ul></li><li>Naive <strong>Physics</strong>: Humans’ natural understanding of the physical world</li><li>Folk <strong>Psychology</strong>: Humans’ innate ability to reason about people’s behavior and intentions</li></ul></li><li>gap between baselines and human performance</li><li>lack of transparency and interpretability<ul><li>how the machines manage to answer commonsense questions and make their inferences.</li></ul></li><li>why exploit commonsense knowledge bases<ul><li>knowledge-aware models can explicitly incorporate external knowledge as <strong>relational inductive biases</strong><ul><li>enhance reasoning capacity</li><li>increase the transparency of model behaviors for more interpretable results</li></ul></li><li>challenges<ul><li>How can we ﬁnd the most relevant paths in KG? ( noisy )</li><li>What if the best path is not existent in the KG? ( incomplete )</li></ul></li></ul></li></ul><h3 id="This-work"><a href="#This-work" class="headerlink" title="This work"></a>This work</h3><p>Knowledge-aware reasoning framework, two major steps:</p><ul><li>schema graph grounding (see figure below)</li><li>graph modeling for inference</li><li><img src="/../images/paper-emnlp2019-kagnet/1-graph.png" alt="k-g"></li></ul><p>Knowledge-aware graph network module: <code>KAGNET</code></p><ul><li><p><code>GCN-LSTM-HPA</code> 结构：</p><ul><li>由GCN, LSTM, 和一个 <strong>hierarchical path-based attention</strong> mechanism组成</li><li>用于 <strong>path-based relational graph representation</strong></li></ul></li><li><p>overall workflow</p><ul><li><p><img src="/../images/paper-emnlp2019-kagnet/2-overview.png" alt="overview"></p></li><li><p>首先，分别识别出 $q$ 和 $a$ 中提及的 concept ，根据这些 concept ，找到他们之间的路径，构建出 (ground) schema graph；</p></li><li>使用 LM encoder 编码 QA 对，产生 statement vector，作为 <code>GCN-LSTM-HPA</code> 的输入，来计算 graph vector；</li><li>最后使用 graph vector 计算最终的QA对的打分</li></ul></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>question $q$ with $N$ candidate answers $\{a_i\}$</p><p>schema graph $g=(V, E)$</p><h3 id="Schema-Graph-Grounding"><a href="#Schema-Graph-Grounding" class="headerlink" title="Schema Graph Grounding"></a>Schema Graph Grounding</h3><h4 id="Concept-Recognition"><a href="#Concept-Recognition" class="headerlink" title="Concept Recognition"></a>Concept Recognition</h4><ul><li>n-gram 匹配：句子中的 token 和 ConceptNet 的顶点集合进行 n-gram 匹配</li><li>Note：从有噪声的知识源中有效地提取上下文相关的知识仍是一个开放问题</li></ul><h4 id="Schema-Graph-Construction"><a href="#Schema-Graph-Construction" class="headerlink" title="Schema Graph Construction"></a>Schema Graph Construction</h4><p>sub-graph matching via path finding</p><ul><li>采取一种直接有效的方法：直接在Q和A中提及的Concept ($\mathcal{C}_q \cup \mathcal{C}_a$) 之间查找路径<ul><li>对于问题中的一个 Concept $\mathcal{c}_i \in \mathcal{C}_q$ 和候选项中的一个 Concept $\mathcal{c}_j \in \mathcal{C}_a$ ，查找他们之间路径长度小于 $k$ 的path，添加到图中<ul><li>本文中，设置 $k=4$，即3-hop paths</li></ul></li></ul></li></ul><h4 id="Path-Pruning-via-KG-Embedding"><a href="#Path-Pruning-via-KG-Embedding" class="headerlink" title="Path Pruning via KG Embedding"></a>Path Pruning via KG Embedding</h4><p>为了从潜在噪声的schema graph中删除无关的path</p><ul><li>使用KGE方法，如TransE等，预训练Concept Embedding和Relation Type Embedding（同时可用于KAGNET的初始化）</li><li>评价路径的质量<ul><li>将路径分解为三元组集合，一条路径的打分为每一组三元组的乘积，通过设置一个阈值进行过滤。</li><li>三元组的打分通过KGE中的打分函数进行计算（例如，the confidence of triple classification）</li></ul></li></ul><h3 id="Knowledge-Aware-Graph-Network"><a href="#Knowledge-Aware-Graph-Network" class="headerlink" title="Knowledge-Aware Graph Network"></a>Knowledge-Aware Graph Network</h3><ul><li><p>overview: <img src="/../images/paper-emnlp2019-kagnet/3-module.png" alt="overviwe"></p></li><li><p>首先，使用GCN对图进行编码</p></li><li>然后，使用LSTM对 $\mathcal{C}_q$ 和 $\mathcal{C}_a$ 之间的路径进行编码，捕捉multi-hop relational Information</li><li>最后，使用 hierarchical path-based attention 计算 relational schema graph 和 QA 之间路径的关系</li></ul><h4 id="Graph-Convolution-Networks"><a href="#Graph-Convolution-Networks" class="headerlink" title="Graph Convolution Networks"></a>Graph Convolution Networks</h4><p>使用GCN的目的：</p><ul><li>contextually refine the concept vector<ul><li>这里的 context 指节点在图中的上下文，即邻接关系</li><li>使用邻居来对预训练的Concept Embedding进行消歧</li></ul></li><li>capture structural patterns of schema graphs for generalization</li><li>schema graph 的模式为推理提供了潜在的有价值的信息<ul><li>QA对Concept之间的 更短、更稠密的连接 可能意味着更大的可能性，在特定的上下中。</li><li>评价 候选答案 的可能性</li></ul></li><li>$h_i^{(l+1)} = \sigma(W_{self}^{(l)} h_i^{(l)} + \sum_{j \in N_i} \frac{1}{|N_i|} W^{(l)} h_j^{l})$</li></ul><h4 id="Relational-Path-Encoding"><a href="#Relational-Path-Encoding" class="headerlink" title="Relational Path Encoding"></a>Relational Path Encoding</h4><p>定义问题中的第 $i$个 concept $c_i^{(q)}$ 和候选答案中的第 $j$ 个 concept $c_j^{(a)}$ 之间的第$k$ 条路径为 $P_{i,j}[k]$ ，路径是三元组序列：</p><ul><li>$P_{i,j}[k]=[(c_i^{(q)}, r_0, t_0), …, (t_{n-1}, r_n, c_j^{(a)})]$<ul><li>relation vector 由 KGE 预训练得到</li><li>concept vector 是 上一环节 GCN 的顶层输出</li></ul></li><li>每个三元组表示为 三个向量的串联，得到 triple vector</li><li>使用LSTM编码三元组向量序列，得到 path vector<ul><li>$R_{i,j} = \frac{1}{|P_{i,j}|} \sum_k LSTM(P_{i,j}[k])$</li></ul></li><li>$R_{i,j}$ 可以视为问题中的concept 和 候选项中的concept 之间的潜在的关系</li></ul><p>聚集所有路径的表示，得到最终的 graph vector $g$</p><ul><li>这里使用了 <code>Relation Network</code> 的方法：<ul><li>$T_{i,j} = MLP([s;c_q^{i};c_a^{(j)}])$</li><li>statement vector $s$ 为 LM encoder <code>[CLS]</code>  的表示</li></ul></li><li>然后通过mean-pooling：称这种计算方式为 <code>GCN-LSTM-mean</code><ul><li><script type="math/tex; mode=display">g=\frac{\sum_{i,j}[R_{i,j};T_{i,j}]}{|\mathcal{C}_q| \times |\mathcal{C}_a|}</script></li></ul></li><li>通过这种简单的方式将分别从 <code>symbolic space</code>  和 <code>semantic space</code> 中计算的relational representation 进行融合</li></ul><p>候选项的 plausibility 打分：</p><ul><li>$\text{score}(q,a)=sigmod(MLP(g))$</li></ul><h4 id="Hierarchical-Attention-Mechanism"><a href="#Hierarchical-Attention-Mechanism" class="headerlink" title="Hierarchical Attention Mechanism"></a>Hierarchical Attention Mechanism</h4><p>考虑到不同的路径对推理的重要程度不同，采用 mean-pooling 不是一种可取的方式。</p><p>基于此，本文提出了 hierarchical path-based attention 机制，有选择地聚集重要的path vector以及更重要的QA concept 对。</p><ul><li>使用 path-level 和 concept-pair-level attention 来学习 根据上下文建模图表示</li><li>path-level<ul><li>$\alpha_{(i,j,k)} = T_{i,j} W_1 LSTM(P_{i,j}[k])$</li><li>$\hat{a}_{(i,j,\cdot)} = softmax(\alpha_{(i,j,k)})$</li><li>$\hat{R}_{i,j} = \sum_k \hat{a}_{(i,j,k)} \cdot LSTM(P_{i,j}[k])$</li></ul></li><li>concept-pair level<ul><li>$\beta_{(i,j)} = s W_2 T_{i,j}$</li><li>$\hat{\beta}_{(\cdot, \cdot)} = softmax(\beta_{(\cdot, \cdot)})$</li><li>$\hat{g} = \sum_{i,j} \hat{\beta}_{(i,j)} [\hat{R}_{(i,j)}; T_{i,j}]$ </li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Transferability"><a href="#Transferability" class="headerlink" title="Transferability"></a>Transferability</h3><h3 id="Case-Study-on-Interpretibility"><a href="#Case-Study-on-Interpretibility" class="headerlink" title="Case Study on Interpretibility"></a>Case Study on Interpretibility</h3><p><img src="/../images/paper-emnlp2019-kagnet/4-showcase.png" alt="exp"></p><h3 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h3><ul><li>negative reasoning<ul><li>graph grounding 对否定词不敏感</li></ul></li><li>comparative reasoning strategy<ul><li>没有进行答案之间的比较</li></ul></li><li>subjective reasoning<ul><li>有些答案是根据带有主观性的推理得到的</li></ul></li></ul><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><ul><li>kAGNET 可以看做是 knowledge-augmented Relation Network (RN) module</li></ul><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">Philip N Johnson-Laird. 1980. Mental models in cognitive science. Cognitive science, 4(1):71–115.</span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">Drew A. Hudson and Christopher D. Manning. 2018. Compositional attention networks for machine reasoning. In Proc. of ICLR.</span><a href="#fnref:2" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> commonsense </tag>
            
            <tag> graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACL2019 | Modeling Semantic Compositionality with Sememe Knowledge</title>
      <link href="/2019/08/27/paper-acl2019-sc-with-sememe/"/>
      <url>/2019/08/27/paper-acl2019-sc-with-sememe/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: Modeling Semantic Compositionality with Sememe Knowledge<br>Authors: Fanchao Qi, Junjie Huang, Chenghao Yang, Zhiyuan Liu, Xiao Chen, Qun Liu, Maosong Sun<br>Org: Tsinghua University, Beihang University, Huawei Noah’s Ark Lab<br>Published: ACL 2018</p></blockquote><p>official code: <a href="https://github.com/thunlp/Sememe-SC" target="_blank" rel="noopener">https://github.com/thunlp/Sememe-SC</a></p><h2 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h2><ul><li>Semantic Compositionality (SC, 语义组合):<ul><li>is deﬁned as the linguistic phenomenon that the meaning of a syntactically complex unit is a function of meanings of the complex unit’s constituents and their combination rule <sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></li></ul></li><li>Multiword Expressions (MWE, 多字/词表达):<ul><li>大多数关于 SC 的工作都是集中在 基于向量的分布式语义模型来学习 多字/词构成的 短语、成分的表示</li><li>基本框架（公式1），以两个词为例：<ul><li><script type="math/tex; mode=display">\mathbf{p} = f(\mathbf{w}_1, \mathbf{w}_2, R, K)</script></li><li>其中: <ul><li>$f$ 是组合函数</li><li>$\mathbf{p}$ 是MWE的embedding</li><li>$\mathbf{w}_1$ 和 $\mathbf{w}_2$ 表示 MWE 成分的embedding，也就是 组成 MWE 的词的表示</li><li>$R$ 是 <strong>组合规则（Combination Rule）</strong></li><li>$K$ 指在构建MWE语义时需要的额外的知识</li></ul></li></ul></li></ul></li><li>Sememe (义原)：<ul><li>the minimum Semantic units of human language</li><li><strong>all the words can be composed of a limited set of sememes</strong>, which is similar to the idea of <em>semantic primes</em> (语义启动)<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></li><li>HowNet：义原知识库</li></ul></li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>之前的工作：利用复杂的组合函数，而很少考虑外部知识</li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><ul><li>1、设计了一个简单的 Semantic Compositionality Degree (SCD，语义组合程度) 测量实验：<ul><li>通过实验发现 MWE 的 SCD 可以通过简单的基于义原的公式计算，并且和人类的判断高度相关</li><li>义原可以很好的刻画 MWE 的含义和 MWE 的组成成分，并且可以捕捉这两者之间的语义关联</li><li>证实：义原适用于建模 SC，并且可以提高 SC 相关任务的效果，如 MWE 表示学习</li></ul></li><li>2、提出了两个结合义原的SC模型同于学习 MWE embedding，同时将公式1中的组合规则也结合到了两个模型中:<ul><li>模型1：Semantic Compositionality with Aggregated Sememe model (SCAS)</li><li>模型2：Semantic Compositionality with Mutual Sememe Attention model (SCMSA)</li></ul></li><li>Note: 在这篇文章中，主要关注<strong>两个词组成的中文 MWE 的语义组合建模</strong></li></ul><h2 id="Measuring-SC-Degree-with-Sememes"><a href="#Measuring-SC-Degree-with-Sememes" class="headerlink" title="Measuring SC Degree with Sememes"></a>Measuring SC Degree with Sememes</h2><p>这一部分的主要工作是通过一个SCD验证实验来证明义原适用于建模语义组合</p><h3 id="Sememe-based-SCD-Computation-Formulae"><a href="#Sememe-based-SCD-Computation-Formulae" class="headerlink" title="Sememe-based SCD Computation Formulae"></a>Sememe-based SCD Computation Formulae</h3><p>基于义原的SCD计算规则</p><p>基本原则：</p><ul><li>不同的MWE具有不同的 SC degrees</li><li>一个词的所有义原可以准确的描述一个词的意思</li></ul><p>启发式的设计了SCD的计算规则集合，见下图：</p><ul><li><img src="/images/paper-acl2019-sc-with-sememe/scd.png" alt="scd"></li><li>数字越大表示SCD值越高</li><li>$S_p$, $S_{w_1}$, $S_{w_2}$ 分别表示 MWE 的义原集合 和 MWE 两个组成成分(词)的义原集合</li><li>SCD = 3:<ul><li>MWE 的义原集合为两个组成成分义原集合的的并集（union）相同</li><li>也就是说，MWE 的含义正好是两个成分含义的组合</li><li>the MWE is <strong>fully semantically compositional</strong></li></ul></li><li>SCD = 2:<ul><li>MWE 的义原集合为两个组成成分义原集合的的并集（union）的一个适合的子集</li><li>组成成分的语义有覆盖 MWE 语义的部分，但是不能准确的推出 MWE 的语义</li></ul></li><li>SCD = 1:<ul><li>MWE 与 组成成分共享一部分义原，但是他们分别具有各自私有的义原</li></ul></li><li>SCD = 0:<ul><li>MWE 的含义与其组成成分的含义完全不同</li><li>无法从组成成分的含义中推出 MWE 的语义</li><li>the MWE is <strong>completely</strong> <strong>non-compositional</strong></li></ul></li></ul><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>构建了一个人工标注的SCD数据集</p><p>评测人工标注的与规则预测的SCD之间的相关系数 (Pearson 和 Spearman)</p><h2 id="Sememe-incorporated-SC-Models"><a href="#Sememe-incorporated-SC-Models" class="headerlink" title="Sememe-incorporated SC Models"></a>Sememe-incorporated SC Models</h2><h3 id="Incorporating-Sememes-Only"><a href="#Incorporating-Sememes-Only" class="headerlink" title="Incorporating Sememes Only"></a>Incorporating Sememes Only</h3><p>仅结合义原计算 MWE 的情况对应于公式1的一个简化：</p><ul><li>$\mathbf{p} = f(w_1, w_2, K)$</li><li>使用 $S$ 表示所有的义原集合，词w的义原集合为 $S_w = \{s_1, …, s_{|S_w|}\} \subset S$</li><li>$w$ 和 $\mathbf{s}$ 的 embedding 维度都是 $\mathbb{R}^d$</li></ul><h4 id="Semantic-Compositionality-with-Aggregated-Sememe-model-SCAS"><a href="#Semantic-Compositionality-with-Aggregated-Sememe-model-SCAS" class="headerlink" title="Semantic Compositionality with Aggregated Sememe model (SCAS)"></a>Semantic Compositionality with Aggregated Sememe model (SCAS)</h4><p>SCAS 的结构如下图所示：</p><ul><li><img src="/images/paper-acl2019-sc-with-sememe/m1-scas.png" alt="scad"></li><li>SCAS 模型仅仅串联了MWE组成成分和他们的义原的embedding</li><li>$\mathbf{p} = \text{tanh} (W_c [w_1 + w_2; w_1^\prime + w_2^\prime] + b_c)$<ul><li>$w_1^\prime = \sum_{s_i \in S_{w_1}} \mathbf{s}_i$</li><li>$w_2^\prime = \sum_{s_j \in S_{w_2}} \mathbf{s}_j$</li></ul></li></ul><h4 id="Semantic-Compositionality-with-Mutual-Sememe-Attention-model-SCMSA"><a href="#Semantic-Compositionality-with-Mutual-Sememe-Attention-model-SCMSA" class="headerlink" title="Semantic Compositionality with Mutual Sememe Attention model (SCMSA)"></a>Semantic Compositionality with Mutual Sememe Attention model (SCMSA)</h4><p>SCMSA 的结构如下图所示：</p><ul><li><img src="/images/paper-acl2019-sc-with-sememe/m2-scmsa.png" alt="scmsa"></li><li>SCMSA 模型 计算了 一个组成成分的义原的 Mutual Attention 以及 与其他成分的义原集合的 Mutual Attention<ul><li>动机：组成成分之间的义原互不相同，在进行义原组合的时候，MWE 的义原应对组成成分的义原应该分配不同的权重</li></ul></li><li>$\mathbf{p}$ 的计算与SCAS相同，不同的是 $w^\prime$的计算：<ul><li>$\mathbf{e}_1 = \text{tanh} (W_a w_1 +b_a)$</li><li>$a_{2,i} = \frac{\text{exp}(\mathbf{s}_i \cdot \mathbf{e}_1)}{\sum_{s_j \in S_{w_2}} \text{exp}(\mathbf{s}_j \cdot \mathbf{e}_1) }$</li><li>$w_2^\prime = \sum_{s_i \in S_{w_2}} a_{2,i} \mathbf{s}_i$</li></ul></li></ul><h3 id="Integrating-Combination-Rules"><a href="#Integrating-Combination-Rules" class="headerlink" title="Integrating Combination Rules"></a>Integrating Combination Rules</h3><p>根据原始的公式1：</p><ul><li>使用不同的MWE组合矩阵来表示不同的组合规则，即：<ul><li>$W_c = W_c^r, r\in R_s$<ul><li>$W_c^r \in \mathbb{R}^{d \times 2d}$</li></ul></li><li>$R_s$ refers to combination rule set containing syntax rules of MWE<ul><li>e.g., adjective-noun, noun-noun</li></ul></li></ul></li><li>考虑到组合矩阵的稀疏性，以及组合矩阵之间会存在通用的组合信息，将上述矩阵进行拆分：<ul><li>$W_c = U^rV^r + W_c^c$<ul><li>$U^r \in \mathbb{R}^{d \times h_r}$</li><li>$V^r \in \mathbb{R}^{d \times h_r}$</li><li>$h_r \in \mathbb{N}_+$ 为超参数，根据组合规则变化</li><li>$W_c^c \in \mathbb{R}^{d\times 2d}$</li></ul></li></ul></li></ul><h3 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h3><ul><li><p>training for MWE similarity computation</p><ul><li>squared Euclidean distance</li></ul></li><li><p>training for MWE sememe prediction</p><ul><li>weighted cross-entropy loss</li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">Francis Jeffry Pelletier. 1994. The Principle of Semantic Compositionality. Topoi, 13(1):11–24.</span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">Anna Wierzbicka. 1996. Semantics: Primes and Universals: Primes and Universals. Oxford University Press, UK.</span><a href="#fnref:2" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> chinese </tag>
            
            <tag> sememe </tag>
            
            <tag> hownet </tag>
            
            <tag> semantic-compositionality </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 | Improving Question Answering with External Knowledge</title>
      <link href="/2019/08/26/paper-2019-edl-md/"/>
      <url>/2019/08/26/paper-2019-edl-md/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: <a href="https://arxiv.org/pdf/1902.00993v1.pdf" target="_blank" rel="noopener">Improving Question Answering with External Knowledge</a><br>Authors: Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, Dong Yu<br>Org: Rensselaer Polytechnic Institute/ CMU/ Tencent AI Lab<br>Published: unpublished</p></blockquote><p>之前在组内汇报过的一篇文章，最近翻出来了，直接把之前做的slide分享一下</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li><img src="/images/paper-2019-edl-md/EDL-4.png" alt="4"></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul><li><img src="/images/paper-2019-edl-md/EDL-6.png" alt="6"></li><li><img src="/images/paper-2019-edl-md/EDL-7.png" alt="7"></li><li><img src="/images/paper-2019-edl-md/EDL-8.png" alt="8"></li><li><img src="/images/paper-2019-edl-md/EDL-9.png" alt="9"></li><li><img src="/images/paper-2019-edl-md/EDL-10.png" alt="10"></li><li><img src="/images/paper-2019-edl-md/EDL-11.png" alt="11"></li><li><img src="/images/paper-2019-edl-md/EDL-12.png" alt="12"></li><li><img src="/images/paper-2019-edl-md/EDL-13.png" alt="13"></li></ul><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul><li><img src="/images/paper-2019-edl-md/EDL-15.png" alt="15"></li></ul><h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> qa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 | SenseBERT - Driving Some Sense into BERT</title>
      <link href="/2019/08/18/paper-2019-sense-bert/"/>
      <url>/2019/08/18/paper-2019-sense-bert/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: <a href="https://arxiv.org/abs/1908.05646" target="_blank" rel="noopener">SenseBERT: Driving Some Sense into BERT</a><br>Authors: Yoav Levine, Barak Lenz, Or Dagan, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, Yoav Shoham<br>Org: AI21 Labs, Tel Aviv, Israel<br>Published: unpublished</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>现有的工作通过应用自监督学习，使神经网络语言模型在NLU上取得了很大的进步</li><li>但是，目前自监督技术主要在 <strong>word-form level</strong> 上进行操作（即在word-form level上提取自监督信号指导模型进行学习）<ul><li>这种 <strong>word-form</strong> 级别的监督只是本质的语义内容信号的一种替代。</li><li>从词汇语义 (lexical semantic) 的角度来看，word-form 可以看做是词汇的 surface-level 的表现形式</li></ul></li><li>一词多义是自然语言处理中的一个常见现象，一个词具有多种不同的含义（文中举的例子是 ‘bass’，既可以指一种鲈鱼，低音吉他，还可以指低音歌唱家）<ul><li>一个词，其本身的形式，仅仅是在给定上下文/特定语境中的实际意义的一个替代。</li><li>一词多义现象所带来的一个重要挑战就是自然语言理解中的歧义问题</li></ul></li><li>BERT中的MLM只是对word-form进行的mask，无法捕捉word-sense信息，即缺乏对 lexical semantic 的建模</li></ul><h2 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h2><p>这篇文章对于BERT的改进，正如其题目所说，drive some sense into BERT：</p><ul><li>除了基础的预测 masked word 任务，还引入了一类 <strong>explicit word-sense</strong> 作为 BERT的 <strong>semantic-level</strong> 的自监督信号<ul><li><strong>explicit word-sense</strong> 信息指的是 每个词在 WordNet 中所对应的 <strong>supersense</strong> （共有45个supersense分类，具体参见论文Appendix）</li><li>相应的增加了一个预测 masked word-sense 任务，即预测被mask的词所对应的supersense</li></ul></li><li>外部语言学知识的引入，还可以提高模型对于词汇语义（lexical semantics）的归纳偏置。</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>本文通过两种实验来证明所提出的SenseBERT的有效性: <strong>Lexical Semantics 实验</strong>和通用的<strong>GLUE</strong>评测</p><p>Lexical Semantics实验数据集为：SemEval WSD 和 WiC(Word in Context)</p><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><ul><li>一词多义是自然语言处理中普遍存在的现象之一</li><li>而传统的词向量训练方法得到的 Word Embedding 都是静态的向量表示，无法准确的表示一个 word-form 的多种不同 <strong>词义</strong></li><li>这两年以来，GPT、ELMO、BERT等一系列预训练语言模型，通过在大规模的无监督语料上进行预训练，从而使PLM产生 dynamic contextual word/token representation ，可以认为间接的缓解了 一词多义 问题。<ul><li>对于下游任务来说，使用PLM产生了更符合当前语境或上下文的词表示，使词表示更加准确</li><li>但还是从 word surface level 进行词义的学习，缺乏直接针对 lexical semantic 的监督信号</li></ul></li><li>相比于过去一些工作，使用 WordNet 中的 lexical semantic 信息作为词级别的特征输入，SenseBERT 使用 lexical semantic 信息作为监督信号参与到PLM中的训练中，还可以使模型具有区分 lexical semantic 信息的能力，增加了PLM对 word-sense 的建模能力。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> note </tag>
            
            <tag> wordnet </tag>
            
            <tag> pre-trained-lm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACL2019 | Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension</title>
      <link href="/2019/07/29/paper-acl2019-kt-net/"/>
      <url>/2019/07/29/paper-acl2019-kt-net/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension<br>Author: An Yang et al.<br>Org.: MOE(PKU), Baidu<br>Published: ACL,2019</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>预训练语言模型在机器阅读理解任务上取得了突破性进展。通过在海量无标注文本数据上对足够deep的网络结构进行预训练而得到的LM，能够捕捉复杂的语言现象，更好地理解语言。</li><li>然而，真正意义上的阅读理解不仅要求机器具备语言理解的能力，还要求机器具备知识以支撑复杂的推理。</li><li>例如：<img src="/images/paper-acl2019-kt-net/kt-net-example.png" alt="example"><ul><li>这个例子需要用到下面的知识<ul><li>world knowledge：<em>Trump is the person who leads US</em></li><li>word knowledge：<em>sanctions has a common hypernym with ban</em></li></ul></li><li>BERT在不知道额外知识的情况下无法正确地判断出结果</li></ul></li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><ul><li>为了同时利用强大的PLM捕获的语言规律和外部高质量的知识/事实，提出了语言表示与知识表示的深度融合模型 <strong>KT-NET（Knowledge and Text Fusion Net）</strong></li><li>外部知识使用的是 包含词汇知识的WordNet 以及 包含实体信息的NELL<ul><li>没有采取符号化的知识表示，而是使用了KB embedding，文中给出了两点原因：</li><li>1、KB embedding 带有整个KB的全局信息</li><li>2、易于同时融合多个KBs，而不需要过多的task-specific设计</li></ul></li><li>在ReCoRD（<a href="https://sheng-z.github.io/ReCoRD-explorer/）数据集上取得了SOTA" target="_blank" rel="noopener">https://sheng-z.github.io/ReCoRD-explorer/）数据集上取得了SOTA</a></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>模型的整体结构如图所示，主要包含4个模块：</p><ul><li><img src="/images/paper-acl2019-kt-net/kt-net-model.png" alt="model"></li></ul><ol><li>BERT encoding layer</li><li>knowledge integration layer: select desired KB embeddings and integrate KB representation with BERT representation</li><li>self-matching layer: fuse BERT and KB representations</li><li>output layer: predict the final answer</li></ol><p>使用 BERT 进行编码时，将question和passage连在一起输给BERT，question 作为 sentence1，passage 作为 sentence2。<br>使用 BERT 最后一层的输出 $h_i^L \in \mathbb{R}^{d_1}$ 作为 passage 和 question 中每个token的上下文编码。</p><p>下面主要介绍一下KT-NET中最重要的两个模块和KB编码与抽取过程：</p><h3 id="Knowledge-Integration-Layer"><a href="#Knowledge-Integration-Layer" class="headerlink" title="Knowledge Integration Layer"></a>Knowledge Integration Layer</h3><p>对于每个token $s_i$，我们可以得到上下文编码 $h_i^L$，以及检索到的相关KB concepts集合 $C(s_i)$</p><ul><li>$C(s_i)$中的每个 $c_j \in \mathbb{R}^{d_2}$ 是预训练好的KB embedding</li></ul><p>使用Attention机制自适应地选择最相关的KB Concepts，同时引入了一个sentinel向量$\bar{c} \in \mathbb{R}^{d_2}$，来控制检索出来的KB Concepts中没有相关KB的情况：</p><ul><li>concept 相关度：$\alpha_{ij} \propto \text{exp}(c_j^\top W h_i^L)$<ul><li>$W\in \mathbb{R}^{d_2 \times d_1}$</li></ul></li><li>sentinel 相关度：$\beta_i \propto \text{exp}(\bar{c}^\top W h_i^L)$</li></ul><p>经过Attention之后得到该token的 Knowledge State 表示向量：</p><ul><li>$\mathbf{k}_i = \sum_j \alpha_{ij}c_j + \beta_i \bar{c}$</li><li>由于sentinel向量的加入，需要对attention score进行约束：$\sum_j \alpha_{ij} + \beta_{i} = 1$</li></ul><p>最后，将得到是knowledge state向量与上下文编码表示进行<strong>串联</strong>，得到本模块的输出：</p><ul><li>knowledge-enriched 表示：$u_i = [h_i^L,k_i] \in \mathbb{R}^{d_1 + d_2}$</li></ul><h3 id="Self-Matching-Layer"><a href="#Self-Matching-Layer" class="headerlink" title="Self-Matching Layer"></a>Self-Matching Layer</h3><p>获取了knowledge-enriched表示之后，通过self-attention机制使上下文表示和知识表示进行交互，本文中设计了直接交互和间接交互两种self-attention计算方式</p><p>直接（direct）交互：采用的是BIDAF中的trilinear公式计算attention，计算token $s_j$ 和 token $s_i$ :</p><ul><li>$r_{ij} = w^\top [u_i, u_j, u_i \odot u_j]$<ul><li>$w \in \mathbb{R}^{3d_1 + 3d_2}$</li></ul></li><li>row-wise softmax: $a_{ij}=\frac{\text{exp}(r_{ij})}{\sum_j \text{exp} (r_{ij})}$</li><li>attended vector: $\mathbf{v}_i  = \sum_j a_{ij} u_j$</li><li>$\mathbf{v}_i$ 表示 每个token j与token i的直接交互程度</li></ul><p>间接（indirect）交互：</p><ul><li>间接交互指的是：token i和token j可以通过一个中间token k产生间接的关联，计算间接交互的方式很简单</li><li>attention matrix $\bar{\mathbf{A}} = \mathbf{A}^2$</li><li>$\bar{\mathbf{v}}_i = \sum_j \bar{a}_{ij} u_j$</li></ul><p>最后，将两种交互得到的attended向量表示进行串接，作为本模块的输出：</p><ul><li>$\mathbf{o}_i = [u_i, v_i, u_i - v_i, u_i \cdot v_i, \bar{v}_i, u_i - v_i] \in \mathbb{R}^{6d_1 + 6d_2}$</li></ul><h3 id="Knowledge-Embedding-and-Retrieval"><a href="#Knowledge-Embedding-and-Retrieval" class="headerlink" title="Knowledge Embedding and Retrieval"></a>Knowledge Embedding and Retrieval</h3><p>KB Embedding：</p><ul><li>采用的是 Yang et al. (2015)<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> 提出的 BILINEAR 算法</li></ul><p>KB Concepts Retrieval:</p><ul><li>WordNet：给定一个词，返回其在WordNet中的额同义词集（synsets）作为候选的KB Concepts</li><li>NELL：首先对Passage和Question进行NER，通过字符串匹配在NELL找到实体对应的mention，抽取对应的KB作为候选的KB Concepts</li><li>每个KB Concept 其实是在KB中对应的尾实体？</li></ul><p>在这一部分最后，文中还列举了自身的几点Advantages：</p><ul><li>之前的一些结合knowledge的MRC工作，都可以算作是一种 <strong>retrieve-then-encode</strong> 范式，这部分工作只对抽取出来的相关知识进行编码（参考另一篇blog<a href="/2019/01/09/paper-knreader/" title="note">note</a>）与整合，获得的是局部的、与文本相关的知识信息<ul><li>本文的工作首先是在整个KB上预训练出kb embedding，可以捕获全局的信息</li><li>基于使用的KB Embedding方法来说，是全局的表示</li></ul></li><li><strong>易于扩展至融合多个KB的信息</strong><ul><li>对于每个token可以从不同的KB中抽取出不同的KB Concepts集合 $C^j(s_i)$</li><li>通过不同的KB Concepts集合 $C^j(s_i)$ 计算出不同的 Knowledge State 向量 $k_i^j$</li><li>直接将多个 $k^j_i$ 与 $h_i^L$ 串联获得融合多个KB的knowledge-enriched表示：$u_i = [h_i^L, k_i^1, k_i^2,…]$</li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul><li>KT-NET中的encoder使用的是BERT-Large-Cased版本，输入最大长度限制为 384，学习率 3e-05，batch大小为24</li><li>KB embedding是预训练好的，在训练过程中固定，不进行微调</li><li>实验数据集为：ReCoRD 和 SQuAD</li><li>case study<ul><li>从例子的热力图中，可以明显看出与BERT相比，KT-Net可以使不同的question词对passage中的词赋予不同的相关性，并且具有KB信息的支持，而BERT中，不同的question词对同一个passage中的词的相关性是相近的，也就是说，question中的每个词对于passage中的词的相关性相等，</li><li><img src="/../images/paper-acl2019-kt-net/kt-net-case.png" alt="case"></li></ul></li></ul><h2 id="Analysis-amp-Summary"><a href="#Analysis-amp-Summary" class="headerlink" title="Analysis &amp; Summary"></a>Analysis &amp; Summary</h2><ul><li>KT-NET 仅在 ReCoRD 数据集上取得了SOTA的成绩，印象中在leaderboard上放了将近半年也没有其他模型可以超越它的成绩，足以证明这个模型的性能</li><li>如何选择预训练KB embedding的方法？</li><li>从在SQuAD数据集上的实验可以看出，相比$KT-NET_{BOTH}$和$KT-NET_{NELL}$，$KT-NET_{wordnet}$取得了最好的结果，可以间接的说明，SQuAD数据集仅需要文本的信息就可以很好的回答问题，并不依赖外部的世界知识（实体信息）</li></ul><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">Embedding entities and relations for learning and inference in knowledge bases. ICLR,2015.</span><a href="#fnref:1" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> pre-trained-lm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACL2019 | Explicit Utilization of General Knowledge in Machine Reading Comprehension</title>
      <link href="/2019/07/17/paper-acl2019-kar/"/>
      <url>/2019/07/17/paper-acl2019-kar/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: Explicit Utilization of General Knowledge in Machine Reading Comprehension<br>Author: Chao Wang, Hui Jiang<br>Org.: York University<br>Published: ACL,2019<br>Old Version: Exploring Machine Reading Comprehension with Explicit Knowledge. arXiv:1809.03449</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>MRC模型和人类之间的差距有两方面<ul><li>1、MRC模型需要大量的训练样例来学习</li><li>2、MRC模型对于有意加入噪声数据不鲁棒</li></ul></li><li>造成差距的原因在于目前MRC模型仅利用了给定passage-question对中的信息，而没有像人类一样利用一些general knowledge</li><li>如何利用抽取的知识<ul><li>目前的做法都是隐式地将抽取到的知识的编码用于增强相应词的lexical/contextual表示</li><li>缺点：缺乏解释性和控制性</li></ul></li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><ul><li>本文中提出，词之间的语义联系（<strong>inter-word semantic connections</strong>）可以作为一种general knowledge<ul><li>例如：<img src="/images/paper-acl2019-kar/data-example.png" alt="example"></li></ul></li></ul><p>此外，本文</p><ul><li>提出了一种 <strong>data enrichment</strong> 方法，利用 <strong>WordNet</strong> 作为知识源，为passage-question对抽取 inter-word semantic connections</li><li>提出了一个 <strong>Knowledge Aided Reader（KAR）</strong> 模型，用于显示地将抽取到的 general knowledge 引入到模型中，并辅助 attention 机制</li></ul><h2 id="Data-Enrichment-Method"><a href="#Data-Enrichment-Method" class="headerlink" title="Data Enrichment Method"></a>Data Enrichment Method</h2><p>本文介绍的基于 WordNet 的数据富集的方法是一个可控的抽取过程</p><h3 id="Semantic-Relation-Chain"><a href="#Semantic-Relation-Chain" class="headerlink" title="Semantic Relation Chain"></a>Semantic Relation Chain</h3><p>WordNet 中的几个要素：</p><ul><li><strong>synset</strong>：同义集合（a set of words expressing the same sense）<ul><li>一个词可以有多个不同的senses，可以属于多个synset</li></ul></li><li><strong>semantic relations</strong>：synset 之间的语义关系<ul><li>在NLTK中，共有16种<ul><li>hypernyms, hyponyms, holonyms, meronyms, attributes, etc.</li></ul></li></ul></li></ul><p>本文根据 synset 和 semantic relation 定义了一个新的概念：<font color="blue"><strong>Semantic Relation Chain</strong></font></p><ul><li>连接一个 synset 与另一个 synset 的一系列 semantic relation（a concatenated sequence of semantic relations）</li><li>semantic relation chain 中的每个 semantic relation 定义为一跳（a hop）</li></ul><h3 id="Inter-word-Semantic-Connection"><a href="#Inter-word-Semantic-Connection" class="headerlink" title="Inter-word Semantic Connection"></a>Inter-word Semantic Connection</h3><p>Data Enrichment 方法的核心问题就是确定两个词之间是否存在 <strong>语义的联系（semantically connections）</strong><br>为了解决这个问题，定义了一个新的概念：<strong>扩展同义词集（the extended synsets of a word）</strong></p><ul><li>定义：通过 semantic relation chain 可以到达的 synset</li><li>定义符号：$S_w$ 表示 synset，$S_w^*$ 表示扩展同义词集</li><li>理论上来看，如果不加以限制的话，WordNet 中所有的 synset 都将属于 $S_w^*$<ul><li>故此，引入了一个超参数 $k \in \mathbb{N}$，表示 semantic relation chain 的最大跳数</li><li>即，只有小于 $k$ 的 chains 才用于构建 $S_w^*$：<ul><li>$ S_w^{*}(k) $</li><li>$\text{ if } k=0, \text{ we will have }S_w^{*}(0)=S_w$</li></ul></li></ul></li></ul><p>构建 inter-word semantic connections，定义：</p><ul><li>当且仅当 $S_{w_1}^* (k) \cap S_{w_2} \neq \emptyset$ 时，$w_1$ 和 $w_2$ 具有语义关联</li></ul><h3 id="General-Knowledge-Extraction"><a href="#General-Knowledge-Extraction" class="headerlink" title="General Knowledge Extraction"></a>General Knowledge Extraction</h3><p>遵循上述的定义，为给定的 passage-question 对抽取任意词与 passage 中的词的 inter-word semantic relation：</p><ul><li>只抽取 positional information</li><li>对词 $w$，抽取一个集合 $E_w$，包含所有 passage 中与 $w$ 有语义关联的词的位置，如果 $w$ 本身是 passage 中的词，去除其本身在 passage 中的位置</li><li>通过上一节定义的超参数 $k$ 就可以控制抽取的 扩展同义词集 的大小，即抽取出来的 general Knowledge 的数量<ul><li>超参数 $k$ 通过在验证集上的效果确定</li></ul></li></ul><h2 id="Knowledge-Aided-Reader"><a href="#Knowledge-Aided-Reader" class="headerlink" title="Knowledge Aided Reader"></a>Knowledge Aided Reader</h2><p>KAR 模型的结构如下图所示：</p><ul><li><img src="/images/paper-acl2019-kar/model-kar.png" alt="model-kar"></li><li>与现有的一些模型相比，主要的改进集中在右侧部分的输入和引入 general Knowledge 之后， attention 计算的改进</li></ul><p>Notation:</p><ul><li>$P = \{p_1, … , p_n\}$</li><li>$Q = \{q_1, … , q_m\}$</li></ul><p>Knowledge Aided Mutual Attention</p><ul><li>利用提起抽取好的 general Knowledge 为每个词 $w$ 构建 增强的上下文表示 $c_w^*$<ul><li>通过 $E_w$ 和 原始上下文向量 $C_p$ 的对应关系 得到 matching context embeddings $Z \in \mathbb{R}^{d\times |E_w|}$</li><li>计算 matching vector $c_w^+$<ul><li>$t_i = v_c^{\top} tanh(W_c x_i + U_c c_w) \in \mathbb{R}$</li><li>$c_w^+ = Z \text{ softmax} ({t_1,…,t_{|E_w|}}) \in \mathbb{R}^{d}$</li></ul></li></ul></li><li>attention 的计算方式 同 BIDAF</li></ul><p>Knowledge Aided Self Attention</p><ul><li>方法同上</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul><li>超参数 $k$ 的选择<ul><li><img src="/images/paper-acl2019-kar/exp-k.png" alt="exp-k"></li></ul></li></ul><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><ul><li>抽取出来的 general knowledge 相当于为 passage-question 对中的词构建了一个隐式的关联矩阵，通过这个关联矩阵，在两次 attention 中抽取出对应的 contextual representation 和 coarse memory （result of mutal attention）来辅助增强 attention 的输入</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> wordnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACL2019 | Explain Yourself! Leveraging Language Models for Commonsense Reasoning</title>
      <link href="/2019/07/10/paper-acl2019-cos-e/"/>
      <url>/2019/07/10/paper-acl2019-cos-e/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: Explain Yourself! Leveraging Language Models for Commonsense Reasoning<br>Authors: Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, Richard Socher<br>Org.: Salesforce Research<br>Published: ACL 2019</p></blockquote><p>official code: <a href="https://github.com/salesforce/cos-e" target="_blank" rel="noopener">https://github.com/salesforce/cos-e</a></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote><p>Commonsense reasoning that draws upon world knowledge derived from spatial and temporal relations, laws of physics, causes and effects, and social conventions is a feature of human intelligence.<br>However, it is difficult to instill such commonsense reasoning abilities into artificial intelligence implemented by deep neural networks. While neural networks effectively learn from a large number of examples, commonsense reasoning for humans precisely hits upon the kind of reasoning that is in less need of exemplification.<br>Rather, humans pick up the kind of knowledge required to do commonsense reasoning simply by living in the world and doing everyday things.<br>AI models have limited access to the kind of world knowledge that is necessary for commonsense reasoning.</p><p>from official blog: <a href="https://blog.einstein.ai/leveraging-language-models-for-commonsense/" target="_blank" rel="noopener">https://blog.einstein.ai/leveraging-language-models-for-commonsense/</a></p></blockquote><ul><li>DL 模型在需要常识推理的任务上表现不佳，原因可能是往往需要某种形式的世界知识，或是推理的信息未在输入中直接显示出来</li><li>虽然已经有了一些用于检验模型Commonsense推理能力的数据集，但是 it is still unclear how these models perform reasoning and to what extent that reasoning is based on world knowledge.</li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><ul><li>构建了一个Common Sense Explanations（CoS-E）数据集：收集了人类对于常识推理的解释（模拟人类的常识推理过程）以及highlight annotation（针对问句的）</li><li>提出了一个常识自动生成解释（Commonsense Auto-Generate Explanation，CAGE）框架：使用CoS-E数据集来训练语言模型，是LM可以自动的产生解释（这个解释可以在训练和推理过程中使用），证明可以有效地利用语言模型来进行Commonsense Reasoning</li><li>在任务数据集（CommonsenseQA）上取得了10%的提升（Version 1.0）</li><li>还进行了跨领域迁移的实验</li></ul><h3 id="Background-and-Related-Work"><a href="#Background-and-Related-Work" class="headerlink" title="Background and Related Work"></a>Background and Related Work</h3><ul><li>Commonsense reasoning</li><li>Natural language explanations</li><li>Knowledge Transfer in NLP</li></ul><h2 id="CoS-E-Corpus"><a href="#CoS-E-Corpus" class="headerlink" title="CoS-E Corpus"></a>CoS-E Corpus</h2><p>Common Sense Explanations 数据集是在基于CommonsenseQA任务数据集上构建的，包含两种形式的人类解释：</p><ul><li>1、open-ended 自然语言解释 （<em>CoS-E-open-ended</em>）</li><li>2、对于问句，标注了 highlighted span annotations，标注的是对于预测正确答案起到重要作用的词 （<em>CoS-E-selected</em>）</li><li>示例：<ul><li><img src="/images/paper-acl2019-cos-e/cos-e-example.png" alt="cos-e"></li></ul></li></ul><h2 id="Algorithm-and-Model"><a href="#Algorithm-and-Model" class="headerlink" title="Algorithm and Model"></a>Algorithm and Model</h2><p>（在这篇文章中，进行实验时使用的是CommonsenseQA Version 1.0 的数据集，所以只有三个候选答案，在 Version 1.1 中，每个问题对应5个候选答案）</p><p>框架图：</p><ul><li><img src="/images/paper-acl2019-cos-e/framework-overview.png" alt="overview"></li></ul><h3 id="Commonsense-Auto-Generated-Explanations-CAGE"><a href="#Commonsense-Auto-Generated-Explanations-CAGE" class="headerlink" title="Commonsense Auto-Generated Explanations (CAGE)"></a>Commonsense Auto-Generated Explanations (CAGE)</h3><p>在CAGE框架中，使用GPT进行微调。微调部分的框架如上图中的图（a）所示。</p><p>本文提出了两种生成解释（即微调GPT-LM）的方法：</p><ul><li>1、explain-and-then-predict（reasoning）</li><li>2、predict-and-then-explain（rationalization）</li></ul><p>方法一：<strong>Reasoning</strong></p><ul><li>输入：$C_{RE} = q,c_0, c_1, c_2 \text{ ?}\text{ commonsense says} $</li><li>目标：条件式生成 explanations $e$<ul><li>$\sum_i \text{log} P(e_i|e_{i-k},…,e_{i-1}, C_{RE}; \Theta)$</li><li>其中，$k$ 是语言模型生成句子的窗口大小，在本文中，设为大于explanation的长度</li></ul></li><li>称为reasoning的原因：<ul><li>在推理阶段也可以自动的生成explanation，为常识问答提供额外的上下文</li><li>Note：在生成explanation的过程中不知道正确答案的标签</li></ul></li></ul><p>方法二：<strong>Rationalization</strong></p><ul><li>输入：$C_{RA} = q,c_0, c_1, c_2 \text{ ? } a \text{ because} $</li><li>目标：同上</li><li>在这个算法中，在训练时，生成explanation的过程中是已知正确答案的标签的<ul><li>相当于为正确的 q-a 对 产生一个合理的解释</li><li>而不是真正的常识推理</li></ul></li></ul><p>CAGE的参数设置：</p><ul><li>最大序列长度：20</li><li>batch size：36</li><li>train epochs：10</li><li>learning rate：$1e-6$</li><li>warmup linearly with proportion $0.002$</li><li>weight decay：0.01</li><li>模型选择指标：在验证集上的BLEU和perplexity</li></ul><h3 id="Commonsense-Predictions-with-Explanations"><a href="#Commonsense-Predictions-with-Explanations" class="headerlink" title="Commonsense Predictions with Explanations"></a>Commonsense Predictions with Explanations</h3><p>这部分关注的是，给定一个人类生成的解释或是LM推理生成的解释，使模型在CQA任务上进行预测。见上图图（b）</p><p>任务模型采用BERT with multiple choice</p><ul><li>BERT输入：question [SEP] explanation [SEP] candidate answer choice</li><li>微调BERT的参数：<ul><li>batch size：24</li><li>test batch size：12</li><li>train epochs：10</li><li>maximum sequence length：175（baseline=50）</li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>这篇文章的实验部分很充分、很完整，包含以下几个方面：</p><ul><li>使用CoS-E进行CQA任务，在dev上的效果：Table-2<ul><li><em>仅在训练时应用</em> CoS-E-open-ended</li><li>BERT baseline：63.8%</li><li>在BERT中加入CoS-E-open-ended，提升了2个点（65.5%）</li><li>CAGE-reasoning：72.6%</li><li>CQA test set 上的效果见 Table-3</li></ul></li><li>Table-4：在训练和验证时同时应用CoS-E</li><li>Table-5：在CQA v1.1 测试集上的结果</li><li>迁移到域外数据集</li></ul><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><p>论文中有意思的几点分析</p><ul><li>explanations可以帮助阐明更长更复合的问题</li><li>CAGE-reasoning 生成的explanations中，有43%的可能性包含答案选项</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> note </tag>
            
            <tag> commonsense </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EMNLP2018 | Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text</title>
      <link href="/2019/07/08/paper-emnlp2018-graft-net/"/>
      <url>/2019/07/08/paper-emnlp2018-graft-net/</url>
      
        <content type="html"><![CDATA[<blockquote><p><em>Title</em>: Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text<br><em>Authors</em>: Haitian Sun, Bhuwan Dhingra et al.<br><em>Org.</em>: CMU<br><em>Published</em>: EMNLP 2018</p></blockquote><p>official code: <a href="https://github.com/OceanskySun/GraftNet" target="_blank" rel="noopener">https://github.com/OceanskySun/GraftNet</a></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>大多数 open domain QA 任务都是使用单信息源（要么是text from encyclopedia，或者是 single KB）来回答问题</li><li>判断一个信息源的适用性（suitability），取决于<strong>信息源的覆盖度（coverage）</strong>和从其中<strong>抽取答案的难度（difficulty of extracting answers from it）</strong><ul><li>非结构化，large text corpus作为信息源：具有很高的覆盖度，但是信息被不同的text pattern表示（这里可以理解为，不同领域/体裁的文本有不同的表现形式），模型还需要学习这些text pattern，导致模型难以泛化到其他领域以及新的推理类型</li><li>结构化，KB作为信息源：覆盖度低（由于不可避免的不完全性和有限的模式），但是更易于抽取答案</li></ul></li><li>由于有两种信息源存在，有些问题可被text回答，有些问题更适合用KB回答，但是只使用一种信息源不足以回答问题，一个很自然的问题就是<strong>如何有效地结合多种类型的信息</strong>，有以下两种方式：<ul><li><font color="blue"><strong>late fusion</strong></font>: <ul><li>为每种信息源设计SOTA的QA模型，得到他们的预测结果之后，再用一些启发式的方法将得到的答案进行聚合</li><li>问题：sub-optimal的解决方法，模型受限于从不同的信息源中聚集证据信息</li></ul></li><li><font color="green"><strong>early fusion</strong></font>：本文所采取的方式<ul><li>只利用一个模型，训练其从一个问题子图中抽取答案</li><li>问题子图，既包含相关的 KB fact 又包含 text</li><li>可以灵活地结合多个信息源的知识</li></ul></li></ul></li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><p>这篇文章进行的是开放域的KBQA任务（incomplete KB），结合图表示学习，提出了一个GRAFT-Net模型，可以从同时包含文本、KB实体与关系的 Question-specific 子图中抽取答案</p><ul><li>为了实现early fusion，提出了一个 Graphs of Relations Among Facts and Text Network（GRAFT-Net）</li><li>基于图卷积神经网络模型，可以在由KB facts和text sentences组成的异构图上进行运算<ul><li>提出了 <strong>heterogeneous update rules</strong> 来处理KB节点；LSTM-based update rules来更新text节点</li><li>提出了 <strong>directed propagation method</strong>， 启发自 Personalized PageRank 算法，用于限制图中 embedding 在基于从seed节点链接到question的路径上进行传播</li></ul></li><li>实验数据集：WikiMovies，WebQuestionSP</li></ul><h2 id="Task-Setup"><a href="#Task-Setup" class="headerlink" title="Task Setup"></a>Task Setup</h2><h3 id="Task-Description"><a href="#Task-Description" class="headerlink" title="Task Description"></a>Task Description</h3><h3 id="Question-Subgraph-Retrieval"><a href="#Question-Subgraph-Retrieval" class="headerlink" title="Question Subgraph Retrieval"></a>Question Subgraph Retrieval</h3><h2 id="GRAFT-Net"><a href="#GRAFT-Net" class="headerlink" title="GRAFT-Net"></a>GRAFT-Net</h2><h3 id="1-Node-Initialization"><a href="#1-Node-Initialization" class="headerlink" title="1.Node Initialization"></a>1.Node Initialization</h3><h3 id="2-Heterogeneous-Updates"><a href="#2-Heterogeneous-Updates" class="headerlink" title="2.Heterogeneous Updates"></a>2.Heterogeneous Updates</h3><p>Entities<br>Documents</p><h3 id="3-Conditioning-on-the-Question"><a href="#3-Conditioning-on-the-Question" class="headerlink" title="3.Conditioning on the Question"></a>3.Conditioning on the Question</h3><p>Attention over Relations<br>Directed Propagation</p><h3 id="4-Answer-Selection"><a href="#4-Answer-Selection" class="headerlink" title="4.Answer Selection"></a>4.Answer Selection</h3><h3 id="5-Regularization-via-Fact-Dropout"><a href="#5-Regularization-via-Fact-Dropout" class="headerlink" title="5.Regularization via Fact Dropout"></a>5.Regularization via Fact Dropout</h3><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> note </tag>
            
            <tag> reasoning </tag>
            
            <tag> kbqa </tag>
            
            <tag> graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 | Incorporating Sememes into Chinese Definition Modeling</title>
      <link href="/2019/07/08/paper-2019-cdm-with-sememe/"/>
      <url>/2019/07/08/paper-2019-cdm-with-sememe/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Incorporating Sememes into Chinese Definition Modeling<br>2019<br>Linear Yang et al.</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Chinese Definition Modeling(CDM) 任务：为给定的中文词产生词典式的中文定义</p><h3 id="This-work"><a href="#This-work" class="headerlink" title="This work"></a>This work</h3><ul><li>为了解决CDM任务，构建了一个CDM数据集，每个example由 <code>&lt;word, sememes, definition&gt;</code> 三元组组成</li><li>两个新模型 <ul><li>1、Adaptive-Attention Model（AAM）：利用adaptive注意力机制结合sememes（义原）信息生成 Definition</li><li>2、Self- and Adaptive-Attention Model（SAAM ）：进一步使用self-attention替代AAM中的recurrent connection，减少word，sememes，definition之间的路径长度</li></ul></li></ul><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>Chinese Definition Modeling Corpus：</p><ul><li>包含 104,517 个条目</li><li>三元组：<code>&lt;a word, the sememes of a speciﬁc word sense, and the deﬁnition in Chinese of the same word sense&gt;</code></li><li>Sememes：义原，是描述词义的最小的语义单位<ul><li>具体信息请参考：HowNet</li><li>为什么要使用义原：可以为生成定义提供额外的语义信息</li></ul></li><li>例子：<img src="/images/paper-2019-cdm-with-sememe/fig-1-corpus.png" alt="cdm-corpus-example"></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>RNN-based Seq-to-Seq Model</p><h3 id="Adaptive-Attention-Model"><a href="#Adaptive-Attention-Model" class="headerlink" title="Adaptive-Attention Model"></a>Adaptive-Attention Model</h3><p>引入 Adaptive-Attention 的原因：</p><ul><li>vanilla attention 在每一步都会关于sememes</li><li>在生成definition的过程中，不是所有词都与sememes有关</li></ul><p>Adaptive-Attention</p><ul><li>利用 time-varying sememes 信息作为 sememe context</li><li>LM的信息作为 LM context<ul><li>首先，由 decoder 的 hidden state 和 上一时刻生成的definition词 通过线性映射和sigmoid运算得到 一个 gate 向量</li><li>再，对上一时刻的hidden state 进行tanh激活运算，通过 gated unit，得到 LM context</li></ul></li><li>再根据 context，引入一个新的attention，决定在生成当前时刻词是依赖 sememe context 还是 LM context</li></ul><h3 id="Self-and-Adaptive-Attention-Model"><a href="#Self-and-Adaptive-Attention-Model" class="headerlink" title="Self- and Adaptive-Attention Model"></a>Self- and Adaptive-Attention Model</h3><p>将 AAM 中的 RNN，换成 transformer</p><!-- ## Experiments --><!-- ## Summary & Analysis -->]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> note </tag>
            
            <tag> tg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AAAI2019 | Improving Question Answering by Commonsense-Based Pre-Training</title>
      <link href="/2019/07/06/paper-aaai2019-cs-based-pre-train/"/>
      <url>/2019/07/06/paper-aaai2019-cs-based-pre-train/</url>
      
        <content type="html"><![CDATA[<blockquote><p>AAAI,2019<br>中山大学，微软亚研</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>现有的神经网络模型不能很好的<strong>回答常识问题</strong>是由于<strong>缺乏concepts之间的常识联系</strong></li><li>回答有些问题需要模型有能力在常识知识上进行推理</li><li>回答这类的问题既需要词本身的知识又需要世界知识</li></ul><h2 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h2><ul><li>利用外部常识知识（ConceptNet）提高QA系统的常识推理能力</li><li>根据外部关于世界的常识知识预训练一个模型，对concepts之间的<strong>直接关系和间接关系</strong>进行<strong>预训练</strong><ul><li>预训练的functions可以轻松地加到神经网络中</li></ul></li><li>concept之间的关系可以分为<strong>直接</strong>和<strong>间接</strong><ul><li>可以学习<strong>两个</strong>度量每对concept之间直接和间接关系的functions</li></ul></li><li>好处：<ol><li>模型具有很大的concept/entity覆盖度</li><li>模型常识推理的能力不受限于训练实例的数量和不需要覆盖所有终端任务中需要的推理类型</li><li>易于扩展</li></ol></li><li>实验数据集：ARC / MCScripts</li></ul><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><ul><li>候选答案的打分函数由两部分组成<ul><li>$f(a_i) = \alpha f_{doc}(a_i) + \beta f_{cs}(a_i)$</li><li>分别是 document-based model 给出的打分 和 commonsense-based model 给出的打分</li><li>document-based model 同 yuanfudao 的 tri-attention</li></ul></li></ul><h3 id="Commonsense-based-Model"><a href="#Commonsense-based-Model" class="headerlink" title="Commonsense-based Model"></a>Commonsense-based Model</h3><ul><li>预训练知识表示的参数：</li><li><p>关系表示：</p><ul><li>两个concept之间的关系表示：$f_{cs}(c_1,c_2)=Enc(c_1) \odot Enc(c_2)$</li><li>concept encoder $Enc(\cdot)$ 的计算：<ul><li>$h^w(c) = BiLSTM(Emb(c))$</li><li>考虑到邻居节点：$h^n(c) = \sum_{c^\prime\in NBR(c)}(W^{r(c,c^\prime)} h^w(c^\prime) + b^{r(c,c^\prime)})$</li><li>$Enc(c) = [h^w(c);h^n(c)]$</li></ul></li><li>基于排序的损失函数：<ul><li>$l(c_1,c_2,c^\prime) = max(0, f_{cs}(c_1, c^\prime) - f_{cs}(c_1,c_2) + mgn)$</li><li>$c_1$和$c_2$是正例</li><li>$c_1$和$c^\prime$是负例</li><li>根据不同的策略对负例进行采样<ul><li>直接关系：直接根据kg中的邻接图进行采样</li><li>间接关系：拥有共同邻节点的作为正例，没有one-hop或two-hop关系的作为负例</li></ul></li></ul></li></ul></li><li><p>$f_{cs}(a_i)$函数是commonsense-based model 的打分函数：</p><ul><li>$f_{cs}(a_i) = \frac{1}{|E_1|} \sum_{x\in E_1} max_{y\in E_2}(f_{cs}(x,y))$<ul><li>max 表示选择$E_1$中最相关的concept</li></ul></li><li>其中<strong>$E_1$和$E_2$分别表示从问题句子Q和候选答案抽取出来的常识事实</strong></li><li>同样还可以计算从文章和候选答案抽取处理的常识事实<ul><li>对于P-Q对，为了保证和候选答案的相关性，去除不在候选答案抽取出来的知识集中的concept</li></ul></li><li>每个$E$是从知识库中抽取出的三元组</li></ul></li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>Topic：结合外部知识库或结构知识库的相关工作<br>这方面的工作可以分为两类，大部分属于第一类</p><ul><li>enhance each basic computational unit (word or noun phrase)<ol><li>Leveraging knowledge bases in lstms for improving machine reading</li><li>Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge.</li></ol></li><li>support external signals at the top layer before the model makes the final decision</li></ul><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><ul><li>cons:<ul><li>对文本和问题的建模和对知识的建模是分开的，通过最终的打分函数进行关联</li></ul></li><li>pros:<ul><li>通过区分直接关系和间接关系的采样来训练得到的知识表示向量效果更好</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cognition - Primary Research</title>
      <link href="/2019/06/11/mrc-research-cognitive/"/>
      <url>/2019/06/11/mrc-research-cognitive/</url>
      
        <content type="html"><![CDATA[<p>To Be Update</p>]]></content>
      
      
      <categories>
          
          <category> MRC-Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> research </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Recent Advances on Multi-Hop RC - WikiHop</title>
      <link href="/2019/06/06/paper-wikihop-1/"/>
      <url>/2019/06/06/paper-wikihop-1/</url>
      
        <content type="html"><![CDATA[<p>WikiHop in QAngaroo benchmark is a dataset for Multi-hop Reading Comprehension <strong>Across Documents</strong>.<br>Style: Multiple-Choice</p><p>Reference papers on WikiHop task:</p><blockquote><ol><li>Neural models for reasoning over multiple mentions using coreference. NAACL,2018.</li><li>Exploring graph-structured passage representation for multihop reading comprehension with graph neural networks. 2018.</li><li>Exploiting explicit paths for multihop reading comprehension. 2018.</li><li>BAG: Bi-directional Attention Entity Graph Convolutional Network for Multi-hop Reasoning Question Answering. NAACL,2019.</li><li>Question Answering by Reasoning Across Documents with Graph Convolutional Networks. NAACL,2019.</li><li>Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs. ACL,2019.</li><li>Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question Answering. ICLR,2019.</li><li>Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension. ACL,2019.</li></ol></blockquote><h2 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h2><p>Input: $&lt; q, P, C_q &gt;$</p><ul><li>query: $q$ in the form of triple without tail entity $&lt; h_e, r, ? &gt;$</li><li>a set of supporting documents: $P = \{P_1, …, P_M\}$</li><li>a set of candidate answers (all of which are entities mentioned in $P$): $C_q = \{C_1,…,C_N\}$</li></ul><p>Goal: </p><ul><li>select $a^{\star} \in C_q$, which is the entity that correctly answers the question</li><li>need to aggregate information from multiple evidences across documents</li></ul><h2 id="Coref-GRU"><a href="#Coref-GRU" class="headerlink" title="Coref-GRU"></a>Coref-GRU</h2><blockquote><p> <em>Neural models for reasoning over multiple mentions using coreference.</em><br> NAACL,2018.<br> Bhuwan Dhingra. (William W. Cohen)<br> CMU.</p></blockquote><h3 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>existing RNN layer are biased towards short-term dependencies</li></ul><p>This work:</p><ul><li>adapt a standard RNN layer by introducing a bias towards <strong>coreferent recency</strong></li></ul><h3 id="2-Model-Details"><a href="#2-Model-Details" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><ul><li>coreference relationships between words (Directed acyclic graph(DAG) style graph)</li><li>introduce a term in the update equations for GRU which depends on the hidden state of the coreferent antecedent of the current token/word<ul><li>hidden states are propagated along coreference chains and the original sequence in parallel</li></ul></li></ul><h2 id="MHQA-GCN-GRN"><a href="#MHQA-GCN-GRN" class="headerlink" title="MHQA-GCN/GRN"></a>MHQA-GCN/GRN</h2><blockquote><p><em>Exploring graph-structured passage representation for multihop reading comprehension with graph neural networks.</em><br>2018.<br>Linfeng Song. (Yue Zhang)<br>University of Rochester. (Westlake University)</p></blockquote><h3 id="1-Motivation-1"><a href="#1-Motivation-1" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>local coreference information is limited in providing information for rich inference and global evidence</li></ul><p>This work:</p><ul><li>form more complex graphs to better connecting global evidence</li><li>considering two more types of edges in addition to coreference<ul><li>same entity mentions (<strong>cross-document</strong>)</li><li>window-typed (<strong>within-document</strong>)<ul><li>two mentions of different entities within a context window</li></ul></li></ul></li></ul><h3 id="2-Model-Details-1"><a href="#2-Model-Details-1" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><ul><li>Encoding: evidence integration with graph network<ul><li>graph recurren network</li><li>graph convolutional network</li></ul></li><li>Prediction: summing the probabilities over all occurrences of the same entity mention<ul><li><script type="math/tex; mode=display">Pr_{\epsilon}=\frac{\sum_{k\in{C_q}}\alpha_k}{\sum_{k^{\prime} \in {C_q}} \alpha_{k^{\prime}}}</script></li><li><script type="math/tex; mode=display">\alpha_k = \frac{exp(e^k)}{\sum_{k^{\prime} \in C_q} exp(e^{k^{\prime}})}</script></li><li>$e^k$ is the representation of entity mention of $\epsilon_k$</li></ul></li></ul><h2 id="Path-based-Kundu-2018"><a href="#Path-based-Kundu-2018" class="headerlink" title="Path-based(Kundu.2018.)"></a>Path-based(Kundu.2018.)</h2><blockquote><p><em>Exploiting Explicit Paths for Multi-hop Reading Comprehension</em><br>2018</p></blockquote><h3 id="1-Motivation-2"><a href="#1-Motivation-2" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>graph-based models only <strong>implicitly</strong> <strong>combine</strong> knowledge from all the passages<ul><li>unable to provide explicit reasoning paths for the selected answer.</li></ul></li></ul><p>This work</p><ul><li>present a path-based reasoning approach for textual reading comprehension<ul><li>generating potential paths across multiple passages</li><li>extracting implicit relations along this path</li><li>composing relations to encode each path</li></ul></li></ul><h3 id="2-Model-Details-2"><a href="#2-Model-Details-2" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><ul><li>Path Define:<ul><li>only consider <strong>two-hop</strong> path: eg: $path_{kj}=h_e \rightarrow e_1 \rightarrow c_k$</li><li>$e_1$: intermediate entity and can be extended to multi-hop</li><li>an extracted path is <strong>a set of entity sequences</strong></li></ul></li><li>Path Extraction: for each candidate<ul><li>step #1: find a passage $P_1$ contains $h_e$ of the query</li><li>step #2: find intermediate entities: all Named Entities and Noun Phrases that appear in the same sentence with $h_e$ or in the subsequent sentence</li><li>step #3: find another passage $P_2$ containes any of the intermediate entities found in step#3<ul><li>for distinguishing the $e_1$ in different passage，use $e_1^{\prime}$ to stand for the same mention in the second passage</li></ul></li><li>step #4: check if passage $P_2$ contains any of the candidate answer choices</li></ul></li><li>Path Encoding:<ul><li>context-based path encoding<ul><li>use the concatenation of the boundary vectors of the passage encoding as the location encoding vector of entity<ul><li>$g_{e} = [s_{p_1,i_1};s_{p_1,i_2}]$</li></ul></li><li>extract implicit relation with a feed forward layer<ul><li>$r_{h_e,e_1}=FFL(g_{h_e}, g_{e_1})$</li><li>as well as $r_{e_1^{\prime}, c_k}$</li></ul></li><li>compose implicit relation vector with a feed forward layer<ul><li>$x_{ctx}=FFL(r_{h_e,e_1}, r_{e_1^{\prime},c_k})$</li></ul></li><li>feed forward layer $FFL(a,b)=tanh(aW_a + bW_b + bias)$</li></ul></li><li>passage-based path encoding<ul><li>question-weighted passage representation<ul><li>query-aware passage representation: $S_p^1$ and $S_p^2$</li></ul></li><li>aggregate passage representation: get single passage vector<ul><li>self-attention</li><li>$\tilde{s}_{p_1}$ and $\tilde{s}_{p_2}$ </li></ul></li><li>$x_{psg} = FFL(\tilde{s}_{p_1}, \tilde{s}_{p_2})$</li></ul></li></ul></li><li>Path Scorer:<ul><li>context-based path scoring<ul><li>$\tilde{q}=([q_0;q_L])W_q$</li><li>$y_{ctx,q}=FFL(x_{ctx},\tilde{q})$</li><li>$z_{ctx}=y_{ctx,q}W_{ctx}^T$</li></ul></li><li>passage-based path scoring<ul><li>self attention get single candidate answer choice vector $\tilde{c}_k$</li><li>$z_{psg}=\tilde{c}_k x_{psg}^T$</li></ul></li><li>unormalized score $z = z_{ctx} + z_{psg}$<ul><li>softmax over all the paths and candidates get $score(path_{kj})$</li></ul></li></ul></li><li>Prediction<ul><li>$prob(c_k)=\sum_j score(path_{kj})$</li></ul></li></ul><h2 id="BAG"><a href="#BAG" class="headerlink" title="BAG"></a>BAG</h2><blockquote><p><em>BAG: Bi-directional Attention Entity Graph Convolutional Network for Multi-hop Reasoning Question Answering.</em><br>NAACL,2019.</p></blockquote><h3 id="1-Motivation-3"><a href="#1-Motivation-3" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>comprehend the relationships of entities across documents before answering questions</li></ul><h3 id="2-Model-Details-3"><a href="#2-Model-Details-3" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><ul><li>Graph Construction<ul><li>node: all mentions of candidates</li><li>edge: undirected<ul><li>1)cross-document edge</li><li>2)within-document edge</li></ul></li></ul></li><li>Multi-Level Features:<ul><li>input node embedding</li><li>concatenation of GLoVe/ELMo(+linear)/NER/POS</li></ul></li><li>Query Encoding: BiLSTM + linear</li><li>Relational Graph Convolutional Network, same with Entity-GCN.</li><li>Bi-directional Attention Between a Graph and a Query<ul><li>similarity matrix $S = pooling_{mean}(f_a ([h_n; f_q; h_n \circ f_q] ))$<ul><li>$f_q$: query representation</li><li>$h_n$: all node representation</li></ul></li><li>directional computation is the same with BiDAF</li></ul></li><li>Prediction: the probability of each node becoming answer.<ul><li>the probability of each candidate is the sum of all corresponding nodes.</li></ul></li></ul><h2 id="Entity-GCN"><a href="#Entity-GCN" class="headerlink" title="Entity-GCN"></a>Entity-GCN</h2><blockquote><p><em>Question Answering by Reasoning Across Documents with Graph Convolutional Networks.</em><br>NAACL,2019.</p></blockquote><h3 id="1-Motivation-4"><a href="#1-Motivation-4" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><p>This work</p><ul><li>frame question answering as an inference problem on a graph representing the document collection.</li></ul><h3 id="2-Model-Details-4"><a href="#2-Model-Details-4" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><ul><li>Graph Construct<ul><li>node: mentions of candidate choices and head entity in the query</li><li>edge: <ul><li>co-occurrence in the same document</li><li>mentions that exactly match across document</li></ul></li></ul></li><li>Encoding<ul><li>ELMo: a concatenation of three 1024-dimensional vectors resulting in 3072-dimensional input vectors</li></ul></li><li>Graph Encoding: Relational GCN to model message passing process<ul><li>at layer $l$:</li><li>aggregation: aggregate information from neighbors of each node<ul><li><script type="math/tex; mode=display">z_i^l = \frac{1}{|N_i|} \sum_{j\in N_i} \sum_{r \in R_{ij}} f_r(h_j^l)</script></li><li>$N_i$ is the set of indices of nodes neighbouring $i$-th node</li><li>$R_{ij}$ is the set of edge annotations between $i$ and $j$</li></ul></li><li>combination<ul><li><script type="math/tex; mode=display">u_i^l = f_s(h_i^l) + z_i^l</script></li></ul></li><li>updating: how much of the update message propagates to the next step<ul><li><script type="math/tex; mode=display">g_i^l = sigmoid(f_g ( [z_i^l; h_i^l] ))</script></li><li><script type="math/tex; mode=display">h_i^{l+1} = tanh(u_i^l) \odot g_i^l + h_i^l \odot (1-g_i^l)</script></li></ul></li></ul></li><li>Prediction<ul><li>$Prob(c|q,C_q,P) \varpropto exp(max_{i\in M_c} f_o( [q, h_i^L] ) )$<ul><li>$M_c$ is the set of node indicate that $i \in M_c$ only if node $i$ is a mention of candidate choice $c$</li></ul></li></ul></li></ul><h2 id="Heterogeneous-Document-Entity"><a href="#Heterogeneous-Document-Entity" class="headerlink" title="Heterogeneous Document-Entity"></a>Heterogeneous Document-Entity</h2><blockquote><p><em>Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs.</em><br>ACL,2019.</p></blockquote><h3 id="1-Motivation-5"><a href="#1-Motivation-5" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>mainly compared with Entity-GCN</li><li>contains different granularity levels of information including candidates, documents and entities in speciﬁc document contexts</li></ul><p>This work</p><ul><li>design the Heterogeneous graph contains<ul><li>three kinds of nodes</li><li>seven types of edges</li></ul></li><li>include nodes corresponding to candidates, documents and entities.</li></ul><h3 id="2-Model-Details-5"><a href="#2-Model-Details-5" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><ul><li>Graph Construction<ul><li>node: <ul><li>1) candidate entity nodes</li><li>2) entity nodes extracted from documents</li><li>3) document nodes</li></ul></li><li>edge<ul><li>between (doc, entity)<ul><li>1) if the candidate appear in the document at least one time</li></ul></li><li>between (doc, entity)<ul><li>2) if the entity is extracted from the document</li></ul></li><li>between (entity, candidate)<ul><li>3) if the entity is a mention of the candidate</li></ul></li><li>between (entity, entity)<ul><li>4) if they are extracted from the same document</li><li>5) if they are mentions of the same candidate or query subject and they are extracted from different documents</li><li>7) entity nodes that do not meet previous conditions are connected</li></ul></li><li>between (cnadidate, candidate)<ul><li>6) all candidate nodes connect with each other</li></ul></li></ul></li></ul></li><li>Graph Encoding<ul><li>Relational GCN, the same with Entity-GCN</li></ul></li><li>Prediction<ul><li>$a = f_C(H^C) + ACC_{max}(f_E(H^E)$</li><li>$H^C$ : node representations of all candidate nodes </li><li>$H^E$: node representations of all entity nodes that correspond to candidates</li><li>$ACC_{max}$: max pooling of entites belong to the same candidate</li><li>$f(\cdot)$: two-layers MLP with tanh</li></ul></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul><li>related works can be categorized as follows:<ul><li>graph-based<ul><li>coreference</li><li>co-occurrence</li><li>heterogeneous</li></ul></li><li>path-based</li><li>neural network based</li></ul></li><li>official leaderboard: <a href="http://qangaroo.cs.ucl.ac.uk/leaderboard.html" target="_blank" rel="noopener">http://qangaroo.cs.ucl.ac.uk/leaderboard.html</a></li></ul><div class="table-container"><table><thead><tr><th>Models</th><th>UnMask<br>Dev</th><th>UnMaks<br>Test</th><th>Mask<br>Dev</th><th>Maks<br>Test</th></tr></thead><tbody><tr><td>BiDAF</td><td>49.7</td><td>42.9</td><td>59.8</td><td>-</td></tr><tr><td>Coref-GRU</td><td>56.0</td><td>59.3</td><td>-</td><td>-</td></tr><tr><td>MHQA-GCN</td><td>62.6</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MHQA-GRN</td><td>62.8</td><td>65.4</td><td>-</td><td>-</td></tr><tr><td>Entity-GCN</td><td>64.8</td><td>67.6</td><td>-</td><td>-</td></tr><tr><td>CFC</td><td>66.4</td><td>70.6</td><td>-</td><td>-</td></tr><tr><td>Kundu.2018</td><td>67.1</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BAG</td><td>66.5</td><td>69</td><td>70.9</td><td>68.9</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> multi-hop </tag>
            
            <tag> gnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Recent Advances on Multi-Hop RC - HotpotQA</title>
      <link href="/2019/05/30/mrc-paper-hotpotqa/"/>
      <url>/2019/05/30/mrc-paper-hotpotqa/</url>
      
        <content type="html"><![CDATA[<p><strong>HotpotQA</strong> is a dataset for <strong>Diverse</strong>, <strong>Explainable</strong> <strong>Multi-hop</strong> Question Answering.<br>Style: Extractive-based</p><p>HotpotQA has 4 features, which can also be seen as challenges:</p><ul><li>questions require finding and reasoning over multiple supporting documents to answer</li><li>questions are diverse and not constrained to any pre-existing knowledge</li><li>contains two types of questions: <ul><li>1.<strong>bridge</strong></li><li>2.<strong>comparison</strong> (factoid comparision questions)</li></ul></li><li>provides sentence-level supporting facts required for reasoning<ul><li>introduces the <strong>strong supervision</strong> for reasoning and make the final predictions <strong>explainable</strong></li><li>(can form a reasoning chain through the support facts)</li></ul></li></ul><p>Reference papers on HotpotQA task:</p><blockquote><ol><li>Cognitive Graph for Multi-Hop Reading Comprehension at Scale. ACL, 2019.</li><li>Dynamically Fused Graph Network for Multi-Hop Reasoning. ACL, 2019.</li><li>Answering while Summarizing: Multi-task Learning for Multi-Hop QA with Evidence Extraction. ACL, 2019.</li><li>Compositional Questions Do Not Necessitate Multi-hop Reasoning. ACL,2019. (short)</li><li>Multi-hop Reading Comprehension through Question Decomposition and Rescoring. ACL,2019.</li></ol></blockquote><h2 id="Task-Defination"><a href="#Task-Defination" class="headerlink" title="Task Defination"></a>Task Defination</h2><p>Input:</p><ul><li>query</li><li>$N_p = 10$ paragraphs</li></ul><p>Output</p><ul><li>answer span</li><li>supprot facts (sentence), which can be regarded as the supervised signal during training</li></ul><h2 id="Cognitive-Graph-QA"><a href="#Cognitive-Graph-QA" class="headerlink" title="Cognitive Graph QA"></a>Cognitive Graph QA</h2><blockquote><p>Cognitive Graph for Multi-Hop Reading Comprehension at Scale.<br>ACL, 2019.<br>Ming Ding (Chang Zhou)<br>THU (DAMO, Alibaba)</p></blockquote><h3 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><p>Dual Process Theory from the cognitive process of humans:</p><ul><li>System1 first retrieve relevant information following attention via an <strong>implicit, unconscious, intuitive</strong> process<ul><li>efficiently provides resources according to requests</li></ul></li><li>System2 conduct <strong>explicit, conscious, controllable</strong> reasoning process based on the result of System1<ul><li>enable diving deeper into relational information by performing sequential thinking in the working memory</li><li>slower but human-unique rationality</li></ul></li><li>For complex reasoning, two systems are coordinated to perform fast and slow thinking iteratively</li></ul><p>This work:</p><ul><li>System1 extracts question-relevant entities and answer candidates from paragraphs and encodes their semantic information<ul><li>Extracted entities are organized as a cognitive graph —&gt; working memory</li></ul></li><li>System2 conducts the reasoning procedure over the graph, and collects clues to guide System 1 to better extract next-hop entities.</li><li>iterate until all possible answers are found, and then the final answer is chosen based on reasoning results from System 2.</li></ul><h3 id="2-Model-Details"><a href="#2-Model-Details" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><p>Algorithm of Cognitive Graph QA</p><ul><li><img src="/images/mrc-paper-hotpotqa/Cog-algo.png" alt="alog"></li></ul><p>Model Overview</p><ul><li><img src="/images/mrc-paper-hotpotqa/Cog-model.png" alt="model"></li></ul><h3 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3.Experiments"></a>3.Experiments</h3><p>case study</p><ul><li><img src="/images/mrc-paper-hotpotqa/Cog-case.png" alt="case-study"></li></ul><h2 id="Dynamically-Fused-Graph-Network"><a href="#Dynamically-Fused-Graph-Network" class="headerlink" title="Dynamically Fused Graph Network"></a>Dynamically Fused Graph Network</h2><blockquote><p>Dynamically Fused Graph Network for Multi-Hop Reasoning.<br>ACL,2019.<br>Yunxuan Xiao<br>Shanghai Jiao Tong University. (ByteDance AT Lab)</p></blockquote><h3 id="1-Motivation-1"><a href="#1-Motivation-1" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><p><i class="fa fa-thumbtack"></i> Challenges:</p><ul><li>1) filtering out noises from multiple paragraphs and extracting useful information<ul><li>previous work: build entity graph from input paragraphs and apply GNN to aggregate the information</li><li>shortcomings: static global entity graph of each QA pair —&gt; <strong>implicit reasoning, lack of explainability</strong></li></ul></li><li>2) aggregate document information to an entity graph and answers are then <strong>directly</strong> selected on entities of the entity graph<ul><li>shortcomings: answers may not reside in entities of the extracted entity graph</li></ul></li></ul><p><i class="fa fa-highlighter"></i> This work:</p><ul><li>intuition: mimic human reasoning process in multi-hop QA<ul><li>start from an entity of interest in the query</li><li>focus on the words surrounding the start entities</li><li>connect to some related entity in the neighborhood guided by the question</li><li>repeat the step to form a reasoning chain, and lands on some entity or snippets likely to be answer.</li></ul></li><li>consturcts <strong>dynamic</strong> entity graph</li><li>propagating information on dynamic entity graph under soft-mask constraint</li><li><strong>bidirectional fusion</strong>:<ul><li>aggregate information from document to the entity graph and entity graph to document</li></ul></li></ul><h3 id="2-Model-Details-1"><a href="#2-Model-Details-1" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><h3 id="2-1-Graph-Constructing"><a href="#2-1-Graph-Constructing" class="headerlink" title="2.1 Graph Constructing"></a>2.1 Graph Constructing</h3><p>node: POL (<code>Persion</code>, <code>Organization</code>, <code>Location</code>) entities<br>edge:</p><ul><li>1) sentence-level: co-occurence</li><li>2) context-level: coreference</li><li>3) paragraph-level: link with central entities extracted from title sentence of each paragraph</li></ul><p><img src="/images/mrc-paper-hotpotqa/DFGN-graph.png" alt="dfgn-graph"></p><h3 id="2-2-Model"><a href="#2-2-Model" class="headerlink" title="2.2 Model"></a>2.2 Model</h3><p><img src="/images/mrc-paper-hotpotqa/DFGN-model.png" alt="dfgn-model"></p><p>Paragraph Selector</p><ul><li>use BERT sentence classification: [Q , Pi]</li><li>concat all selected Pi —&gt; C</li></ul><p>Encoder</p><ul><li>use BERT encoding concatenating of query and context performs better</li><li>output:<ul><li>query $Q_0 \in \mathbb{R}^{L\times d_2}$</li><li>context $C_0 \in \mathbb{R}^{M\times d_2}$</li></ul></li></ul><p><strong>Fusion Block</strong></p><ul><li>document to graph<ul><li>binary Matrix $M \in \mathbb{R}^{M \times N}$ : get text span associated with an entity</li><li>entity embedding: mean-max pooling $E_{t-1} = [e_{t-1,1},…,e_{t-1,N}] \in \mathbb{R}^{2d_2 \times N}$</li></ul></li><li>dynamic graph attention<ul><li>soft-mask: signify the start entities in the t-th reasoning step<ul><li>query vector $\tilde{q}^{t-1} = MeanPooling(Q^{t-1})$</li><li>masked entity $\tilde{E}^{t-1} = [m_1^t e_1^{t-1},…]$<ul><li><script type="math/tex; mode=display">\gamma_i^t = \tilde{q}^{t-1} V^t e_t^{t-1} / \sqrt{d_2}</script></li><li><script type="math/tex; mode=display">m^t = \sigma([\gamma_1^t,...,\gamma_N^t])</script></li></ul></li></ul></li><li>propagate information in graph by GAT (the more relevant to the query, the neighbor nodes receive more information from nearby)<ul><li>$e^t_i = ReLu( \sum_{j\in B_i} \alpha_{j,i}^t h_j^t)$<ul><li><script type="math/tex; mode=display">h_i^t = U_t \tilde{e}_i^{t-1} + b_t</script></li><li><script type="math/tex; mode=display">\beta_{i,j}^t = LeakyReLu(W_t^T [h_i^t, h_j^t])</script></li><li><script type="math/tex; mode=display">\alpha_{i,j}^t = \frac{exp(\beta_{i,j}^t)}{\sum_k exp(\beta_{i,k}^t)}</script></li></ul></li><li>$E_t = [e_1^t,…,e_N^t]$</li></ul></li></ul></li><li>updating query<ul><li>$Q^t = BiAttention(Q^{t-1}, E^t)$</li></ul></li><li>graph to document<ul><li>issue: the unrestricted answer still cannot be back traced</li><li>keep information flowing from entity back to tokens in the context<ul><li>the same matrix $M$</li><li>update the context embedding</li></ul></li><li>$C^t = LSTM( [C^{t-1}, {ME^t}^T] ) \in \mathbb{R}^{M\times d_2}$</li></ul></li></ul><p>Prediction Layer</p><ul><li>4 targets<ul><li>supporting sentences</li><li>start position of answer span</li><li>end position of answer span</li><li>answer type</li></ul></li><li>use four LSTMs to get final representation<ul><li>$O_{sup} = F_0 (C^t)$</li><li>$O_{start} = F_1([C^t, O_{sup}])$</li><li>$O_{end} = F_2([C^t, O_{sup}, O_{start}])$</li><li>$O_{type} = F_3([C^t, O_{sup}, O_{end}])$</li><li>$L = L_{start} + L_{end} + \lambda_s L_{sup} + \lambda_t L_{type}$</li></ul></li></ul><h3 id="3-Experiments-1"><a href="#3-Experiments-1" class="headerlink" title="3.Experiments"></a>3.Experiments</h3><h2 id="Query-Focused-Extractor"><a href="#Query-Focused-Extractor" class="headerlink" title="Query Focused Extractor"></a>Query Focused Extractor</h2><blockquote><p>Answering while Summarizaing: Multi-task Learning for Multi-hop QA with Evidence Extraction<br>ACL,2019.<br>Kosuke Nishida.<br>NTT Media Intelligence Laboratories.</p></blockquote><h3 id="1-Motivation-2"><a href="#1-Motivation-2" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><h3 id="2-Model-Details-2"><a href="#2-Model-Details-2" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><h3 id="3-Experiments-2"><a href="#3-Experiments-2" class="headerlink" title="3.Experiments"></a>3.Experiments</h3><h2 id="DecompRC"><a href="#DecompRC" class="headerlink" title="DecompRC"></a>DecompRC</h2><blockquote><p>Multi-hop Reading Comprehension through Question Decomposition and Rescoring<br>ACL,2019.<br>Sewon Min.<br>University of Washington. AI2.</p></blockquote><h3 id="1-Motivation-3"><a href="#1-Motivation-3" class="headerlink" title="1.Motivation"></a>1.Motivation</h3><ul><li>decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models<ul><li>inspired by the idea of compositionality from semantic parsing</li></ul></li></ul><h3 id="2-Model-Details-3"><a href="#2-Model-Details-3" class="headerlink" title="2.Model Details"></a>2.Model Details</h3><p>3 step process:</p><ul><li>decomposes the original, multi-hop question into several single-hop sub-questions according to a few reasoning types in parallel, based on span predictions.</li><li>for every reasoning types D ECOMP RC leverages a single-hop reading comprehension model to answer each sub-question, and combines the answers according to the reasoning type.</li><li>leverages a decomposition scorer to judge which decomposition is the most suitable, and outputs the answer from that decomposition as the ﬁnal answer.</li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> multi-hop </tag>
            
            <tag> gnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Commonsense Reasoning for Natural Language Understanding - A Survey of Benchmarks, Resources, and Approachs</title>
      <link href="/2019/04/18/mrc-cs-reasoning-for-nlu-survey/"/>
      <url>/2019/04/18/mrc-cs-reasoning-for-nlu-survey/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Authors: Shane Storks, Qianzi Gao, Joyce Y. Chai<br>Org.: Department of Computers Science and Engineering, Michigan State University<br>Year: 2019</p></blockquote><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Commonsense Knowledge (CS Know.) 和 Commonsense Reasoning 是机器智能的两大重要瓶颈。</p><p>现有的NLP研究中，已经提出了一些需要常识推理的benchmarks和tasks，旨在评估机器获得和学习常识知识的能力。</p><p>这篇文章的主要目的是针对NLU的常识推理，提供关于以下四个方面的一个综述：</p><ul><li>现有的任务和benchmarks</li><li>Knowledge Resources</li><li><p>Learning and Inference Approachs</p></li><li><p>关于本篇文章的思维导图：</p><ul><li><img src="/images/mrc-cs-reasoning-for-nlu-survey/mind.png" alt="mind-note"></li></ul></li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>Davis and Marcus (2015)<sup><a href="#fn_davis" id="reffn_davis">davis</a></sup> 指出常识推理的挑战：spans from difficulties in <strong>understanding and formulating commonsense knowledge for specific or general domains</strong> to complexities in <strong>various forms of reasoning and their integration for problem solving</strong>.</p><p>现有的研究主要关注如下图所示的几个方面：(在这篇文章中主要关注文本数据源)</p><p><img src="/images/mrc-cs-reasoning-for-nlu-survey/overview.png" alt="image-overview"></p><h2 id="2-Benchmarks-and-Tasks"><a href="#2-Benchmarks-and-Tasks" class="headerlink" title="2.Benchmarks and Tasks"></a>2.Benchmarks and Tasks</h2><p>这章主要介绍一些需要常识推理的benchmarks，以及对构建这类benchmarks的重要要求进行一个总结。</p><ul><li>benchmarks数据集的发展：<ul><li><img src="/images/mrc-cs-reasoning-for-nlu-survey/benchmark-trend.png" alt="benchmark-trend"></li></ul></li></ul><h3 id="2-1-Overivew-of-ExistingBenchmarks"><a href="#2-1-Overivew-of-ExistingBenchmarks" class="headerlink" title="2.1 Overivew of ExistingBenchmarks"></a>2.1 Overivew of ExistingBenchmarks</h3><p>很多常识benchmark数据集都是基于classic language processing问题建立起的，从focused task (共指消解、命名实体识别) 到更理解性的任务和应用。</p><p>Benchmarks不应该局限于需要 language processing 能力提升性能的类型，应该更有针对性，更关注某类的常识知识和推理 (或是某几类的混合) 。</p><p>将Benchmarks分为6类，分别展开介绍</p><h4 id="2-1-1-Coreference-Resolution"><a href="#2-1-1-Coreference-Resolution" class="headerlink" title="2.1.1 Coreference Resolution"></a>2.1.1 Coreference Resolution</h4><p>共指消解是NLU中的一个基本任务，在句子中出现<strong>多个代名词或明显复杂的过程</strong>时，需要常识知识确定决策。</p><ul><li>代表数据集：Winograd Schema Challenge (<a href="http://commonsensereasoning.org/winograd.html" target="_blank" rel="noopener">link</a>)</li></ul><h4 id="2-1-2-Question-Answering"><a href="#2-1-2-Question-Answering" class="headerlink" title="2.1.2 Question Answering"></a>2.1.2 Question Answering</h4><p>相比于只关注某些特定的语言处理或是推理的任务，QA提供了一种在单个任务中更全面地混合语言处理和推理技巧的benchmark。</p><blockquote><p>contain questions requiring commonsense knowledge alongside question requiring comprehension of a given text.</p></blockquote><ul><li>代表数据集：ARC、MCScript、ProPara、MultiRC、SQuADv2、CoQA、OpenBookQA、CommonsenseQA<ul><li>ProPara：面向过程性文本，旨在学习目标的追踪和状态变化</li></ul></li></ul><h4 id="2-1-3-Textual-Entailment"><a href="#2-1-3-Textual-Entailment" class="headerlink" title="2.1.3 Textual Entailment"></a>2.1.3 Textual Entailment</h4><p>文本推理任务旨在推理两个句子之间的关系，需要多种语言处理能力（paraphrase）以及object tracking、causal reasoning和常识知识。</p><ul><li>代表数据集：RTE challenges、SICK、SNLI、SciTail</li><li>RTE knowledge resources: <a href="https://aclweb.org/aclwiki/RTE_Knowledge_Resources" target="_blank" rel="noopener">https://aclweb.org/aclwiki/RTE_Knowledge_Resources</a></li></ul><h4 id="2-1-4-Plausible-Inference"><a href="#2-1-4-Plausible-Inference" class="headerlink" title="2.1.4 Plausible Inference"></a>2.1.4 Plausible Inference</h4><p>似然推理：require hypothetical, intermediate or uncertain conclusions defined as plausible inference.</p><p>这类数据集关注的是everyday events和interactions，包含各种的实际的常识关系。</p><ul><li>代表数据集：COPA、CBT、ROCStories、JOCI、CLOTH、SWAG、ReCoRD</li></ul><h4 id="2-1-5-Psychological-Reasoning"><a href="#2-1-5-Psychological-Reasoning" class="headerlink" title="2.1.5 Psychological Reasoning"></a>2.1.5 Psychological Reasoning</h4><p>心理推理：关于情绪（情感）和意图的推理，需要社会心理学的常识知识</p><ul><li>代表数据集：Triangle-COPA、Story Commonsense、Event2Mind<ul><li>StoryCommonsense<sup><a href="#fn_rashkin-2018a" id="reffn_rashkin-2018a">rashkin-2018a</a></sup>：要求预测Motivation和Emotions，以及Maslow (human need)、Reiss (human motives)、Plutchik (emotions)<ul><li>link：<a href="http://uwnlp.github.io/storycommonsense" target="_blank" rel="noopener">http://uwnlp.github.io/storycommonsense</a></li><li>Theories of Motivation (Maslow and Reiss) and Emotional Reaction (Plutchik): <img src="/images/mrc-cs-reasoning-for-nlu-survey/MaslowReissPlutchik.png" alt="png"></li></ul></li><li>Event2Mind<sup><a href="#fn_rashkin-2018b" id="reffn_rashkin-2018b">rashkin-2018b</a></sup>：推理关于事件的intentions和reactions，每个事件都有1到2个参与者，三个任务：预测主要参与者的意图和反应，并预测其他人的反应<ul><li>link：<a href="http://uwnlp.github.io/event2mind" target="_blank" rel="noopener">http://uwnlp.github.io/event2mind</a></li></ul></li></ul></li></ul><h4 id="2-1-6-Multiple-Tasks"><a href="#2-1-6-Multiple-Tasks" class="headerlink" title="2.1.6 Multiple Tasks"></a>2.1.6 Multiple Tasks</h4><p>consist of several focused language processing or reasoning tasks so that reading comprehension skills can be learned one by one in a consistent format</p><ul><li>代表数据集：bAbI、IIE、GLUE、DNC<ul><li>IIE：Inference is Everything，RTE的形式</li><li>DNC <sup><a href="#fn_poliak" id="reffn_poliak">poliak</a></sup>：Diverse Natural Language Inference Collection，包含9个NLI任务需要7中不同类型的推理<ul><li>recast from：<ul><li>Event Factuality, recast from UW (Lee, Artzi, Choi, &amp; Zettlemoyer, 2015), MEANTIME (Minard, Speranza, Urizar, Altuna, van Erp, Schoen, &amp; van Son, 2016), and (Rudinger, White, &amp; Van Durme, 2018b)</li><li>Named Entity Recognition, recast from the Groningen Meaning Bank (Bos, Basile, Evang, Venhuizen, &amp; Bjerva, 2017) and the ConLL-2003 shared task (Tjong Kim Sang &amp; De Meulder, 2003)</li><li>Gendered Anaphora Resolution, recast from the Winogender dataset (Rudinger et al., 2018a)</li><li>Lexicosyntactic Inference, recast from MegaVeridicality (White &amp; Rawlins, 2018), VerbNet (Schuler, 2005), and VerbCorner (Hartshorne, Bonial, &amp; Palmer, 2013)</li><li>Figurative Language, recast from puns by Yang, Lavie, Dyer, and Hovy (2015) and Miller, Hempelmann, and Gurevych (2017)</li><li>Relation Extraction, partially from FACC1 (Gabrilovich, Ringgaard, &amp; Subramanya, 2013)</li><li>Subjectivity, recast from Kotzias, Denil, De Freitas, and Smyth (2015)</li></ul></li><li>link：<a href="http://github.com/decompositional-semantics-initiative/DNC" target="_blank" rel="noopener">http://github.com/decompositional-semantics-initiative/DNC</a></li></ul></li></ul></li></ul><h3 id="2-2-Criteria-and-Consideration-for-Creating-Benchmarks"><a href="#2-2-Criteria-and-Consideration-for-Creating-Benchmarks" class="headerlink" title="2.2 Criteria and Consideration for Creating Benchmarks"></a>2.2 Criteria and Consideration for Creating Benchmarks</h3><h4 id="2-2-1-Task-Format"><a href="#2-2-1-Task-Format" class="headerlink" title="2.2.1 Task Format"></a>2.2.1 Task Format</h4><p>决定任务形式对于Benchmarks的创建是重要的一步，现有的任务形式有三类：</p><ul><li>Classification Task：有三种形式<ul><li>Textual Entailment Task</li><li>Cloze Task</li><li>Traditional Multiple-Choice Task</li></ul></li><li>Open-ended Task：开放式任务<ul><li>Span</li><li>Subset of category labels：Story Commonsense</li><li>Purely open-ended：Event2Mind、bAbI</li><li><del>Generative</del></li></ul></li></ul><h4 id="2-2-2-Evaluation-Schemes"><a href="#2-2-2-Evaluation-Schemes" class="headerlink" title="2.2.2 Evaluation Schemes"></a>2.2.2 Evaluation Schemes</h4><p>评测形式</p><p>现有的评测结果都是直接给出是否通过(pass or fail grade)，没有任何反馈</p><p>理想的评测形式应该考虑有信息的指标，可以比较不同的方法，比较机器和人之间性能表现的差异</p><ul><li>Evaluation Metrics：Precision、Recall、F-Measure、Exact-Match、Recall@k、BLEU、ROUGE</li><li>Comparison of Approaches</li><li>Human Performance Measurement</li></ul><h4 id="2-2-3-Data-Biases"><a href="#2-2-3-Data-Biases" class="headerlink" title="2.2.3 Data Biases"></a>2.2.3 Data Biases</h4><p>数据分布的平衡</p><ul><li>Label Distribution Bias</li><li>Question Type Bias</li><li>Superficial Correlation Bias：gender bias</li></ul><h4 id="2-2-4-Collection-Methods"><a href="#2-2-4-Collection-Methods" class="headerlink" title="2.2.4 Collection Methods"></a>2.2.4 Collection Methods</h4><p>[Not Focus]</p><ul><li>Manual versus Automatic Generation</li><li>Automatic Generation versus Text Mining</li><li>Crowsourcing Considerations</li></ul><h2 id="3-Knowledge-Resources"><a href="#3-Knowledge-Resources" class="headerlink" title="3.Knowledge Resources"></a>3.Knowledge Resources</h2><h3 id="3-1-Overview-of-Knowledge-Resources-for-NLU"><a href="#3-1-Overview-of-Knowledge-Resources-for-NLU" class="headerlink" title="3.1 Overview of Knowledge Resources for NLU"></a>3.1 Overview of Knowledge Resources for NLU</h3><p>为了理解自然语言，通常需要语言学知识来却确定文本的句法、语义结构，再进一步使用通识、常识知识来增强对结构的理解，以达到更全面的理解</p><h4 id="3-1-1-Linguistic-Knowledge-Resources"><a href="#3-1-1-Linguistic-Knowledge-Resources" class="headerlink" title="3.1.1 Linguistic Knowledge Resources"></a>3.1.1 Linguistic Knowledge Resources</h4><p>带标记的句法、语义、篇章结构资源</p><ul><li>Annotated Linguistic Corpora<ul><li>Penn TreeBank：POS tags &amp; syntactic structures based on context-free grammar</li><li>PropBank：predicate-argument structures</li><li>Penn Discourse TreeBank</li><li>Abstract Meaning Representation (AMR)</li></ul></li><li>Lexical Resources<ul><li>WordNet</li><li>VerbNet：hierarchical English Verb lexicon</li><li>FrameNet：frame semantics for a set of verbs</li></ul></li></ul><h4 id="3-1-2-Common-Knowledge-Resources"><a href="#3-1-2-Common-Knowledge-Resources" class="headerlink" title="3.1.2 Common Knowledge Resources"></a>3.1.2 Common Knowledge Resources</h4><blockquote><p><strong><font color="blue">Common knowledge refers to speciﬁc facts about the world that are often explicitly stated.</font></strong></p></blockquote><p>与Commonsense Knowledge的不同是<sup><a href="#fn_cambria-2011" id="reffn_cambria-2011">cambria-2011</a></sup>：CS Know. required to achieve a deep understanding of both the low- and high-level concepts found in language.</p><ul><li>Yet Another Great Ontology (YAGO): with common knowledge facts extracted from Wikipedia, converting WordNet from a primarily linguistic resource to a common knowledge base.</li><li>DBpedia: Wikipedia-based knowledge base originally consisting of structured knowledge from more than 1.95 million Wikipedia articles.</li><li>WikiTaxonomy: consists of about 105,000 well-evaluated semantic links between categories in Wikipedia articles. Categories and relationships are labeled using the connectivity of the conceptual network formed by the categories.</li><li>Freebase</li><li>NELL</li><li>Probase</li></ul><h4 id="3-1-3-Commonsense-Knowledge-Resources"><a href="#3-1-3-Commonsense-Knowledge-Resources" class="headerlink" title="3.1.3 Commonsense Knowledge Resources"></a>3.1.3 Commonsense Knowledge Resources</h4><blockquote><p><strong><font color="blue">Commonsense knowledge, on the other hand, is considered obvious to most humans, and not so likely to be explicitly stated</font></strong></p><p>Cambria, E., Song, Y., Wang, H., &amp; Hussain, A. (2011). Isanette: A Common and Common Sense Knowledge Base for Opinion Mining. In 2011 IEEE 11th International Conference on Data Mining Workshops, pp. 315–322, Vancouver, BC, Canada. IEEE.</p></blockquote><ul><li>Cyc</li><li>ConceptNet</li><li>AnalogySpace<ul><li>is an algorithm for reducing the dimensionality of commonsense knowledge so that knowledge bases can be more efﬁciently and accurately reasoned over.</li></ul></li><li>SenticNet: intended for sentiment analysis</li><li>IsaCore: a set of “is a” relationships and conﬁdences.<ul><li><a href="http://sentic.net/downloads" target="_blank" rel="noopener">http://sentic.net/downloads</a></li></ul></li><li>COGBASE</li><li>WebChild</li><li>LocatedNear</li><li>Atlas of Machine Commonsense (ATOMIC)<ul><li>about 300,000 nodes corresponding to short textual descriptions of events, and about 877,000 “if-event-then” triples representing if-then relationships between everyday events.</li></ul></li></ul><h3 id="3-2-Approaches-to-Creating-Knowledge-Resources"><a href="#3-2-Approaches-to-Creating-Knowledge-Resources" class="headerlink" title="3.2 Approaches to Creating Knowledge Resources"></a>3.2 Approaches to Creating Knowledge Resources</h3><blockquote><p>The goal is to create general knowledge bases to <strong>provide inductive bias for a variety of learning and reasoning tasks</strong></p></blockquote><ul><li>Manual Encoding</li><li>Text Mining</li><li>Crowdsourcing</li></ul><h2 id="4-Learning-and-Inference-Approaches"><a href="#4-Learning-and-Inference-Approaches" class="headerlink" title="4.Learning and Inference Approaches"></a>4.Learning and Inference Approaches</h2><h3 id="4-1-Symbolic-and-Statistical-Approaches"><a href="#4-1-Symbolic-and-Statistical-Approaches" class="headerlink" title="4.1 Symbolic and Statistical Approaches"></a>4.1 Symbolic and Statistical Approaches</h3><h3 id="4-2-Neural-Approaches"><a href="#4-2-Neural-Approaches" class="headerlink" title="4.2 Neural Approaches"></a>4.2 Neural Approaches</h3><ul><li>common components in neural models: <ul><li><img src="/images/mrc-cs-reasoning-for-nlu-survey/neural-approachs.png" alt="neural"></li></ul></li></ul><h4 id="4-2-1-Memory-Augmentation"><a href="#4-2-1-Memory-Augmentation" class="headerlink" title="4.2.1 Memory Augmentation"></a>4.2.1 Memory Augmentation</h4><p>针对需要理解状态变化或是具有多个支撑事实来进行文本理解的任务</p><h5 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h5><ul><li>add a long-term memory component to track the world state and context</li><li>MemNet can efficiently leverage a wider context in making inferences<ul><li>outperform primarily RNN and LSTM based models</li></ul></li></ul><h5 id="Recurrent-Entity-Networks-ENTENT"><a href="#Recurrent-Entity-Networks-ENTENT" class="headerlink" title="Recurrent Entity Networks (ENTENT)"></a>Recurrent Entity Networks (ENTENT)</h5><blockquote><p>Henaff, M., Weston, J., Szlam, A., Bordes, A., &amp; LeCun, Y. (2017).<br>Tracking the World State with Recurrent Entity Networks. In Proceedings of the 5th International Conference on Learning Representations.<br>ICLR,2017.</p></blockquote><ul><li>composed of several dynamic memory cell<ul><li>each cell learns to represent the <strong>state</strong> or <strong>properties</strong> concerning entities mentioned in the input.</li><li>each cell is a Gated-RNN, only updates its content when new information relevant to the particular entity is received</li><li>run in parallel, allow multiple locations of memory to be updated at the same time.</li></ul></li><li>unlike MemNet:<ul><li>MemNet only preform reasoning when the entire supporting text and the question are processed and loaded to the memory.</li><li>when given a supporting text with multiple questions:<ul><li>ENTENT do not need to process the input text multiple times to answer these question.</li><li>MemNet need to re-process the whole input for each question.</li></ul></li></ul></li><li>drawbacks<ul><li>perform well in bAbI, but not in ProPara</li><li>maintain memory registers for entities, it has no separate embedding for individual states of entities over time</li><li>do not explicitly update coreferences in memory</li></ul></li></ul><h5 id="KG-MRC"><a href="#KG-MRC" class="headerlink" title="KG-MRC"></a>KG-MRC</h5><blockquote><p>Das, R., Munkhdalai, T., Yuan, X., Trischler, A., &amp; McCallum, A.<br>Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension.<br>ICLR, 2019</p></blockquote><p>Knowledge Graph Machine Reading Comprehension</p><ul><li>maintain a dynamic memory<ul><li>memory is in the form of knowledge graphs generated after every sentence of procedural text.</li></ul></li><li>generated knowledge graphs:<ul><li>are bipartite, connecting entities in the paragraph with their locations (currently, only capture the location relation)</li><li>connections between entities and locations are updated to generate a new graph after each sentence</li></ul></li><li>KG-MRC learns some commonsense constraints automatically.<ul><li>recurrent graph representations help.</li></ul></li></ul><h4 id="4-2-2-Attention-Mechanism"><a href="#4-2-2-Attention-Mechanism" class="headerlink" title="4.2.2 Attention Mechanism"></a>4.2.2 Attention Mechanism</h4><ul><li>automatically provides an alignment between inputs and outputs</li><li>have limitations when the alignment between inputs and outputs is not straightforward.</li><li>sequentail attention</li><li>self-attention</li><li>multi-head</li><li>comparison score function</li></ul><h4 id="4-2-3-Pre-Trained-Models-and-Representations"><a href="#4-2-3-Pre-Trained-Models-and-Representations" class="headerlink" title="4.2.3 Pre-Trained Models and Representations"></a>4.2.3 Pre-Trained Models and Representations</h4><ul><li>ELMO</li><li>GPT</li><li>BERT<ul><li>still far from human: <del>SciTail</del>、ReCoRD、OpenBookQA</li></ul></li><li>When to fine-tune<ul><li>sentence pair tasks</li></ul></li></ul><h3 id="4-3-Incorporating-External-Knowledge"><a href="#4-3-Incorporating-External-Knowledge" class="headerlink" title="4.3 Incorporating External Knowledge"></a>4.3 Incorporating External Knowledge</h3><ul><li>WordNet in Textual Entailment</li><li>ConceptNet in Commonsense Task</li><li>Main Problems：<ul><li>how to incorporate external knowledge in modern neural approaches</li><li>how to acquire relevant external knowledge</li></ul></li></ul><h2 id="5-Other-Related-Benchmarks"><a href="#5-Other-Related-Benchmarks" class="headerlink" title="5.Other Related Benchmarks"></a>5.Other Related Benchmarks</h2><ul><li>language-related tasks</li><li>visual benchmarks<ul><li>perception</li></ul></li></ul><h2 id="6-Discussion-and-Conclusion"><a href="#6-Discussion-and-Conclusion" class="headerlink" title="6.Discussion and Conclusion"></a>6.Discussion and Conclusion</h2><ul><li><p>two types of commonsense knowledge are considered fundamental for human reasoning and decision making:</p><ul><li>intuitive psychology：心理</li><li>intuitive physics：物理</li></ul></li><li><p>Challenges</p><ul><li>relation with humans: understanding <strong>how much Commonsense Knowledge is developed and acquire in humans</strong> and how they related to human Language Production and Comprehension may shed light on computational models for NLP</li><li>difficult to identify and formalize Commonsense Knowledge</li><li>disconnect between Commonsense Knowledge resources and approaches to tackle these benchmarks<ul><li>One likely reason is that these knowledge bases do not cover the kind of knowledge that is required to solve those tasks<ul><li>To address this problem, several methods have been proposed for <strong>leveraging incomplete knowledge bases</strong></li><li>Eg1 <strong>AnalogySpace</strong>：uses principle component analysis to make analogies to smooth missing commonsense axioms</li><li>Eg2 <strong>Memory Comparison Networks</strong>：allow machines to generalize over existing temporal relations in Knowledge Sources in order to acquire new relations</li></ul></li><li>jointly develop benchmark tasks and construct knowledge bases<ul><li>Event2Mind &amp; ATOMIC</li><li>CommonsenseQA &amp; ConceptNet</li></ul></li></ul></li><li>only learning superficial artifacts from the dataset<ul><li>obscure statistical biases — high preformance, but not actual reasoning</li></ul></li></ul></li></ul><blockquote id="fn_davis"><sup>davis</sup>. Commonsense reasoning and commonsense knowledge in artiﬁcial intelligence. Commun. ACM, 58(9), 92–103.<a href="#reffn_davis" title="Jump back to footnote [davis] in the text."> &#8617;</a></blockquote><blockquote id="fn_rashkin-2018a"><sup>rashkin-2018a</sup>. Modeling Naive Psychology of Characters in Simple Commonsense Stories. ACL,2018.<a href="#reffn_rashkin-2018a" title="Jump back to footnote [rashkin-2018a] in the text."> &#8617;</a></blockquote><blockquote id="fn_rashkin-2018b"><sup>rashkin-2018b</sup>. Event2Mind: Commonsense Inference on Events, Intents, and Reactions. ACL,2018.<a href="#reffn_rashkin-2018b" title="Jump back to footnote [rashkin-2018b] in the text."> &#8617;</a></blockquote><blockquote id="fn_poliak"><sup>poliak</sup>. Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation. EMNLP, 2018.<a href="#reffn_poliak" title="Jump back to footnote [poliak] in the text."> &#8617;</a></blockquote><blockquote id="fn_cambria-2011"><sup>cambria-2011</sup>. Isanette: A Common and Common Sense Knowledge Base for Opinion Mining. In 2011 IEEE 11th International Conference on Data Mining Workshops, pp. 315–322, Vancouver, BC, Canada. IEEE.<a href="#reffn_cambria-2011" title="Jump back to footnote [cambria-2011] in the text."> &#8617;</a></blockquote>]]></content>
      
      
      <categories>
          
          <category> MRC-Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> reasoning </tag>
            
            <tag> commonsense </tag>
            
            <tag> survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Memory Networks</title>
      <link href="/2019/04/17/paper-memory-network/"/>
      <url>/2019/04/17/paper-memory-network/</url>
      
        <content type="html"><![CDATA[<p>Memory Networks 是一种框架，在这个框架内部的每个module都可以根据特定任务的需要用不同的方式来实现（task-specific）。</p><p>本篇主要以 《End-to-End Memory Networks》 (2015, MemN2N) 对 Memory Network 的框架进行介绍。</p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><ul><li>记忆网络的核心是记忆模块，可以看做是一个知识存储器。</li><li>在学习的过程中，首先需要对这个存储器的内容进行插入或更新，然后在测试的时候依靠这个存储器中的信息对于答案进行推理判断，具体包含以下四个主要模块：<ul><li>I：输入特征映射<ul><li>将输入转换为内部特征的表示？</li><li>将输入映射到特征空间</li></ul></li><li>G：泛化<ul><li>得到新的输入时，对过去的记忆进行更新；</li><li>称为泛化的原因是：在整个过程中网络能够根据未来的某些特定需要压缩、泛化本身的记忆；</li></ul></li><li>O：输出特征映射<ul><li>根据当前的输入和记忆状态得到输出，输出的是内部特征表示的形式（以内部特征表示作为输出）</li></ul></li><li>R：响应<ul><li>将上一步中的输出转换为指定的响应格式；</li></ul></li></ul></li><li>具体过程：对于一个特定的输入：one-hop<ul><li>1、将转换为内部特征表示的形式；</li><li>2、根据输入更新记忆；</li><li>3、根据输入和记忆计算输出特征；</li><li>4、解码得到响应的结果；</li></ul></li><li>MemN2N 的模型结构图，左侧是单层结构，右侧是多层（3层）结构<ul><li><img src="/images/paper-memory-network/memn2n-model.png" alt="MemN2N-Architecture"></li><li>Multi-hop的计算过程：<ul><li><img src="/images/paper-memory-network/memn2n-arch-flow.png" alt="MemN2N-Architecture2"></li></ul></li><li>Memory Module:<ul><li><img src="/images/paper-memory-network/memn2n-memory-module.png" alt="MemN2N-Memory-Module"></li></ul></li></ul></li></ul><h2 id="Approach-Details"><a href="#Approach-Details" class="headerlink" title="Approach Details"></a>Approach Details</h2><ul><li>模型的输入输出：<ul><li>输入：inputs $x_1,…,x_n$ (会被存储到memory中，$x_i$是一个句子) 和 query $q$，词典大小为 $|V|$</li><li>输出：answer $a$</li></ul></li></ul><h3 id="Single-Layer"><a href="#Single-Layer" class="headerlink" title="Single Layer"></a>Single Layer</h3><ol><li>input memory representation，将输入映射到特征/memory空间<ul><li>将 $x_i$ 通过 input embedding $A \in \mathbb{R}^{d\times |V|}$ 映射为 memory vector $m_i$</li><li>将 $q$ 通过 question embedding $B \in \mathbb{R}^{d\times |V|}$ 映射为 internal state $u$</li></ul></li><li>计算每个 memory 和 query 之间的attention，得到匹配程度：<ul><li>$p_i = softmax(u^T m_i)$</li><li>有多少个 memory vector 就有多少个 $p$</li></ul></li><li>output memroy representation：<ul><li>再将 $x_i$ 通过 output embedding $C \in \mathbb{R}^{d\times |V|}$ 映射为相应的 output vector $c_i$</li><li>计算 response context vector $o$:<ul><li>$o = \sum_i p_i c_i$</li></ul></li></ul></li><li>generating final prediction:<ul><li>使用 $o$ 和 $u$ 一起预测答案标签（可以是一个词）</li><li>$\hat{a} = softmax(W(o+u))$</li></ul></li></ol><ul><li>模型中的主要训练参数为：$A$、$B$、$C$、$W$</li></ul><h3 id="Multiple-Layers"><a href="#Multiple-Layers" class="headerlink" title="Multiple Layers"></a>Multiple Layers</h3><p>多层的结构就是对memory进行多次寻址（addressing/attention），每次关注不同的memory，主要的几点不同是：</p><ul><li>第一层之后的每层/每个hop的 query vector 是前一层的 response context vector 和 query vector 的结合，可以用不同的结合方式计算：<ul><li>$u^{k+1} = u^k + o^k$</li></ul></li><li>每层之间的embedding矩阵$A^k$和$C^k$不是共享的，具体有两种 权重初始化方式，参考下面的weight typing。</li></ul><h4 id="Weight-Typing"><a href="#Weight-Typing" class="headerlink" title="Weight Typing"></a>Weight Typing</h4><p>每个embedding A 和 embedding C 都是与词典大小相等的词向量矩阵，在multiple layers的结构中引入这两个参数矩阵会带来很大的参数开销</p><ul><li>Adjacent方式<ul><li>使第$k+1$层的input embedding $A^{k+1}$ 等于 第$k$ 层的output embedding $C^{k}$：$A{k+1} = C^k$</li><li>还是增加其他的约束：<ul><li>(a) 用最后一层的output embedding $C^{K}$ 去对 answer prediction中的参数矩阵 $W$ 进行赋值：$W^T = C^K$</li><li>(b) 使 question embedding 等于 第一层的input embedding $A^1$：$B = A^1$</li></ul></li></ul></li><li>Layer-wise（RNN-like）方式<ul><li>不同的层之间使用相同的embedding参数，在层间加入一个线性映射 $H$ 来更新 $u$：$u^{k+1} = H u^k + o^k$</li><li>在这种方式下，整体模型可以看成一个传统的rnn，将rnn的输出分为 internal 和 external 两类，$u$ 是rnn的hidden state</li></ul></li></ul><h2 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h2><ul><li>Memory Networks</li><li>Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> note </tag>
            
            <tag> network </tag>
            
            <tag> memory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ICLR2019 | Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question Answering</title>
      <link href="/2019/04/11/paper-iclr2019-cfc/"/>
      <url>/2019/04/11/paper-iclr2019-cfc/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question Answering<br>ICLR 2019<br>Victor Zhong, Caiming Xiong, Nitish Shirish Keskar, Richard Socher<br>University of Washington, Salesforce Research<br>Datasets: Qangaroo-WikiHop</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Analysis of Multi-Passage RC Task</title>
      <link href="/2019/04/11/mrc-analysis-multi-passage/"/>
      <url>/2019/04/11/mrc-analysis-multi-passage/</url>
      
        <content type="html"><![CDATA[<p>Works of multi-passage MRC task<br>Target Datasets: MS MARCO, Dureader</p><p>Reference papers on this track:</p><blockquote><ol><li>Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification. ACL,2018.</li><li>A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension. EMNLP,2018.</li><li>A Deep Cascade Model for Multi-Document Reading Comprehension. AAAI,2019.</li><li>Multi-Mention Learning for Reading Comprehension with Neural Cascades. ICLR,2018.</li><li>Coarse-Grain Fine-Grain Coattention Network for Multi-Evidence Question Answering. ICLR,2019.</li><li>Multi-style Generative Reading Comprehension. 2019.</li></ol></blockquote><p>TO BE Updated</p>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> research </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Analysis of Multi-Choice RC Task</title>
      <link href="/2019/03/28/mrc-analysis-multichoice/"/>
      <url>/2019/03/28/mrc-analysis-multichoice/</url>
      
        <content type="html"><![CDATA[<ul><li>focus on the strategy of <strong>matching processing</strong> between <code>(P, Q, Ans)</code></li><li>target datasets: <strong>RACE, MCScripts</strong></li></ul><p>Reference papers on multi-choice MRC task, especially toward matching processing.</p><blockquote><ol><li>Hierarchical Attention Flow for Multiple-Choice Reading Comprehension. AAAI,2018.</li><li>Dynamic Fusion Networks for Machine Reading Comprehension. 2017.</li><li><strong>A Co-Matching Model for Multi-choice Reading Comprehension</strong>. ACL,2018.</li><li><strong>Dual Co-Matching Network for Multi-choice Reading Comprehension</strong>. 2019.</li><li>Convolutional Spatial Attention Model for Reading Comprehension with Multiple-Choice Questions. AAAI,2019.</li><li><strong>Option Comparison Network for Multiple-choice Reading Comprehension</strong>. 2019</li><li>Yuanfudao at SemEval-2018 Task 11: Three-way Attention and Relational Knowledge for Commonsense Machine Comprehension. 2018.</li><li>HFL-RC System at SemEval-2018 Task 11: Hybrid Multi-Aspects Model for Commonsense Reading Comprehension. 2018.</li></ol></blockquote><h2 id="Co-Match-Network-HCM"><a href="#Co-Match-Network-HCM" class="headerlink" title="Co-Match Network (HCM)"></a>Co-Match Network (HCM)</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>previous works: 之前的MRC的工作通常是基于句对的序列匹配（Pair-Wise Sequence Matching)，有如下情况：<ul><li>passage 与 question 和 candidate answer 的串联进行比较；</li><li>passage 先与 question 进行比较，计算出 matching 结果，再使结果与 candidate answer 进行比较；</li></ul></li><li><p>这样的计算方式不适用于多选型RC任务，具体存在以下几点问题：</p><ul><li>1、仅将 passage 和 question 进行匹配，得到的结果可能没有意义并且会导致原始 passage 的信息丢失；<ul><li>例如：问题 Which statement of the following is true？</li></ul></li><li>若将 question 和 candidate answer 串联成为一个序列，损失了 question 和 candidate answer 的交互信息；</li></ul></li><li><p>基于此，多选RC任务需要解决<strong>匹配序列三元组 (matching sequence triplets)</strong>的问题；</p></li><li>本文的方法：<ul><li>match a question-answer pair to a given passage；<ul><li>explicitly treat the question and the candidate answer as two sequences and jointly match them to the given passage；</li></ul></li><li>对P中的每个位置，都计算两个attention权重，构成两个匹配表示，形成一个co-match状态（同时计算P和Q/A的匹配），然后再用一个层次LSTM框架（2个LSTM）对passage进行编码；<ul><li>层次汇聚信息：<ul><li>在passage中的每个句子内部，信息从word-level汇聚在sentence-level</li><li>在passage中的句子序列维度上，再从sentence-level汇聚到document-level；</li></ul></li><li>可以更好的处理，问题需要的信息分散在passage中不同句子，的情况</li></ul></li></ul></li></ul><h3 id="Model-Details"><a href="#Model-Details" class="headerlink" title="Model Details"></a>Model Details</h3><p>Notation:<br>&nbsp;&nbsp;&nbsp;&nbsp;(one sentence in) Passage: $P\in \mathbb{R}^{d\times P}$<br>&nbsp;&nbsp;&nbsp;&nbsp;Question: $Q \in \mathbb{R}^{d\times Q}$<br>&nbsp;&nbsp;&nbsp;&nbsp;(one candidate answer in) Answer: $A \in \mathbb{R}^{d\times A}$</p><ul><li><p>architecture</p><ul><li><img src="/images/mrc-analysis-multichoice/co-match.png" alt="co-match"></li></ul></li><li><p>co-matching</p><ul><li>encoding: the same BiLSTM<ul><li>$H^p\in \mathbb{R}^{l\times P}$, 每个句子分别计算</li><li>$H^q\in \mathbb{R}^{l\times Q}$,</li><li>$H^a\in \mathbb{R}^{l\times A}$, 每个候选分别计算</li></ul></li><li>attention:<ul><li>$G^q = softmax( (W^gH^q + b^g \otimes e_Q)^T H^p ) \in \mathbb{R}^{Q\times P}$</li><li>$G^a = softmax( (W^gH^a + b^g \otimes e_Q)^T H^p ) \in \mathbb{R}^{A\times P}$</li></ul></li><li>aggregation: attentive passage representation<ul><li>$\bar{H}^q = H^q G^q \in \mathbb{R}^{l \times P}$</li><li>$\bar{H}^a = H^q G^a \in \mathbb{R}^{l \times P}$</li></ul></li><li>co-match passage state: concurrently matches a passage state with both the question and the candidate answer. It represent how each P state can be matched with the Q and Candidate A.<ul><li>$M^q = ReLU(W^m[\bar{H}^q \ominus H^p; \bar{H}^q \otimes H^p]) + b^m \in \mathbb{R}^{l\times P}$</li><li>$M^a = ReLU(W^m[\bar{H}^a \ominus H^p; \bar{H}^a \otimes H^p]) + b^m \in \mathbb{R}^{l\times P}$</li><li>$W^m \in \mathbb{R}^{l\times 2l}$</li><li>$C = [M^q; M^a] \in \mathbb{R}^{2l \times P}$</li></ul></li></ul></li><li>hierarchical aggregation<ul><li>for each triplet $\{P_n, Q, A\}, n\in [1,N]$, get $C_n$ through co-match</li><li>sentence-level aggregation of the co-matching states:<ul><li>sentence sequence representation merge into a single vector</li><li>$h_n^s = MaxPooling(BiLSTM(C_n)) \in \mathbb{R}^l$</li><li>$MaxPooling$： row-wise max pooling</li></ul></li><li>final triplet matching representation:<ul><li>$H^s=[h_1^s, h_2^s,…,h_N^s]$</li><li>$h^t = MaxPooling (BiLSTM (H^s)) \in \mathbb{R}^{l}$</li></ul></li></ul></li><li>Output Layer<ul><li>for each candidate answer $A_i$, get $h_i^t \in \mathbb{R}^{l} $</li><li>$L(A_i|P,Q) = -log \frac{exp(w^Th_i^t)}{\sum_{j=1}^4 exp(w^T h_j^t)}$</li></ul></li></ul><h3 id="Model-Parameters"><a href="#Model-Parameters" class="headerlink" title="Model Parameters"></a>Model Parameters</h3><ul><li>word emb dim: 300</li><li>rnn hidden dim: 150</li><li>optimizer: Adamax, lr=0.002</li><li>batch：10</li><li>epochs：30</li><li>dropout：0.2</li></ul><h2 id="Dual-Co-Matching-Network"><a href="#Dual-Co-Matching-Network" class="headerlink" title="Dual Co-Matching Network"></a>Dual Co-Matching Network</h2><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>previous work: <ul><li>只计算了question-aware P表示和 option-aware P表示；</li><li>一些pretrainLM的做法是将P和Q串联成为一个句子，A单独作为另一个句子；</li></ul></li><li>本文：<ul><li>model the relationship among passage，question and answer <strong>bidirectionally</strong></li><li>在计算question-aware P表示和 option-aware P表示的同时，计算passage-aware Q表示和passage-aware O表示</li></ul></li></ul><h3 id="Model-Details-1"><a href="#Model-Details-1" class="headerlink" title="Model Details"></a>Model Details</h3><ul><li>Encoding<ul><li>$H^p = Bert(P) \in \mathbb{R}^{P\times l}$</li><li>$H^q = Bert(Q) \in \mathbb{R}^{Q\times l}$</li><li>$H^a = Bert(A) \in \mathbb{R}^{A\times l}$</li><li>$l$: Bert hidden state dimension</li></ul></li><li>Matching Layer<ul><li>attention between P and A:<ul><li>$W = softmax(H^p(H^a G+b)^T) \in \mathbb{R}^{P\times A}$<ul><li>$G \in \mathbb{R}^{l\times l}$</li></ul></li><li>$M^p = WH^a \in \mathbb{R}^{P\times l}$</li><li>$M^a = W^TH^p \in \mathbb{R}^{A\times l}$<ul><li>$W \in \mathbb{R}^{P\times A}$</li></ul></li></ul></li><li>attention  between P and Q in the same method, get:<ul><li>$M^q \in\mathbb{R}^{Q\times l}$</li><li>$W^\prime \in \mathbb{R}^{P\times Q}$</li><li><font color="blue">问题：为什么P和Q进行attention，不计算question-aware的passage表示？</font></li></ul></li><li>integration original contextual representation<ul><li>$S^a = F([M^a - H^a;M^a \cdot H^a]W_1 + b_1) \in \mathbb{R}^{P \times l}$</li><li>$S^p = F([M^p - H^p;M^p \cdot H^p]W_2 + b_2)\in \mathbb{R}^{A \times l}$</li><li>$F()$ is activation function $ReLU$</li><li>in the question side:<ul><li>$S^{p^\prime} \in \mathbb{R}^{P\times l}$</li><li>$S^q \in \mathbb{R}^{Q\times l}$</li></ul></li></ul></li></ul></li><li>Aggregation Layer<ul><li>get final representation for each candidate answer<ul><li>row-wise max pooling</li><li>$C^p = Pooling(S^p) \in \mathbb{R}^{l}$</li><li>$C^a = Pooling(S^a) \in \mathbb{R}^{l}$</li><li>$C^{p^\prime} = Pooling(S^{p^\prime}) \in \mathbb{R}^{l}$</li><li>$C^q = Pooling(S^q) \in \mathbb{R}^{l}$</li><li>$C = [C^p;C^a;C^{p^\prime};C^q]$</li></ul></li></ul></li><li>Output Layer<ul><li>$L(A_i|P,Q)=-log\frac{exp(V^TC_i)}{\sum_{j=1}^N exp(V^TC_j)}$</li></ul></li></ul><h3 id="Model-Parameters-1"><a href="#Model-Parameters-1" class="headerlink" title="Model Parameters"></a>Model Parameters</h3><p> No description</p><h2 id="Option-Comparison-Network-OCN"><a href="#Option-Comparison-Network-OCN" class="headerlink" title="Option Comparison Network (OCN)"></a>Option Comparison Network (OCN)</h2><h3 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>previous work:<ul><li>read each option independently.</li><li>compute a fixed-length representation for each option before comparing them.</li></ul></li><li>ideas:<ul><li>humans typically compare the options at multiple-granularity level before reading the article in detail and make reasoning more efficient.</li><li>人解决多选RC任务的策略，通常在仔细阅读文章之前会在不同粒度上比较候选答案。</li><li>通过比较候选答案，可以定位答案选项间的相互关系，在读文章时只关注与选项相互关系有关的文章信息。（更高效？more efﬁciently and effectively）</li></ul></li><li>本文：<ul><li>explicitly compare options at word-level to better identify their correlations to help reasoning</li><li><ol><li>首先使用一个skimmer network对每个option进行独立编码；</li></ol></li><li><ol><li>然后对每个option，将其与其他的options使用attention进行word-level的比较，来建立option之间的相互比较；</li></ol></li><li><ol><li>最后，带着聚集之后的option间的相关性，重读文章，进行推理和答案选择</li></ol></li></ul></li><li>Analysis:<ul><li>这篇文章的主要更新的是option的表示</li></ul></li></ul><h3 id="Model-Details-2"><a href="#Model-Details-2" class="headerlink" title="Model Details"></a>Model Details</h3><p>Notation:<br>&nbsp;&nbsp;&nbsp;&nbsp;Passage: $P=\{w_1^p,…,w_m^p\}$<br>&nbsp;&nbsp;&nbsp;&nbsp;Question: $Q= \{w_1^q,…,w_n^q\}$<br>&nbsp;&nbsp;&nbsp;&nbsp;Answer set: $O=\{O_1,…,O_K\}$<br>&nbsp;&nbsp;&nbsp;&nbsp;Each option: $O_k = \{w_1^o,…,w_{n_k}^o\}$</p><ul><li><p>Overall: 4 stages</p><ol><li>concatenate each (article, question, option) triple into a sequence and use a skimmer to encode them into vector sequences.</li><li>attention-based mechanism is leveraged to compare the options.</li><li>the article is reread with the correlation information gathered in last stage as extra input.</li><li>compute the probabilities for each option.</li></ol></li><li><p>Option Feature Extraction</p><ul><li>skimmer encoding: 将每个option与P和Q串联构成一个句子，使用BERT进行编码<ul><li>$[P^{enc};Q^{enc};O^{enc}_k] = BERT(<p;q;o_k>)$<ul><li>$P^{enc} \in \mathbb{R}^{d\times m}$</li><li>$Q^{enc} \in \mathbb{R}^{d\times n}$</li><li>$O^{enc}_k \in \mathbb{R}^{d\times n_k}$</li></ul></p;q;o_k></li></ul></li><li>由于Q和option的关联紧密，将两者串联，作为option的特征<ul><li>$O_k^q=[Q^{enc}|O^{enc}_k] \in \mathbb{R}^{d\times n_k^\prime}$<ul><li>$n_k^\prime = n+n_k$</li></ul></li></ul></li></ul></li><li>Option Correlation Features Extraction<ul><li>$Att(\cdot)$的计算方式：假设输入为$U\in \mathbb{R}^{d\times N}$和 $V\in \mathbb{R}^{d\times M}$<ul><li>$v \in \mathbb{R}^{3d}$ 是参数</li><li>$s_{ij}=v^T[U_{:i};V_{:j};U_{:i}\circ V_{:j}]$</li><li>$A= Att(U,V;v)=[\frac{exp(s_{ij})}{\sum_i exp(s_{ij})}]_{ij} \in \mathbb{R}^{N\times M}$</li></ul></li><li>option correlation feature extraction 分3步进行<ul><li><ol><li>option $O_k$ 与其他options进行one-by-one比较，收集 pair-wise correlation信息<ul><li>$\bar{O}_k^{(l)}=O^q_l Att(O^q_l,O_k^q;v_o)$</li><li>$\tilde{O}_k^{(l)}=[O_k^q-\bar{O}_k^{(l)};O_k^q \circ \bar{O}_k^{(l)}] \in \mathbb{R}^{2d\times n_k^\prime}$</li></ul></li></ol></li><li><ol><li>gather pair-wise correlation information<ul><li>$\tilde{O}_k^c=tanh(W_c [O_k^q;\{\tilde{O}_k^{(l)}\}_{l\neq k} ])$<ul><li>$W_c \in \mathbb{R}^{d\times (d+2d(|O|-1))}$</li></ul></li></ul></li></ol></li><li><ol><li>element-wise gating 机制控制option feature和option-wise correlation information的融合，以产生option correlation features $O_k^c$<ul><li>$g_k \in \mathbb{R}^{d\times n_k^\prime}$<ul><li>$g_{k,:i}=\sigma (W_g [Q_{K,:i}^q; \tilde{O}_{k,:i}^c; \tilde{O}]+b_g)$</li><li>$g_{k,:i}$ 表示 g 向量的第i列</li></ul></li><li>$\tilde{O}$ 的计算：关于 Q 的attention pooling<ul><li>$A_q = softmax(v_a^T Q^{enc})^T, v_a \in \mathbb{R}^d$</li><li>$\tilde{O}=Q^{enc}A^q \in \mathbb{R}^{d}$</li></ul></li><li>option correlation features: $O_k^c\in \mathbb{R}^{d\times n_k^\prime}$<ul><li>$O_{k,:i}^c = g_{k,:i} \circ O_{k,:i}^q + (1-g_{k,:i}) \circ \tilde{O}_{k,:i}^c$</li><li>Note: $O_k^c$ 不被压缩成fixed-length向量，文中的解释为-这样可以使我们的模型更灵活的使用correlation信息。</li></ul></li></ul></li></ol></li></ul></li></ul></li><li>Article ReReading<ul><li>co-attention + self-attention</li><li>对于每个option $O_k$ 计算 co-attention:<ul><li>$A_k^c = Att(O_k^c,P^{enc};v_p) \in \mathbb{R}^{n_k^\prime \times m}$</li><li>$A_k^p = Att(P^{enc},O_k^c;v_p) \in \mathbb{R}^{m\times n_k^\prime}$</li><li>$\hat{O}_k^p = [P^{enc};O_k^c A_k^c]A_k^p \in \mathbb{R}^{2d\times n_k^\prime}$</li></ul></li><li>fused with correlation information<ul><li>$\tilde{O}_k^p = ReLU(W_p[O_k^c;\hat{O}_k^p]+b_p) \in \mathbb{R}^{d\times n_k^\prime}$</li></ul></li><li>self-attention to get full-info option representation $O_k^f\in \mathbb{R}^{d\times n_k^\prime}$<ul><li>$\tilde{O}_k^s = \tilde{O}_k^p Att(\tilde{O}_k^p, \tilde{O}_k^p;v_r)$</li><li>$\tilde{O}_k^f = [\tilde{O}_k^p;\tilde{O}_k^s;\tilde{O}_k^p-\tilde{O}_k^s;\tilde{O}_k^p \circ \tilde{O}_k^s]$</li><li>$O_k^f = ReLU(W_f\tilde{O}_k^f +b_f)$</li></ul></li></ul></li><li>Answer prediciton<ul><li>score $s_k = v_s^T MaxPooling(O_k^f)$<ul><li>MaxPooling: row-wise</li><li>$v_s \in \mathbb{R}^d$</li></ul></li><li>probability：<ul><li>$P(K|Q,P,O)=\frac{exp(s_k)}{\sum_i exp(s_i)}$</li></ul></li><li>loss:<ul><li>$J(\theta)=-\frac{1}{N}\sum_i log(P(\hat{k}_i | Q_i,P_i,O_i)) + \lambda||\theta||_2^2$</li></ul></li></ul></li></ul><h3 id="Model-Parameters-2"><a href="#Model-Parameters-2" class="headerlink" title="Model Parameters"></a>Model Parameters</h3><ul><li>for BERT base:<ul><li>batch:12</li><li>epochs:3</li><li>lr: $3\times 10^{-5}$ </li></ul></li><li>for BERT large:<ul><li>batch:24</li><li>epochs:5</li><li>lr: $1.5\times 10^{-5}$ </li></ul></li><li>$\lambda$: 0.01</li><li>lengths:<ul><li>P: 400</li><li>Q: 30</li><li>A: 16</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> research </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Practicable Course List (continually updated)</title>
      <link href="/2019/03/12/course-list/"/>
      <url>/2019/03/12/course-list/</url>
      
        <content type="html"><![CDATA[<p>The main areas of concern are:</p><blockquote><p><a href="#NLP">Natural Language Processing</a><br><a href="#MLandDL">Machine Learning and Deep Learning</a><br><a href="#RL">Reinforcement Learning</a><br><a href="#MathBasic">Foundation of Mathematics</a></p></blockquote><h2 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h2><ul><li>Stanford，CS224n<ul><li><a href="http://web.stanford.edu/class/cs224n/index.html" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/index.html</a></li><li>video：<a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z" target="_blank" rel="noopener">Winter 2019</a></li></ul></li><li>CMU，CS 11-731<ul><li>Machine Translation and Seq-to-Seq Models，2018</li><li><a href="http://www.phontron.com/class/mtandseq2seq2018/" target="_blank" rel="noopener">http://www.phontron.com/class/mtandseq2seq2018/</a></li></ul></li><li>CMU，CS 11-747<ul><li>Neural Networks for NLP，Spring 2018</li><li><a href="http://www.phontron.com/class/nn4nlp2018/schedule.html" target="_blank" rel="noopener">http://www.phontron.com/class/nn4nlp2018/schedule.html</a></li><li>video：<a href="https://www.youtube.com/playlist?list=PL8PYTP1V4I8Ajj7sY6sdtmjgkt7eo2VMs" target="_blank" rel="noopener">2019</a></li></ul></li><li>Oxford<ul><li><a href="https://github.com/oxford-cs-deepnlp-2017/lectures" target="_blank" rel="noopener">https://github.com/oxford-cs-deepnlp-2017/lectures</a></li></ul></li><li>Berkeley<ul><li>Applied Natural Language Processing</li><li><a href="http://people.ischool.berkeley.edu/~dbamman/info256.html" target="_blank" rel="noopener">http://people.ischool.berkeley.edu/~dbamman/info256.html</a></li></ul></li><li>Penn，CIS 700<ul><li>Advanced Machine Learning for Natural Language Processing，Dan Roth</li><li><a href="http://www.cis.upenn.edu/~danroth/Teaching/CIS-700-006/index.html" target="_blank" rel="noopener">http://www.cis.upenn.edu/~danroth/Teaching/CIS-700-006/index.html</a></li></ul></li><li>CS 4650 and 7650<ul><li><a href="https://github.com/jacobeisenstein/gt-nlp-class" target="_blank" rel="noopener">https://github.com/jacobeisenstein/gt-nlp-class</a></li></ul></li><li>University of Washington<ul><li>CSE 447/547M: Natural Language Processing</li><li><a href="https://courses.cs.washington.edu/courses/cse447/19wi/" target="_blank" rel="noopener">https://courses.cs.washington.edu/courses/cse447/19wi/</a></li></ul></li></ul><h2 id="MLandDL"><a href="#MLandDL" class="headerlink" title="MLandDL"></a>MLandDL</h2><ul><li>Stanford，CS229<ul><li><a href="http://cs229.stanford.edu/index.html#info" target="_blank" rel="noopener">http://cs229.stanford.edu/index.html#info</a></li></ul></li><li>李宏毅<ul><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses.html" target="_blank" rel="noopener">http://speech.ee.ntu.edu.tw/~tlkagk/courses.html</a></li></ul></li><li>Berkeley，STAT 157<ul><li>Introduction to Deep Learning</li><li><a href="http://courses.d2l.ai/berkeley-stat-157/index.html" target="_blank" rel="noopener">http://courses.d2l.ai/berkeley-stat-157/index.html</a></li></ul></li><li>Berkeley，2019，无监督学习<ul><li>CS294-158 Deep Unsupervised Learning Spring 2019</li><li><a href="https://sites.google.com/view/berkeley-cs294-158-sp19/home" target="_blank" rel="noopener">https://sites.google.com/view/berkeley-cs294-158-sp19/home</a></li></ul></li><li>Stanford，CS 236: Deep Generative Models<ul><li><a href="https://deepgenerativemodels.github.io/" target="_blank" rel="noopener">https://deepgenerativemodels.github.io/</a></li></ul></li><li>MIT，6.883<ul><li>Science of Deep Learning：Bridging Theory and Practice</li><li><a href="https://people.csail.mit.edu/madry/6.883/" target="_blank" rel="noopener">https://people.csail.mit.edu/madry/6.883/</a></li></ul></li><li>Maching Learning Summer School<ul><li><a href="http://mlss.cc/" target="_blank" rel="noopener">http://mlss.cc/</a></li></ul></li></ul><h2 id="RL"><a href="#RL" class="headerlink" title="RL"></a>RL</h2><ul><li>UCL，David Sliver<ul><li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a></li></ul></li><li>Berkeley S294-112<ul><li>Deep Reinforcement Learning</li><li><a href="http://rail.eecs.berkeley.edu/deeprlcourse/" target="_blank" rel="noopener">http://rail.eecs.berkeley.edu/deeprlcourse/</a></li></ul></li><li>Stanford<ul><li>CS234: Reinforcement Learning Winter 2019</li><li><a href="http://web.stanford.edu/class/cs234/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs234/</a></li></ul></li><li>Berkeley CS188<ul><li>Introduction to Artificial Intelligence</li><li><a href="https://inst.eecs.berkeley.edu/~cs188/sp19/" target="_blank" rel="noopener">https://inst.eecs.berkeley.edu/~cs188/sp19/</a></li></ul></li></ul><h2 id="MathBasic"><a href="#MathBasic" class="headerlink" title="MathBasic"></a>MathBasic</h2><ul><li>Stanford，CS229T/STATS231<ul><li>Statistical Learning Theory</li><li><a href="https://web.stanford.edu/class/cs229t/" target="_blank" rel="noopener">https://web.stanford.edu/class/cs229t/</a></li></ul></li><li>CMU，10-708，Probabilistic Graphical Models<ul><li>Eric Xing， Spring 2014</li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708/lecture.html" target="_blank" rel="noopener">http://www.cs.cmu.edu/~epxing/Class/10708/lecture.html</a></li></ul></li><li>NYU，MathsDL-spring18<ul><li>Topics course Mathematics of Deep Learning, NYU, Spring 18. CSCI-GA 3033.</li><li><a href="https://github.com/joanbruna/MathsDL-spring18" target="_blank" rel="noopener">https://github.com/joanbruna/MathsDL-spring18</a></li></ul></li></ul><h2 id="Recommend-Archive-Links"><a href="#Recommend-Archive-Links" class="headerlink" title="Recommend Archive Links"></a>Recommend Archive Links</h2><ul><li><a href="https://deep-learning-drizzle.github.io/index.html#contents" target="_blank" rel="noopener">https://deep-learning-drizzle.github.io/index.html#contents</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> ForStudy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> study </tag>
            
            <tag> resource </tag>
            
            <tag> course </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Knowledge-based MRC Papers</title>
      <link href="/2019/03/10/mrc-knowedge-paper-info/"/>
      <url>/2019/03/10/mrc-knowedge-paper-info/</url>
      
        <content type="html"><![CDATA[<p>A list of recent papers with respect to Knowledge-based Machine Reading Comprehension.<br><a id="more"></a></p><style>    table th:nth-of-type(2){    width: 60%;    }</style><h2 id="Works-on-Knowledge-aware-MRC"><a href="#Works-on-Knowledge-aware-MRC" class="headerlink" title="Works on Knowledge-aware MRC"></a>Works on Knowledge-aware MRC</h2><!-- aware/based/enhanced --><div class="table-container"><table><thead><tr><th style="text-align:center">Conf.</th><th style="text-align:left">Title</th><th style="text-align:left">Authors/Org.</th><th style="text-align:center">Note</th></tr></thead><tbody><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="https://doi.org/10.18653/v1/P17-1132" target="_blank" rel="noopener">Leveraging knowledge bases in lstms for improving machine reading</a></td><td style="text-align:left">Yang, et al.<br>CMU</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="https://www.aclweb.org/anthology/D17-1216" target="_blank" rel="noopener">Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension</a></td><td style="text-align:left">Hongyu Lin, et al.</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/D17-1086" target="_blank" rel="noopener">World knowledge for reading comprehension: Rare entity prediction with hierarchical lstms using external descriptions</a></td><td style="text-align:left">Long, et al.<br>McGill University</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2018</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P18-1076" target="_blank" rel="noopener">Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge</a></td><td style="text-align:left">Mihaylov, et al.<br>Heidelberg University</td><td style="text-align:center"><a href="/2019/01/09/paper-knreader/" title="knreader-note">knreader-note</a></td></tr><tr><td style="text-align:center">2018</td><td style="text-align:left"><a href="https://openreview.net/forum?id=B1twdMCab" target="_blank" rel="noopener">Dynamic Integration of Background Knowledge in Neural NLU Systems</a></td><td style="text-align:left">Dirk Weissenborn, et al.</td><td style="text-align:center"><a href="/2019/03/06/paper-2018-refinewordemb/" title="note">note</a></td></tr><tr><td style="text-align:center">EMNLP<br>2018</td><td style="text-align:left"><a href="http://aclweb.org/anthology/D18-1454" target="_blank" rel="noopener">Commonsense for Generative Multi-Hop Question Answering Tasks</a></td><td style="text-align:left">Lisa Bauer</td><td style="text-align:center"><a href="/2019/02/21/paper-emnlp2018-mhpgm/" title="mhpgm-note">mhpgm-note</a></td></tr><tr><td style="text-align:center">AAAI<br>2018</td><td style="text-align:left"><a href="https://arxiv.org/abs/1811.00625" target="_blank" rel="noopener">Incorporating Structured Commonsense Knowledge in Story Completion</a></td><td style="text-align:left"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">EMNLP<br>2018</td><td style="text-align:left"><a href="https://aclweb.org/anthology/D18-1455" target="_blank" rel="noopener">Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text</a></td><td style="text-align:left">CMU</td><td style="text-align:center"><a href="/2019/07/08/paper-emnlp2018-graft-net/" title="note">note</a></td></tr><tr><td style="text-align:center">(2018v1)<br>ACL<br>2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1809.03449" target="_blank" rel="noopener">Exploring Machine Reading Comprehension with Explicit Knowledge</a><br>Explicit Utilization of General Knowledge in Machine Reading Comprehension</td><td style="text-align:left">York University</td><td style="text-align:center"><a href="/2019/07/17/paper-acl2019-kar/" title="note">note</a></td></tr><tr><td style="text-align:center">ACL<br>2018</td><td style="text-align:left"><a href="https://aclweb.org/anthology/D18-1455" target="_blank" rel="noopener">Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text</a></td><td style="text-align:left">CMU</td><td style="text-align:center"><a href="/2019/07/08/paper-emnlp2018-graft-net/" title="note">note</a></td></tr><tr><td style="text-align:center">AAAI<br>2019</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1809.03568.pdf" target="_blank" rel="noopener">Improving Question Answering by Commonsense-Based Pre-Training</a></td><td style="text-align:left">MSRA</td><td style="text-align:center"><a href="/2019/07/06/paper-aaai2019-cs-based-pre-train/" title="note">note</a></td></tr><tr><td style="text-align:center">NAACL<br>2019</td><td style="text-align:left"><a href="https://www.aclweb.org/anthology/N19-1270" target="_blank" rel="noopener">Improving Machine Reading Comprehension with General Reading Strategies</a></td><td style="text-align:left">TencentAI Lab</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1902.00993" target="_blank" rel="noopener">Improving Question Answering with External Knowledge</a></td><td style="text-align:left"></td><td style="text-align:center"><a href="/2019/08/26/paper-2019-edl-md/" title="note">note</a></td></tr><tr><td style="text-align:center">CIKM<br>2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1908.04530" target="_blank" rel="noopener">Incorporating Relation Knowledge into Commonsense Reading Comprehension with Multi-task Learning</a></td><td style="text-align:left">Alibaba<br>DAMO</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2019</td><td style="text-align:left"><a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1226/" target="_blank" rel="noopener">Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension</a></td><td style="text-align:left">PKU<br>Baidu</td><td style="text-align:center"><a href="/2019/07/29/paper-acl2019-kt-net/" title="note">note</a></td></tr><tr><td style="text-align:center">EMNLP<br>2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1909.02151" target="_blank" rel="noopener">KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning</a></td><td style="text-align:left"></td><td style="text-align:center"><a href="/2019/09/09/paper-emnlp2019-kagnet/" title="note">note</a></td></tr><tr><td style="text-align:center">2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1908.06725v1" target="_blank" rel="noopener">Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models</a></td><td style="text-align:left">Alibaba<br>DAMO</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1909.05311" target="_blank" rel="noopener">Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering</a></td><td style="text-align:left">MSRA</td><td style="text-align:center"><a href="/2019/09/17/paper-csqa-1909-05311/" title="note">note</a></td></tr><tr><td style="text-align:center"></td><td style="text-align:left"></td><td style="text-align:left"></td></tr></tbody></table></div><h2 id="MRC-with-Knowledge"><a href="#MRC-with-Knowledge" class="headerlink" title="MRC with Knowledge"></a>MRC with Knowledge</h2><ul><li><del>how to let the machine obtain Knowledge？</del></li><li>how to extract external knowledge?</li><li>how to represent knowledge？ in which kind of format？</li><li>how to fuse the external knowledge?</li><li>how to let the machine to learn Knowledge incrementally？</li><li>how to make the machine can automatically use Knowledge it already knows or it has been told？</li></ul>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2018 | Dynamic Integration of Background Knowledge in Neural NLU Systems</title>
      <link href="/2019/03/06/paper-2018-refinewordemb/"/>
      <url>/2019/03/06/paper-2018-refinewordemb/</url>
      
        <content type="html"><![CDATA[<blockquote><p><em>Dynamic Integration of Background Knowledge in Neural NLU Systems</em><br><em>ICLR,2018. Reject</em><br><em>Dirk Weissenborn, et.al.</em><br><em>Datasets: SQuAD, TriviaQA, SNLI, MNLI</em></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>the requisite background knowledge is indirectly acquired from static corpora.</li><li>background knowledge learned from task supervision and also by pre-training word embeddings.</li><li>从静态的训练数据中获取背景知识有两点缺陷:<ul><li>1/不是所有的对解决NLU任务重要的背景知识都可以从有限量的训练数据中抽取出来；</li><li>2/随着时间的变化，对于理解文本有帮助的事实也会发生变化；</li></ul></li></ul><p>This work:（不同于仅依赖于从训练数据中获取静态知识）</p><ul><li>develop a new reading architecture for the <strong>dynamic integration</strong> of <strong>explicit background knowledge</strong> in NLU models.</li><li>a new <strong>task-agnostic(任务无关)</strong> reading module provides <strong>reﬁned word representations</strong> to a task-speciﬁc NLU architecture by processing background knowledge in the form of free-text statements, together with the task-speciﬁc inputs.</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>输入是：待理解的文本，即context，和抽取出的相关知识的assertions.<br>然后，使用 word embedding refinement 的策略，增量式地读入context和assertions，最初使用上下文无关的词向量仅初始化.<br>这种 contextually refined word embedding <strong>可以看成是一种动态记忆，用来存储新结合的知识</strong>. </p><h3 id="External-Knowledge-as-Supplementary-Text-Inputs"><a href="#External-Knowledge-as-Supplementary-Text-Inputs" class="headerlink" title="External Knowledge as Supplementary Text Inputs"></a>External Knowledge as Supplementary Text Inputs</h3><ul><li>结合知识的形式：<ul><li>本文中并不限制外部信息的形式：无结构/结构化知识都可以作为补充信息</li></ul></li><li>结合何种知识：<ul><li>从知识源中抽取上下文相关的信息本身就是个复杂的研究，并且依赖于知识库的形式</li><li>全面抽取所有潜在的assertions，然后依赖于我们的阅读结构来学习抽取相关的信息</li><li>Assertion Retrieval<ul><li>抽取知识是为了获得句子之间的关联</li><li>抽取出连接头/尾实体在text中，尾/头实体在question中的知识</li><li>由于抽取出的assertion过多，使用排序分数对assertions进行打分（类似于tf-idf的打分方式，针对的是罕见但是重要的知识，选择top-k个）</li></ul></li></ul></li></ul><h3 id="Refine-Word-Embeddings-by-Reading"><a href="#Refine-Word-Embeddings-by-Reading" class="headerlink" title="Refine Word Embeddings by Reading"></a>Refine Word Embeddings by Reading</h3><p>将词向量看做一种记忆，不仅包含通用的知识，还包含上下文信息和抽取的知识信息.</p><p>本文提出的增量式 refinement 过程编码输入文本，然后使用多个阅读步得到的编码输入来更新词向量矩阵.<br>过程如图：</p><ul><li><img src="/images/paper-2018-refinewordemb/fig-1.png" alt="refinement"></li></ul><p>Notations:</p><ul><li>$E^0$: 初始的词向量</li><li>$E^\ell$: 第$\ell$步更新的词向量</li><li>$X^\ell$: 第$\ell$步的上下文信息</li><li>$FC(z)=W z + b, W\in \mathbb{R}^{n \times m}, b\in \mathbb{R}^{n}, z\in \mathbb{R}^m$</li></ul><h4 id="1-Unrefined-Word-Embeddings"><a href="#1-Unrefined-Word-Embeddings" class="headerlink" title="1.Unrefined Word Embeddings"></a>1.Unrefined Word Embeddings</h4><p>这一步的目标是根据预训练词向量$e_w^p \in \mathbb{R}^{n^\prime}$得到初始的non-contextual词表示，计算如下：</p><ul><li>$e_w^{p^\prime} = ReLU(FC(e_w^p))$</li><li>$g_w = \sigma(FC([e_w^{p^\prime} ; e_w^{char}]))$</li><li>$e_w^0 = g_w \cdot e_w^{p^\prime} + (1-g_w) \cdot e_w^{char}$<br>其中，$e_w^{char}$是通过cnn编码（n convolutional ﬁlters w of width 5 followed by a max-pooling operation over time.）得到</li></ul><h4 id="2-Contextually-Refined-Word-Representation"><a href="#2-Contextually-Refined-Word-Representation" class="headerlink" title="2.Contextually Refined Word Representation"></a>2.Contextually Refined Word Representation</h4><p>在编码输入文本时，</p><ul><li>给每个词concatenate一个长度为L(即进行refienment处理的次数)的one-hot向量表示对应的位($\ell$)置1，</li><li>得到输入文本$X_i^{\ell} \in \mathbb{R}^{d\times |x_i^l|}$</li><li>经过lstm进行context编码: $\hat{X}_i^{\ell} = ReLU(FC(BiLSTM(X_i^{\ell})))$</li><li>在任务中：$X^1$相当于是Passage(Premise)文本的表示，$X^2$相当于是Question(Hypothesis)的表示，额外的知识assertions是$X^3$<ul><li>在实验中，p\q的顺序对最终的结果没有显著的影响</li></ul></li></ul><p>更新词向量：</p><ul><li>首先对所有在文本中与此词的lemma相同的词进行一个maxpool: <ul><li>$\hat{e}_w^{\ell} = max\{\hat{x}_k^{\ell} | x^{\ell} \in X^{\ell}, lemma(x_k^{\ell}) = lemma(w) \}$</li></ul></li><li>然后，用context-independent的表示去计算一个context-sensitive的表示<ul><li>通过门控机制，是模型决定利用多少新读入的信息来改写词向量</li><li>$u_w^{\ell} = \sigma(FC( [e_w^{\ell -1}; \hat{e}_w^{\ell}] ))$</li><li>$e_w^{\ell} = u_w^{\ell} \cdot e_w^{\ell -1} + (1- u_w^{\ell})\cdot \hat{e}_w^{\ell}$</li></ul></li><li>关于pooling操作：在具有相同lemma的词上面进行pooling操作<ul><li>有效的联系可以缓解长距离依赖问题</li><li>更充分的利用输入作为相关背景知识</li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>这篇文章的实验是在NLI（SNLI）和DQA（SQuAD）的任务上进行。</p><p>对NLI任务上：</p><ul><li>使用全部的数据进行训练时的提升不是很大</li><li>但是使用部分数据进行训练时的提升相对较多</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> nli </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACL2018 | Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering</title>
      <link href="/2019/03/03/paper-acl2018-slqa/"/>
      <url>/2019/03/03/paper-acl2018-slqa/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering<br>ACL 2018<br>Wei Wang, Ming Yan, Chen Wu.<br>Alibaba Group.<br>Multi-Granularity; Hierarchical Attention Fusion; Architecture<br>Datasets: SQuAD; TriviaQA</p></blockquote><p>看到题目，首先就产生了三个关注点：</p><ul><li>multi-granularity, 代表哪些粒度？ (word level 和 sentence level)</li><li>hierarchical, 有哪些层次？（co-attention 和 self-attention）</li><li>fusion, 怎样进行融合？对粒度的融合（global level）</li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>启发于人类通常的阅读模式：<ul><li>浏览全篇文章，大致了解文章内容（通读全文）</li><li>浏览问题，记住问题，找到P和Q之间的联系，理解Q的意图</li><li>定位一个大致/粗略的潜在的答案区域，使注意力聚集到定位的上下文（重点阅读上下文）</li><li>再次回顾问题，确定一个最优答案</li></ul></li></ul><h3 id="This-Work"><a href="#This-Work" class="headerlink" title="This Work"></a>This Work</h3><ul><li>建模问题和文章中特定区域关联的同时，借助分层策略逐步集中注意力，是答案边界清晰</li><li>提出 hierarchical attention network：<ul><li>逐步定位答案边界</li><li>建模P和Q之间的不同粒度层级间的关系</li></ul></li><li>在Encoder层：<ul><li>为了更好的建模Q和P的多个aspects：同时使用预训练的glove表示和ELMo表示作为一个词的表示</li><li>针对ELMo的融合，设计了一个 Representation-aware fusion 方法来结合ELMo输出向量和BiLSTM建模的上下文表示</li></ul></li><li>在本文提出的Hierarchical Attention Fusion Network中：<ul><li>利用co-attention和self-attention机制，逐步的将注意力聚集到最优的答案span<ul><li>co-attention with shallow semantic fusion</li><li>self-attention with deep semantic fusion </li><li>memory-wise bilinear alignment function</li></ul></li><li>特点：<ul><li>fine-grained fusion 结合attention vector 更好的建模P和Q的关系</li><li>multi-granularity attention 应用于 word-level 和 sentence-level</li></ul></li><li>与其他方法不同的是：<ul><li>利用全局表示（原始的上下文表示）构建attention表示</li><li>利用fusion layer来对attention表示进行进一步的微调</li></ul></li></ul></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul><li>本文提出模型 SLQA+ (Semantic Learning for Question Answering) 的整体架构：<ul><li>encoder、attention、matching、output</li><li><img src="/images/paper-acl2018-slqa/slqa-model.png" alt="slqa+模型图"></li></ul></li></ul><h3 id="1-Encoder-Layer"><a href="#1-Encoder-Layer" class="headerlink" title="1.Encoder Layer"></a>1.Encoder Layer</h3><ul><li>输入是 P和Q的每个词的glove向量和ELMo向量：<ul><li>问题：$\{e_t^Q\}_{t=1}^m$，$\{c_t^Q\}_{t=1}^m$</li><li>文章：$\{e_t^P\}_{t=1}^n$，$\{c_t^P\}_{t=1}^n$</li></ul></li><li>输出：original context representation<ul><li>BiLSTM 的输出结果，再次和ELMo 串联concat</li><li>问题：Q = $u_t^Q = [BiLSTM_Q(e_t^Q, c_t^Q), c_t^Q]$</li><li>文章：P = $u_t^P = [BiLSTM_P(e_t^P, c_t^P), c_t^P]$</li></ul></li></ul><h3 id="2-Hierarchical-Attention-Fusion-Layer"><a href="#2-Hierarchical-Attention-Fusion-Layer" class="headerlink" title="2.Hierarchical Attention Fusion Layer"></a>2.Hierarchical Attention Fusion Layer</h3><ul><li>original context representation和通过attention得到的aligned representation可以反映上下文不同粒度的语义</li></ul><h4 id="2-1-Co-attention-amp-Fusion"><a href="#2-1-Co-attention-amp-Fusion" class="headerlink" title="2.1 Co-attention &amp; Fusion"></a>2.1 Co-attention &amp; Fusion</h4><ul><li>co-attention 过程：<ul><li>计算一个soft-alignment 矩阵：<ul><li><script type="math/tex; mode=display">S_{ij} = Att(u_t^Q, u_t^P) = ReLU(W^T_{lin}u_t^Q)^T \cdot ReLU(W^T_{lin}u_t^P)</script></li></ul></li><li>计算 P2Q attention<ul><li><script type="math/tex; mode=display">\alpha_{j} = softmax(S_{:j})</script></li><li><script type="math/tex; mode=display">\tilde{Q}_{:t} = \sum_j \alpha_{ij} \cdot Q_{:j}, \forall j \in [1,...,m]</script></li></ul></li><li>计算 Q2P attention<ul><li><script type="math/tex; mode=display">\beta_j = softmax(S_{i:})</script></li><li><script type="math/tex; mode=display">\tilde{P}_{k:} = \sum_i \beta_{ik} \cdot P_{i:}, \forall i \in [1,..,n]</script></li></ul></li></ul></li><li><font color="red"><strong>Fusion</strong></font>：本文重点<ul><li>定义fusion kernel<ul><li>$P^\prime=Fuse(P,\tilde{Q})$</li><li>$Q^\prime=Fuse(Q,\tilde{P})$</li></ul></li><li>simple fusion 过程：<ul><li><script type="math/tex; mode=display">m(P,\tilde{Q}) = tanh(W_f[P;\tilde{Q};P\circ\tilde{Q};P-\tilde{Q}] + b_f)</script></li></ul></li><li>输出：利用gate机制，融合/refine 注意力表示和original contextual 表示<ul><li>希望利用 original context representation 提供的重要的 global level 的语义信息提供指导，进一步引入gate机制控制不同层次表示的融合</li><li>文档：$P^\prime = g(P,\tilde{Q})\cdot m(P,\tilde{Q}) + (1-g(P, \tilde{Q})）\cdot P$</li><li>问题：$Q^\prime = g(Q,\tilde{P})\cdot m(Q,\tilde{P}) + (1-g(Q, \tilde{P})）\cdot Q$</li><li>$g(\cdot)$的定义见2.3</li></ul></li></ul></li></ul><h4 id="2-2-Self-attention-amp-Fusion"><a href="#2-2-Self-attention-amp-Fusion" class="headerlink" title="2.2 Self-attention &amp; Fusion"></a>2.2 Self-attention &amp; Fusion</h4><p>文档的self-attention fusion过程：</p><ul><li>首先将manual feature引入，与refined question-aware passage表示进行串接<ul><li>$D = BiLSTM([P^\prime; feat_{man}])$</li></ul></li><li>self-alignment <strong>fusion</strong> process<ul><li>$L = softmax(D \cdot W_1 \cdot D^T)$</li><li>$\tilde{D} = L \cdot D$</li><li>$D^\prime = Fuse(D,\tilde{D})$</li></ul></li><li>双向LSTM获得最终的上下文文档表示：<ul><li>$D^{\prime\prime} = BiLSTM(D^\prime)$</li></ul></li></ul><p>问题端的self-attention fusion过程</p><ul><li>获得新的问题上下文表示：$Q^{\prime\prime} = BiLSTM(Q^\prime)$<ul><li>$Q^\prime$ 来自于co-attention+fusion的结果</li></ul></li><li>linear self-alignment, 使用 linear transformation (linear sequence attention，同drqa中的问题编码)，将问题编码为向量<ul><li>$\gamma = softmax(w_q^T Q^{\prime\prime})$</li><li>$q = \sum_{j} \gamma_j \cdot Q^{\prime\prime}, \forall j \in [1,…,m]$</li></ul></li></ul><h4 id="2-3-Fusion-Functions"><a href="#2-3-Fusion-Functions" class="headerlink" title="2.3 Fusion Functions"></a>2.3 Fusion Functions</h4><ul><li>simple concat：简单讲两个channel的输入进行串联（计算m(·)时）</li><li>Full Projection：heuristic matching，形如<ul><li>$[P;\tilde{Q};P\circ\tilde{Q};P-\tilde{Q}]$</li></ul></li><li>Scalar-based fusion：训练一个标量参数<ul><li>$g(P,\tilde{Q}) = g_p$</li></ul></li><li><font color="green"><strong>Vector-based Fusion</strong></font>: 效果最好的<ul><li>$g(P,\tilde{Q}) = \sigma(w_g^T \cdot [P;\tilde{Q};P\circ\tilde{Q};P-\tilde{Q}] + b_g)$</li><li>$w_g$是待训练的权重向量</li></ul></li><li>Matrix-based Fusion：<ul><li>$g(P,\tilde{Q}) = \sigma(W_g^T \cdot [P;\tilde{Q};P\circ\tilde{Q};P-\tilde{Q}] + b_g)$</li><li>$W_g$是待训练的权重矩阵</li></ul></li></ul><h3 id="3-Output-Layer"><a href="#3-Output-Layer" class="headerlink" title="3.Output Layer"></a>3.Output Layer</h3><ul><li>bilinear match function<ul><li>$P_{start} = softmax(q \cdot W_s^T D^{\prime\prime})$</li><li>$P_{end} = softmax(q \cdot W_e^T D^{\prime\prime})$</li><li>可以看做是对问题的回顾</li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><ul><li>ELMo 与上下文表示融合的新思路</li><li><strong>原始的context representation可以看做是global level的信息，在fusion中的作用很大</strong>(每一步的fusion都是将attention representation与original context representation进行融合)，利用original context representation对attention之后的表示进行融合，微调带有注意力的表示</li><li>计算attention时，采取 linear + relu 的bilinear的效果最优</li><li>输出层的 <font color="blue"><strong>bilinear match function</strong></font> 对结果的提升很大</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EMNLP2018 | Commonsense for Generative Multi-Hop Question Answering Tasks</title>
      <link href="/2019/02/21/paper-emnlp2018-mhpgm/"/>
      <url>/2019/02/21/paper-emnlp2018-mhpgm/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Commonsense for Generative Multi-Hop Question Answering Tasks<br>EMNLP 2018<br>Lisa Bauer et al. UNC.<br>Multi-Hop reasoning; generative-mrc; commmonsense knowledge<br>Datasets: NarrativeQA; QAngaroo-WikiHop<br>Source code: <a href="https://github.com/yicheng-w/CommonSenseMultiHopQA" target="_blank" rel="noopener">https://github.com/yicheng-w/CommonSenseMultiHopQA</a></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>Multi-Hop Generative Task (<em>e.g. NarrativeQA</em>) requires models to <strong>reasong, gather and synthesize disjoint pieces of information</strong> within the context to generate an answer.</li><li>This type of multi-step reasoning requires understanding <strong>implicit relations</strong> (<em>external, background commonsense knowledge</em>).</li><li>Related work:<ul><li>some <strong>fact-based</strong> datasets (e.g. SQuAD) do not need to place heavy emphasis on multi-step reasoning capabilities.</li><li>some multi-hop datasets (e.g. QAngaroo) prompt a strong focus on multi-hop reasoning in very long texts.<ul><li>QAngaroo is an extractive dataset where answers are guaranteed to be spans within the context, thus, it more focuse on <strong>fact finding</strong> and <strong>linking</strong>.</li></ul></li></ul></li><li>This work:<ul><li>a. a strong generative baseline, <strong>Multi-Hop Pointer-Generator Model</strong>, uses a <strong>multi-attention</strong> to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer.</li><li>b. a novel system for <strong>selecting grounded multi-hop relational commonsense information</strong> from ConceptNet via a <strong>pointwise mutual information</strong> and <strong>term-frequency</strong> based scoring function.</li><li>c. a <strong>selectively gated attention</strong> mechanism to insert <strong>selected commonsense paths</strong> between the hops of document-context reasoning.</li><li>multi-hop commonsense paths: multiple connected edges within ConceptNet graph that give us information beyond a single relationship triple.</li><li>different aspects of the commonsense relationship path at each hop to bridge different inference gaps in the multi-hop task.</li></ul></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul><li>Model Overview:<ul><li><img src="/images/paper-emnlp2018-mhpgm/mhpgm-1-overview.png" alt="MHPGM"></li></ul></li></ul><h3 id="1-Multi-Hop-Pointer-Generator-Baseline"><a href="#1-Multi-Hop-Pointer-Generator-Baseline" class="headerlink" title="1.Multi-Hop Pointer-Generator Baseline"></a>1.Multi-Hop Pointer-Generator Baseline</h3><p>模型的输入:</p><ul><li>context: $X^C={w_1^C, w_2^C,…,w_n^C}$</li><li>query: $X^Q = {w_q^Q, w_2^Q,…,w_m^Q}$</li></ul><p>模型的输出:</p><ul><li>answer tokens: $X^a = {w_1^a,w_2^a,…,w_p^a}$</li></ul><h4 id="1-1-Embedding-Layer"><a href="#1-1-Embedding-Layer" class="headerlink" title="1.1.Embedding Layer"></a>1.1.Embedding Layer</h4><p>pretrained word embeddings with ELMO: $e_i^Q$/$e_i^C$ $\in \mathbb{R}^{d+1024}$</p><h4 id="1-2-Reasoning-Layer"><a href="#1-2-Reasoning-Layer" class="headerlink" title="1.2.Reasoning Layer"></a>1.2.Reasoning Layer</h4><p>$k$ 个 reasoning cell 增量式地更新 context representation.</p><p>第 $t$ 个reasoning cell的计算过程:</p><ul><li>输入: 前一时刻的输出 $\lbrace c_i^{t-1} \rbrace _{i=1}^n$ 和 query 的向量 $\lbrace e_i^Q \rbrace _{i=1}^m$</li><li>a.通过 cell-specific Bi-LSTM 计算一个 step-specific 的 context 和 query 的编码<ul><li>$u^t=BiLSTM(c^{t-1})$; $v_t=BiLSTM(e^Q)$</li></ul></li><li>b.通过bi-attention 计算 context 中的相关方面 来模拟一个hop的推理:<ul><li>context-to-query attention:<ul><li>$S_{ij}^t = W_1^t u_i^t + W_2^t v_j^t + W_3^t (u_i^t \odot v_j^t)$</li><li><script type="math/tex; mode=display">p_{ij}^t = \frac{exp(S_{ij}^t)}{\sum_{k=1}^m exp(S_{ik}^t)}</script></li><li><script type="math/tex; mode=display">(c_q)^t_i = \sum_{j=1}^m p_{ij}^t v_j^t \in \mathbb{R}^{n \times dim}</script></li></ul></li><li>query-to-context attention vector:<ul><li>$m_i^t = \max_{1 \leq j\leq m} S_{ij}^t$</li><li><script type="math/tex; mode=display">p_i^t = \frac{exp(m_i^t)}{\sum_{j=1}^n exp(m_j^t)}</script></li><li><script type="math/tex; mode=display">q_c^t = \sum_{i=1}^n p_i^t u_i^t \in \mathbb{R}^{dim}</script></li></ul></li><li>更新的context representation:<ul><li>$c_i^t = [u_i^t; (c_q)_i^t; u_i^t \odot (c_q)_i^t; q_c^t \odot (c_q)_i^t]$</li></ul></li><li>$c^0 = e^C$</li><li>最后一时刻的输出是 $c^k$</li></ul></li></ul><h4 id="1-3-Self-Attention-Layer"><a href="#1-3-Self-Attention-Layer" class="headerlink" title="1.3.Self-Attention Layer"></a>1.3.Self-Attention Layer</h4><ul><li>帮助模型处理long context中的长期依赖</li><li>输入是 reasoning 层最后一时刻的输出 经过 一个BiLSTM之后的表示 $c^{SA}$</li><li>计算流程：<ul><li>$S_{ij}^{SA} = W_4 c_i^{SA} + W_5 c_j^{SA} + W_6(c_i^{SA}\odot c_j^{SA})$<!-- * $$p_{ij}^{SA} = \frac{exp(S_{ij}^{SA})}{\sum_{k=1}^n exp(S_{ij}^{SA})}$$ --></li><li>$p_{ij}^{SA} = exp(S_{ij}^{SA}) / \left( \sum_{k=1}^n exp(S_{ij}^{SA}) \right)$</li><li>$c^\prime = \sum_{j=1}^n p_{ij}^{SA} c_j^{SA}$</li><li>$c^{\prime\prime} = BiLSTM([c^\prime;c^{SA};c^\prime \odot c^{SA}])$</li><li>$c = c^k + c^{\prime\prime}$</li></ul></li></ul><h4 id="1-4-Pointer-Generator-Decoding-Layer"><a href="#1-4-Pointer-Generator-Decoding-Layer" class="headerlink" title="1.4.Pointer-Generator Decoding Layer"></a>1.4.Pointer-Generator Decoding Layer</h4><ul><li><p>输入: </p><ul><li>$x_t$, 前一时刻解码出的词向量表示</li><li>$s_{t-1}$, 前一时刻的隐藏层状态</li><li>$a_{t-1}$, 上下文向量</li></ul></li><li><p>计算:</p><ul><li>当前时刻的隐藏层状态:<ul><li>$s_t = LSTM([x_t; a_{t-1}], s_{t-1})$</li></ul></li><li>计算在生成词典上的概率分布:<ul><li>$P_{gen} = softmax(W_{gen}s_t + b_{gen})$</li></ul></li><li>计算 attention (使用 Bahdanau 的attention计算过程):<ul><li><script type="math/tex; mode=display">\alpha_i = v^\intercal tanh(W_c c_i + W_s s_t + b_{attn})</script></li><li><script type="math/tex; mode=display">\hat{\alpha}_i = \frac{exp(\alpha_i)}{\sum_{j=1}^n exp(\alpha_j)}</script></li><li><script type="math/tex; mode=display">\mathbf{a}_i = \sum_{i=1}^n \hat{\alpha}_i c_i</script></li></ul></li><li>计算选择生成还是复制的概率:<ul><li>$\mathbf{o} = \sigma(W_a \mathbf{a}_t + W_x x_{t} + W_s s_t + b_{ptr})$</li><li>$\mathbf{p}^{sel} = softmax(\mathbf{o}) \in \mathbb{R}^2$</li></ul></li><li>最终 $t$ 时刻输出的分布为:<ul><li><script type="math/tex; mode=display">P_t(w) = p_1^{sel} P_{gen}(w) + p_2^{sel} \sum_{i:w_i^C=w} \hat{\alpha}_i</script></li></ul></li></ul></li></ul><h3 id="2-Commonsense-Selection-and-Representation"><a href="#2-Commonsense-Selection-and-Representation" class="headerlink" title="2.Commonsense Selection and Representation"></a>2.Commonsense Selection and Representation</h3><p>为什么需要常识知识：知识关系有时候没有直接在文本中指出</p><p>由于语义网络/知识图谱的规模较大，包含很多无关信息，需要设计有效的选择算法来确定需要的信息 (有用的知识且可以在context-query对中落地(<em>grounded</em>:在context-query中出现) )</p><ul><li><p>常识知识选择策略：包含两方面</p><ul><li>1.通过<strong>树结构</strong>，收集潜在的相关知识，目的是选择出具有high recall的候选推理路径；</li><li>2.通过<strong>三步</strong>打分策略对候选路径进行排序和过滤，以确保加入信息的质量和多样性；<ul><li>Initial Node Scoring, Cumulative Node Scoring, Path Selection</li></ul></li></ul></li><li><p>图: Commonsense selection approach</p><ul><li><img src="/images/paper-emnlp2018-mhpgm/mhpgm-2-path.png" alt="Commonsense selection approach"></li></ul></li></ul><h4 id="2-1-Tree-Construction"><a href="#2-1-Tree-Construction" class="headerlink" title="2.1.Tree Construction"></a>2.1.Tree Construction</h4><p>树的根节点为query中的一个词, 通过一些分支操作来构建多步推理路径.针对问题中的一个词/concept $c_1$, 进行如下操作:</p><ol><li>Direct Interaction:方向交互<ul><li>从ConceptNet中选择与$c_1$和文本上下文中concept( $c_2 \in C$ )有直接链接的关系$r_1$(多个)</li><li>例如图中的第一列</li></ul></li><li>Multi-Hop<ul><li>继续在ConceptNet中选择文本中另外的concept($c_3 \in C$)与$c_2$有链接的关系$r_2$</li></ul></li><li>Outside Knowledge<ul><li>无文本上下文的约束,寻找收集外部知识</li><li>在ConceptNet中通过关系$r_3$寻找$c_3$的邻居(one-hop)得到 $c_4 \in nbh(c_3)$</li></ul></li><li>Context-Grounding<ul><li>再次利用context进行约束, 来确保3中额外的知识是对任务有帮助的</li><li>即, 使$c_3$通过$c_4$可找到的二阶邻居$c_5 \in C$是在文本中出现的</li></ul></li></ol><ul><li>这构建路径的过程中，有几点疑问:<ul><li>a. 如果针对一个query中的concept无法查找到这么长的路径如何处理？</li></ul></li></ul><h4 id="2-2-Rank-and-Filter"><a href="#2-2-Rank-and-Filter" class="headerlink" title="2.2.Rank and Filter"></a>2.2.Rank and Filter</h4><p>在构建树的过程中, 会收集大量的潜在相关路径, 相应地会引入很多噪声以及与问题无关的路径, 所以需要对2.1中收集到的知识路径进行排序过滤掉噪音.</p><ol><li>Initial Node Scoring:<ul><li>初始化节点分数</li><li>选择有节点可以提供对context的理解的重要信息的路径</li><li>使用 tf 来估计context中concept的重要度和显著度:<ul><li>$score(c) = \frac{count(c)}{|C|}, c \in \lbrace c_2,c_3,c_5 \rbrace$</li></ul></li><li>对于outside Knowledge选择出来的节点$c_4$, 希望它与推理路径中的$c_1$到$c_3$保持<strong>逻辑一致性</strong><ul><li>利用点互信息</li><li><script type="math/tex; mode=display">PMI(c_4,c_{1-3}) = log(\frac{\mathbb{P}(c_4,c_{1-3})}{\mathbb{P}(c_4) \mathbb{P}(c_{1-3})})</script></li><li><script type="math/tex; mode=display">\mathbb{P}(c_4,c_{1-3}) = \frac{Num.\ of\ paths\ connecting\ c_1,c_2,c_3,c_4}{Num.\ of\ distinct\ paths\ of\ length\ 4}</script></li><li><script type="math/tex; mode=display">\mathbb{P}(c_4) = \frac{Num.\ of\ nodes\ that\ can\ reach\ c_4}{|ConceptNet|}</script></li><li><script type="math/tex; mode=display">\mathbb{P}(c_{1-3}) = \frac{Num.\ of\ paths\ connecting\ c_1,c_2,c_3}{Num.\ of\ paths\ of\ length\ 3}</script></li><li>由于PMI对low-frequency敏感, 改进为 normalized PMI<ul><li>$score(c_4) = PMI(c_4, c_{1-3}) / (-log\mathbb{P}(c_4,c_{1-3}))$</li></ul></li></ul></li><li>由于每个连接分支代表了one-hop，所以不同层级的hops或者是拥有不同父节点的nodes无法与其他节点相比较, 最终对每个节点的打分进行归一化:<ul><li>在同级(siblings)节点中进行归一化</li><li>$n-socre(c) = softmax_{siblings(c)}(score(c))$</li></ul></li></ul></li><li>Cumulative Node Scoring<ul><li>累积节点打分</li><li>选择多跳的Commonsense路径中有相关信息的,根据节点以其子节点的相关性和显著性对节点进行再次打分</li><li>采取bottom-up的累积计算方式</li><li>对于叶子节点(leaf node):<ul><li>$c-socre = n-score$</li></ul></li><li>对于非叶子节点<ul><li>$c-score(c_l) = n-socre(c_l) + f(c_l)$</li><li>$f(c_l)$是该节点的$c-socre$打分top-2的子节点的$c-score$平均分值</li></ul></li></ul></li><li>Path Selection<ul><li>路径选择</li><li>采取top-down breath-first(广度优先)的方式选择路径</li><li>从根节点(回顾: query中的一个concept)开始, 递归的选择其两个具有最高累计得分的子节点, 直到选择到叶子节点.</li><li>选择路径数: $2^4=16 paths$</li><li>路径直接以token序列的方式传给模型</li></ul></li></ol><h3 id="3-Commonsense-Model-Incorporation"><a href="#3-Commonsense-Model-Incorporation" class="headerlink" title="3.Commonsense Model Incorporation"></a>3.Commonsense Model Incorporation</h3><p>有选择性的结合需要的知识，使用常识知识来弥补推理步之间的gaps.<br>提出 Necessary and Optional Information Cell (NOIC) 一种 selectively gated 注意力机制。</p><ul><li><p>输入:以词序列的形式表示给定的 list of commonsense logic paths:</p><ul><li>$X^{CS} = {w_1^{CS}, w_2^{CS},…,w_l^{CS}}$</li><li>$w_l^{CS}$表示构成一条路径的token序列</li><li>使用词向量表示: $e_{ij}^{CS}\in \mathbb{R}^d$</li></ul></li><li><p>NOIC 是基于 Baseline Reasoning Cell 的扩展:</p><ul><li>在第 $t$ 个推理步, 得到 baseline reasoning cell 的输出之后, 为Commonsense信息计算cell-specific表示:<ul><li>将Commonsense paths上的所有向量串接, 每条路径得到一个表示向量$u_i^{CS}$</li><li>通过映射进行降维, 使维度与 $c_i^t$ 的维度相等<ul><li>$v_i^{CS} = ReLU(W u_i^{CS} + b)$</li></ul></li><li>利用attention机制使Commonsense与context之间进行交互:<ul><li><script type="math/tex; mode=display">S_i^{CS}=W_1^{CS}c_i^t + W_2^{CS} v_j^{CS} + W_3^{CS} (c_i^t \odot v_j^{CS})</script></li><li><script type="math/tex; mode=display">p_{ij}^{CS} = \frac{exp(S_{ij}^{CS})}{\sum_{k=1}^l exp(S_{ij}^{CS})}</script></li><li><script type="math/tex; mode=display">c_i^{CS} = \sum_{j=1}^l p_{ij}^{CS} v_j^{CS}</script></li></ul></li><li>最后, 将Commonsense-aware的context表示和baseline reasoning cell的结果进行组合得到NOIC的输出<ul><li><script type="math/tex; mode=display">z_i = \sigma(W_z [c_i^{CS}; c_i^t] + b_z)</script></li><li><script type="math/tex; mode=display">(c_o)_i^t = z_i \odot c_i^t + (1-z_i) \odot c_i^{CS}</script></li></ul></li></ul></li><li>通过这种方式做到在每一个推理步对知识进行有选择的结合</li></ul></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h3 id="Ablations"><a href="#Ablations" class="headerlink" title="Ablations"></a>Ablations</h3><ol><li>Model Ablations</li><li>Commonsense Ablations</li><li>Commonsense Selection</li></ol><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><p>TBA</p>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> multi-hop </tag>
            
            <tag> reasoning </tag>
            
            <tag> commonsense </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Commonsense to Controllable TG</title>
      <link href="/2019/01/23/paper-tg-with-commonsense/"/>
      <url>/2019/01/23/paper-tg-with-commonsense/</url>
      
        <content type="html"><![CDATA[<p>本文将介绍 两篇 关于利用常识知识进行可控地文本生成的论文，都是清华大学黄民烈老师组的研究工作：</p><blockquote><p>Commonsense Knowledge Aware Conversation Generation with Graph Attention. IJCAI,2018.<br>Story Ending Generation with Incremental Encoding and Commonsense Knowledge. AAAI,2019.</p></blockquote><p>相比于文本生成问题，我更关注怎样将常识知识引入到文本的理解，即编码过程。</p><h2 id="1-CS-Know-to-Dialogue-with-GAT"><a href="#1-CS-Know-to-Dialogue-with-GAT" class="headerlink" title="1.CS Know. to Dialogue with GAT"></a>1.CS Know. to Dialogue with GAT</h2><p>这篇文章的主要目的是利用大规模的常识知识帮助对话的理解与生产。<br>在对话任务中，给定一个用户 post：</p><ul><li>在编码端：采取静态策略<ul><li>从(常识)知识库中抽取出与 post 相关的知识子图</li><li>在编码端知识子图是静态的，包含与 post 相关的信息</li><li>用 Graph Attention(GAT) 机制进行编码</li></ul></li><li>在解码端：采取动态策略<ul><li>有关注的读取 Knowledge Graph 以及 Knowledge Graph 中的知识三元组</li></ul></li><li>文章中强调了一点：与已有的其他模型(分开、独立使用 knowledge triples/entities)不同，此模型将每个 knowledge graph 作为一个整体进行处理，可以编码更多的结构信息以及有关联的语义信息。</li></ul><p>模型的整体结构: <img src="/images/paper-tg-with-commonsense/tg-ccm-architecture.png" alt="CCM-overview"></p><h3 id="Task-Definition-and-Overview"><a href="#Task-Definition-and-Overview" class="headerlink" title="Task Definition and Overview"></a>Task Definition and Overview</h3><ul><li>user post(input): $X=x_1 x_2 … x_n$</li><li>some commonsense knowledge graphs: $G={g_1, g_2, …, g_{N_G}}$</li><li>desired response(output): $Y=y_1 y_2 … y_m$</li><li>使用 post 中的每个词作为查询，去常识知识库中 为每个词抽取出一个对应的子图:<ul><li>eg: 对于 post $X=x_1 x_2 … x_n$, 其对应的抽取出来的图是 $G={g_1, g_2,…,g_{N_G}}$</li><li>每个子图由一个三元组集合构成: $g_i = {\tau_1, \tau_2, …, \tau_{N_{g_i}}}$</li><li>每个三元组(head entity, relation, tail entity): $\tau = (h,r,t)$</li><li>使用 TransE 来表示 KG 中的实体和关系;</li><li>最终每个三元组 $\tau$ 表示为: <ul><li>$k = (h,r,t) = MLP(TransE(h,r,t))$<ul><li>$h/r/t$ 为各自的 TransE Embedding</li><li>使用 $MLP$ 是为了缩小 知识库 和 无结构对话文本 之间的 表示差距(bridge the representation gap)</li></ul></li></ul></li><li>对于在知识库中没有检索到匹配的词，使用一个特殊的 <code>Not_A_Fact</code> 来表示</li></ul></li></ul><h3 id="Knowledge-Interpreter"><a href="#Knowledge-Interpreter" class="headerlink" title="Knowledge Interpreter"></a>Knowledge Interpreter</h3><p>Knowledge Interpreter overview: <img src="/images/paper-tg-with-commonsense/tg-ccm-enc-1.png" alt="KI-overview"><br><!-- ![ki-ov](/images/tg-ccm-enc-1.png) --></p><ul><li>知识感知的词表示: $e(x_t) = [w(x_t);g_i]$</li><li>Knowledge Interpreter 的主要目的是用每个词对应的知识子图向量来增强词表示<ul><li>使用 post 中的每个词 $w_t$ 作为 key entity (红色点)去抽取出一个子图 $g_i$ (上图中黄色的部分)</li><li>knowledge interpreter 的输出为静态图注意力机制计算得到的 graph vector $g_i$</li></ul></li><li>Static Graph Attention<ul><li>使用功能 GAT 的好处是：不仅可以编码结构语义信息还可以考虑到图中节点间的关系</li><li>SGA 的输入: knowledge triple vectors<ul><li><script type="math/tex; mode=display">K(g_i) = {k_1, k_2, ..., k_{N_{g_i}}}</script></li></ul></li><li>SGA 的输出: knowledge graph vector<ul><li><script type="math/tex; mode=display">g_i = \sum_{n=1}^{N_{g_i}} \alpha_n^s [h_n;h_t]</script></li><li><script type="math/tex; mode=display">\alpha_n^s = \frac{exp(\beta_n^s)}{\sum_{j=1}^{N_{g_i}} exp(\beta_j^s)}</script></li><li><script type="math/tex; mode=display">\beta_n^s = (W_r r_n)^T tanh(W_h h_n + W_t t_n)</script></li><li>其中, $(h_n, r_n, t_n) = k_n$</li><li>(Note: 在原始的GAT中没有引入relation向量计算attention的score)</li></ul></li></ul></li></ul><h2 id="2-CS-Know-to-Story-Ending-Generation"><a href="#2-CS-Know-to-Story-Ending-Generation" class="headerlink" title="2.CS Know. to Story Ending Generation"></a>2.CS Know. to Story Ending Generation</h2><p>这篇文章的主要目的是生成连贯、合理且有逻辑的故事结尾。</p><ul><li>故事例子：故事与知识的关联<ul><li><img src="/images/paper-tg-with-commonsense/story-data-example.png" alt="story-data-example"></li></ul></li></ul><p>动机：</p><ul><li>为故事生成一个合理的结尾需要对故事有很强的理解，需要以下两个方面</li><li>Context Clues：故事的逻辑、因果关系以及时序依赖（logic/causality/chronological order）通常跨越多个句子，通过句子中的事件/实体序列来获取；</li><li>Implicit/Commonsense Knowledge：超越文本表明信息的隐含知识</li></ul><p>针对以上的需求/目的，本文提出了：</p><ul><li>Incremental Encoding Schema：表示文本线索<ul><li>逐句编码</li></ul></li><li>Multi-Source Attention：帮助故事理解，充分利用常识知识<ul><li>为每个词抽取一个 one-hop Knowledge graph，计算graph的表示</li><li>在编码当前句子时，不仅对前一个句子中的每个词的上下文表示进行Attention，还对每个词的KG进行Attention</li></ul></li></ul><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>模型的整体结构: <img src="/images/paper-tg-with-commonsense/story-architecture.png" alt="storygen-model"></p><h3 id="Incremental-Encoding-Scheme"><a href="#Incremental-Encoding-Scheme" class="headerlink" title="Incremental Encoding Scheme"></a>Incremental Encoding Scheme</h3><p>最直接的对故事进行上下文编码的方法有：</p><ul><li>将所有的句子进行串联，构成一个长句子，然后用LSTM进行计算</li><li>使用带有 Hierarchical-Attention 的层次LSTM</li><li>但是这两种都不能够有效的表示上下文的线索信息，相邻句子中事件/实体的时序顺序(chronological order)和因果关系(causal relationship)</li></ul><p>本文提出的增量编码模式：编码当前句子 $X_i$ 中的第j个位置的隐状态计算如下</p><ul><li>$h_j^{(i)} = LSTM(h_{j-1}^{(i)}, e(x_j^{(i)}), c_{lj}^{(i)}), i&gt;2$<ul><li>其中，$e(\cdot)是词向量$， $c_{lj}^{(i)}$ 是关于前一个句子的上下文向量（基于隐状态$h_{j-1}^{(i)}$）</li></ul></li></ul><h3 id="Multi-Source-Attention"><a href="#Multi-Source-Attention" class="headerlink" title="Multi-Source Attention"></a>Multi-Source Attention</h3><p>通过多源Attention来获取上下文向量表示文本线索信息，由两部分组成：</p><ul><li>$c_{lj}^{(i)} = W_l( [c_{hj}^{(i)} ; c_{xj}^{(i)}] ) + b_l$</li><li>$c_{hj}^{(i)}$ 是 state context vector，即前一个句子隐状态的权重和<ul><li><script type="math/tex; mode=display">c_{hj}^{(i)}  = \sum_{k=1}^{l_{i-1}} \alpha_{hk}^{(i)} h_k^{(i-1)}</script></li><li><script type="math/tex; mode=display">\alpha_{hk}^{(i)} = \frac{ exp( \beta_{h_{k,j}}^{(i)} ) }{ \sum_{m=1}^{l_{i-1}} exp( \beta_{h_{m,j}}^{(i)} ) }</script></li><li><script type="math/tex; mode=display">\beta_{h_{k,j}}^{(i)} = h_{j-1}^{(i)} W_s h_k^{(i-1)}</script></li><li>$h_k^{(i-1)}$ 前一个句子第k时刻的隐状态</li></ul></li><li>$c_{xj}^{(i)}$ 是 knowledge context vector， 即前一个句子的知识表示权重和<ul><li><script type="math/tex; mode=display">c_{xj}^{(i)}  = \sum_{k=1}^{l_{i-1}} \alpha_{xk}^{(i)} g(x_k^{(i-1)})</script></li><li><script type="math/tex; mode=display">\alpha_{xk}^{(i)} = \frac{ exp( \beta_{x_{k,j}}^{(i)} ) }{ \sum_{m=1}^{l_{i-1}} exp( \beta_{x_{m,j}}^{(i)} ) }</script></li><li><script type="math/tex; mode=display">\beta_{x_{k,j}}^{(i)} = h_{j-1}^{(i)} W_k g(x_k^{(i-1)})</script></li><li>$g(x_k^{(i-1)})$ 是前一个句子中第k个词的graph vector</li><li>$h_{j-1}^{(i)}$ 是第i个句子中的第j个位置的隐状态，即当前句子中的前一时刻的隐状态</li></ul></li></ul><h3 id="Knowledge-Graph-Representation"><a href="#Knowledge-Graph-Representation" class="headerlink" title="Knowledge Graph Representation"></a>Knowledge Graph Representation</h3><p>对知识图编码有两种方式：</p><ul><li>graph attention</li><li>contextual attention (knowledgeable-reader)</li></ul><p>针对词$x$抽取（以$x$为头实体）出的图: $G(x) = {R_1,R_2, …,R_{N_x}}$</p><p>$R_i$ 为知识三元组 $(h,r,t)$， 用词向量表示h/t， r向量作为参数进行学习；<br>$N_x$ 为 $x$ 的邻居数量；</p><h4 id="Graph-Attention-的图表示方式"><a href="#Graph-Attention-的图表示方式" class="headerlink" title="Graph Attention 的图表示方式"></a>Graph Attention 的图表示方式</h4><ul><li><script type="math/tex; mode=display">g(x) = \sum_{i=1}^{N_x} \alpha_{R_i} [h_i；t_i]</script></li><li><script type="math/tex; mode=display">\alpha_{R_i} = \frac{ exp(\beta_{R_i}) }{ \sum_{j}^{N_x} exp(\beta_{R_j})}</script></li><li><script type="math/tex; mode=display">\beta_{R_i} = (W_r r_i)^T tanh( W_h h_i + W_t t_i )</script></li></ul><h4 id="Contextual-Attention-的图表示方式"><a href="#Contextual-Attention-的图表示方式" class="headerlink" title="Contextual Attention 的图表示方式"></a>Contextual Attention 的图表示方式</h4><ul><li><script type="math/tex; mode=display">g(x) = \sum_{i=1}^{N_x} \alpha_{R_i} M_{R_i}</script></li><li><script type="math/tex; mode=display">M_{R_i} = BiGRU(h_i, r_i, t_i)</script></li><li><script type="math/tex; mode=display">\alpha_{R_i} = \frac{ exp(\beta_{R_i}) }{ \sum_{j}^{N_x} exp(\beta_{R_j})}</script></li><li><script type="math/tex; mode=display">\beta_{R_i} =h_{(x)}^T W_c M_{R_i}</script></li><li>$h_{(x)}$ 是词$x$的隐状态</li></ul><h2 id="CS-Know-Process"><a href="#CS-Know-Process" class="headerlink" title="CS Know. Process"></a>CS Know. Process</h2><p>对于常识知识的抽取方式以及和数据的融合方式</p><ul><li>CCM中：<ul><li>实体和关系的向量通过transE进行学习，维度均为100</li><li>去除了由多个词构成的头/尾实体</li><li>最终的数据量：<ul><li>三元组：120850</li><li>实体数：21471</li><li>关系数：44</li></ul></li></ul></li><li>StoryEndGen中：<ul><li>只抽取出one-hop的三元组</li><li>为每个词抽取最多10个三元组;</li><li>头/尾实体为名词(noun)或动词(verb)</li><li>最终的数据量：<ul><li>关系数：45</li><li>三元组：16652</li></ul></li></ul></li><li>两篇论文都是直接给出了处理好的数据，并没有给出数据的预处理过程，github链接分别为：<ol><li><a href="https://github.com/tuxchow/ccm" target="_blank" rel="noopener">https://github.com/tuxchow/ccm</a></li><li><a href="https://github.com/JianGuanTHU/StoryEndGen" target="_blank" rel="noopener">https://github.com/JianGuanTHU/StoryEndGen</a></li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> text-generation </tag>
            
            <tag> note </tag>
            
            <tag> commonsense </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Survey about Works of MOSAIC</title>
      <link href="/2019/01/21/note-research-ai2-mosaic/"/>
      <url>/2019/01/21/note-research-ai2-mosaic/</url>
      
        <content type="html"><![CDATA[<p>MOSAIC 是 AI2 (ALLEN Institute for Artificial Intelligence) 研究院中进行机器常识智能研究的小组<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>.</p><p>现有的研究项目：(Update at July, 2019)</p><ol><li><strong>Commonsense Knowledge Graphs: Exploring semi-structured representations of commonsense.</strong><sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></li><li>Visual Commonsense Reasoning: 视觉常识推理项目<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></li><li>Mosaic Commonsense Benchmarks: Measuring progress on Machine Common Sense.<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup></li></ol><p>对应的主要Papers：</p><ol><li>ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. AAAI,2019.</li><li>From Recognition to Cognition: Visual Commonsense Reasoning</li><li>SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. EMNLP,2018.</li></ol><p>其他Papers:</p><ol><li>Event2Mind: Commonsense Inference on Events, Intents, and Reactions. ACL,2018.</li><li>Modeling Naive Psychology of Characters in Simple Commonsense Stories. ACL,2018.</li><li>Reasoning about Actions and State Changes by Injecting Commonsense Knowledge. EMNLP,2018.</li></ol><p>重点关注 <code>Commonsense Knowledge Graphs</code> 这部分的工作</p><h2 id="Commonsense-Knowledge-Graphs"><a href="#Commonsense-Knowledge-Graphs" class="headerlink" title="Commonsense Knowledge Graphs"></a>Commonsense Knowledge Graphs</h2><blockquote><ol><li>Event2Mind: Commonsense Inference on Events, Intents, and Reactions. ACL,2018.</li><li>ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. AAAI,2019.</li></ol></blockquote><h3 id="Event2Mind"><a href="#Event2Mind" class="headerlink" title="Event2Mind"></a>Event2Mind</h3><p>常识推理任务</p><p>推理类型：2类（语用推理 pragmatic inference）</p><ul><li><p>intent 意图</p><ul><li>可以解释为什么施事者导致事件发生，是行动或事件的心理前提</li></ul></li><li><p>emotional reaction 情绪反应</p><ul><li>反应定义为解释施事者和参与事件的其他人的心理状态将如何因此改变</li><li>反应可以被认为是一种行为或事件的心理后置条件</li></ul></li></ul><p>任务目标：给定一个 event phrase，预测 X intent / X reaction / Y reaction</p><ul><li>pre-condition: X intent</li><li>post-condition: X/Y reaction</li></ul><p>例子：</p><ul><li><img src="/images/note-research-ai2-mosaic/event2mind-example.png" alt="event2mind-example"></li></ul><h3 id="ATOMIC"><a href="#ATOMIC" class="headerlink" title="ATOMIC"></a>ATOMIC</h3><p>关于 if-then 推理类型的推理知识库（Knowledge base about inferential knowledge on if-then reasoning types）</p><ul><li>例子：<img src="/images/note-research-ai2-mosaic/atomic-example.png" alt="atomic-example"></li></ul><p>nine if-then relation types to distinguish: （不同上不全面的刻画维度）</p><ul><li><strong>cause vs. effect</strong><ul><li>[Xintent，Xneed] vs. [Xwant，Xreaction，Effect on X，Effect on Y，Ywant，Yreaction]</li></ul></li><li><strong>agents vs. themes</strong>：agent（X，who cause the event），themes（Other）<ul><li>[Xintent，Xneed，Xwant，Xreaction，Effect on X， Xattribute] vs. [Effect on Y，Ywant， Yreaction]</li></ul></li><li><strong>voluntary vs. involuntary</strong> events<ul><li>[Xintent，Xneed，Xwant，Ywant] vs. [Xreaction，Effect on X，Effect on Y，Yreaction，Xattribute]</li></ul></li><li><strong>actions vs. mental states</strong><ul><li>[] vs. []</li></ul></li><li><strong>dynamic vs. static</strong><ul><li>[Xintent，Xneed，Xwant，Xreaction，Effect on X，Effect on Y，Ywant，Yreaction] vs. [Xattribute]</li></ul></li></ul><p>taxonomy of if-then reasoning types:</p><ul><li>based on the content being predicted:<ul><li><font color="green">**if-event-then-mental-state**</font>* mental pre-/post- conditions of an event* 3 relations: <font color="blue">**X inent/ X reaction/ Other reaction**</font></li><li><font color="green">**if event-then event**</font>* events that constitute probable pre- and postconditions of a given event* 5 relations: <font color="blue">**X need/ X want/ Effect on X/ Other want/ Effect on Other**</font></li><li><font color="green">**if event-then-person**</font>* a stative relation that describes how the subject of an event is described or perceived* 1 relations: <font color="blue">**X attribute**</font></li></ul></li><li>based on their causal relations<ul><li>causes（因）</li><li>effect（果）</li><li>stative（状态）</li></ul></li><li>图例：<img src="/images/note-research-ai2-mosaic/atomic-taxonomy-example.png" alt="example"></li></ul><p>与现有knowledge base的对比：</p><ul><li><img src="/images/note-research-ai2-mosaic/atomic-comp.png" alt="kb-comp"></li></ul><h2 id="DARPA-Leaderboards"><a href="#DARPA-Leaderboards" class="headerlink" title="DARPA Leaderboards"></a>DARPA Leaderboards</h2><h3 id="Abductive-Natural-Language-inference-alpha-NLI"><a href="#Abductive-Natural-Language-inference-alpha-NLI" class="headerlink" title="Abductive Natural Language inference (alpha NLI)"></a>Abductive Natural Language inference (alpha NLI)</h3><h3 id="HellaSwag-Can-a-Machine-Really-Finish-Your-Sentence"><a href="#HellaSwag-Can-a-Machine-Really-Finish-Your-Sentence" class="headerlink" title="HellaSwag: Can a Machine Really Finish Your Sentence?"></a>HellaSwag: Can a Machine Really Finish Your Sentence?</h3><h3 id="Physical-IQa-Pyhsical-Interaction-QA"><a href="#Physical-IQa-Pyhsical-Interaction-QA" class="headerlink" title="Physical IQa: Pyhsical Interaction QA"></a>Physical IQa: Pyhsical Interaction QA</h3><h3 id="Social-IQA-Social-Interaction-QA"><a href="#Social-IQA-Social-Interaction-QA" class="headerlink" title="Social IQA: Social Interaction QA"></a>Social IQA: Social Interaction QA</h3><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">https://mosaic.allenai.org/</span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">https://mosaic.allenai.org/projects/commonsense-knowledge-graphs</span><a href="#fnref:2" rev="footnote"> ↩</a></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">3.</span><span style="display: inline-block; vertical-align: top;">https://visualcommonsense.com/</span><a href="#fnref:3" rev="footnote"> ↩</a></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">4.</span><span style="display: inline-block; vertical-align: top;">https://mosaic.allenai.org/projects/mosaic-commonsense-benchmarks</span><a href="#fnref:4" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> MRC-Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> research </tag>
            
            <tag> note </tag>
            
            <tag> cs-know </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>20190121 | 清华大学AI研究院知识智能研究中心发布会的记录</title>
      <link href="/2019/01/21/note-20190121-thukc/"/>
      <url>/2019/01/21/note-20190121-thukc/</url>
      
        <content type="html"><![CDATA[<p>Minute and thoughts of the launch conference of THUKC (清华大学人工智能研究院知识智能研究中心).<br>Mainly about the part of Commonsense-Awared Natural Language Generation (常识知识感知的语言生成, 黄民烈)</p><h2 id="Commonsense-Extraction"><a href="#Commonsense-Extraction" class="headerlink" title="Commonsense Extraction"></a>Commonsense Extraction</h2><ul><li>what is Commonsense Knowledge (CS Know.)</li><li>what is the Boundary of commonsense</li><li>Commonsense Extraction<ul><li>From Embedding<ul><li>Extracting Commonsense Properties from Embeddings with Limited Human Guidance. 2018</li></ul></li><li>Commonsense Knowledge base completion<ul><li>Commonsense Knowledge base Completion. 2018</li></ul></li><li>From RAW data(text, image)<ul><li>Automatic Extraction of Commonsense LocatedNear Knowledge. 2018</li></ul></li></ul></li></ul><h2 id="Commonsense-Knowledge-in-RC"><a href="#Commonsense-Knowledge-in-RC" class="headerlink" title="Commonsense Knowledge in RC"></a>Commonsense Knowledge in RC</h2><ul><li>代表工作：Knowledgeable Reader</li></ul><h2 id="Commonsense-Knowledge-to-Intent-Reaction-Emotion-etc"><a href="#Commonsense-Knowledge-to-Intent-Reaction-Emotion-etc" class="headerlink" title="Commonsense Knowledge to Intent, Reaction, Emotion, etc"></a>Commonsense Knowledge to Intent, Reaction, Emotion, etc</h2><ul><li>代表工作：<ul><li>Event2Mind: Commonsense Inference on Events, Intents and Reaction</li><li>Modeling Naive Psychology of Characters in Simple Commonsense Stories</li></ul></li><li>AI2 实验室做了大量这方面的工作，相关调研：<a href="/2019/01/21/note-research-ai2-mosaic/" title="link">link</a></li></ul><h2 id="Commonsense-to-Controllable-TG"><a href="#Commonsense-to-Controllable-TG" class="headerlink" title="Commonsense to Controllable TG"></a>Commonsense to Controllable TG</h2><ul><li>黄民烈老师组的一些很好的工作（黄老师称这些工作是常识知识感知的语言生成的初步探索）：<ul><li>Commonsense Knowledge Aware Conversation Generation with Graph Attention. IJCAI,2018.(distinguished paper)<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></li><li>Story Ending Generation with Incremental Encoding Commonsense Knowledge. AAAI,2019.</li><li>学习笔记：<a href="/2019/01/23/paper-tg-with-commonsense/" title="note_link">note_link</a></li></ul></li><li>Three Fundamental problems in current Neural Language Generation Models<ul><li>semantics(real understanding)</li><li>Consistency(long text generation)</li><li>Logic(reasonable and making sense)</li><li>在评价方面，关于如何评价生成的文本具有逻辑性还是个挑战（相关研究点：文本生成评价指标(BLEU、ROUGE等)的研究）</li></ul></li><li>New Architecture: <ul><li><strong>symbolic knowledge + planning + neural computing</strong></li></ul></li></ul><h2 id="发布的重要资源"><a href="#发布的重要资源" class="headerlink" title="发布的重要资源"></a>发布的重要资源</h2><p>知识智能研究中心：<a href="http://ai.tsinghua.edu.cn/kirc/#" target="_blank" rel="noopener">http://ai.tsinghua.edu.cn/kirc/#</a></p><ul><li>中英文跨语言百科知识图谱XLORE — <span id="inline-blue">世界知识</span><ul><li>特点：<ul><li>大规模跨语言百科知识图谱</li><li>通过融合维基百科和百度百科，并对百科知识进行结构化和跨语言链接构建而成。</li><li>以结构化形式描述客观世界中的概念、实例、属性及其丰富语义关系。</li><li>XLORE目前包含约247万概念、44.6万属性/关系、1628万实例和260万跨语言链接。</li></ul></li><li>项目地址：<a href="https://xlore.org/" target="_blank" rel="noopener">https://xlore.org/</a></li></ul></li><li>基于义原的开放语言知识库 OpenHowNet — <span id="inline-green">语言知识</span>、<span id="inline-orange">常识知识</span><ul><li>！HowNet 核心数据首次开源</li><li>项目地址：<a href="https://openhownet.thunlp.org/" target="_blank" rel="noopener">https://openhownet.thunlp.org/</a></li></ul></li></ul><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">https://juejin.im/post/5b6a9e085188251a8d37136d</span><a href="#fnref:1" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> MRC-Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> research </tag>
            
            <tag> report </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Natural Language Inference | Analysis and Paper List</title>
      <link href="/2019/01/16/nli-paper-info/"/>
      <url>/2019/01/16/nli-paper-info/</url>
      
        <content type="html"><![CDATA[<h2 id="NLI-Introduction"><a href="#NLI-Introduction" class="headerlink" title="NLI Introduction"></a>NLI Introduction</h2><h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><ul><li>词语匹配的多元性（同义词、一词多义）lexical gap、lexical variant；</li><li>短语匹配的结构性、多粒度计算语义；</li><li>文本匹配的层次性；</li><li>利用不相似部分的信息；</li><li>自然语言的基础现象：Variability of Semantic expression<ul><li>是语言歧义的对偶问题；</li><li>语义表达的多变性、语言结构的多样性；</li></ul></li></ul><h3 id="Approachs"><a href="#Approachs" class="headerlink" title="Approachs"></a>Approachs</h3><p>models on NLI(Natural Language Infernce), concluded as: <strong>sentence-encoding-based</strong> vs. <strong>interaction-based</strong></p><ul><li>基于encoding的基本框架：<ul><li>input encoding —&gt;concat vector —&gt;vector &amp; element-wise difference/product—&gt; prediction</li></ul></li><li>基于交互的基本框架：<ul><li>input encoding —&gt; inference(interaction) —&gt; composition —&gt; prediction</li></ul></li></ul><h3 id="Measure-of-Similarity"><a href="#Measure-of-Similarity" class="headerlink" title="Measure of Similarity"></a>Measure of Similarity</h3><h2 id="Models-of-Sentence-Encoding-Based"><a href="#Models-of-Sentence-Encoding-Based" class="headerlink" title="Models of Sentence Encoding Based"></a>Models of Sentence Encoding Based</h2><ul><li>BiLSTM-Max</li><li>NSE</li><li>Deep Gated Attn. BiLSTM</li><li>Residual Stacked Encoder</li><li>Reinforced Self-Attention Network</li><li>Distance-based Self-Attention Network</li><li>Hierarchical BiLSTM with Max Pooling</li><li>Dynamic Self-Attention Model</li></ul><h2 id="Models-of-Interaction-Based"><a href="#Models-of-Interaction-Based" class="headerlink" title="Models of Interaction Based"></a>Models of Interaction Based</h2><ul><li>Decomposable Attention</li><li>ESIM</li><li>KIM</li><li>Densely Interactive Inference Network (DIIN)</li><li>BIMPM</li><li>Multi-Way Attention</li><li>DR-BiLSTM</li><li>CAFE</li><li>Densely-Connected Recurrent and Co-Attentive Network</li><li>DMAN</li><li>SLRC</li><li>AFN</li></ul>]]></content>
      
      
      <categories>
          
          <category> NLI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> paper </tag>
            
            <tag> research </tag>
            
            <tag> nli </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WIP| Analysis of FEVER</title>
      <link href="/2019/01/15/mrc-analysis-fever/"/>
      <url>/2019/01/15/mrc-analysis-fever/</url>
      
        <content type="html"><![CDATA[<h2 id="FEVER-Introduction"><a href="#FEVER-Introduction" class="headerlink" title="FEVER Introduction"></a>FEVER Introduction</h2><ul><li>FEVER data: <a href="http://fever.ai/resources.html" target="_blank" rel="noopener">link</a><ul><li>包含 185,445 条断言(claims)</li></ul></li><li>FEVER shared Task(with NAACL 2018) info: <a href="http://fever.ai/task.html" target="_blank" rel="noopener">details here</a></li><li>FEVER official baseline code: <a href="https://github.com/sheffieldnlp/fever-naacl-2018" target="_blank" rel="noopener">github repo</a></li></ul><p>FEVER(Fact Extraction and VERification) 任务中，给定一个未经验证的断言(claim)，即一个句子，要求模型/系统从Wikipedia中找到对应的证据句(evidence)，来验证这个断言，是否可以被证实(<strong>SUPPORTED</strong>)、反驳(<strong>REFUTED</strong>)或是没有足够的信息判断(<strong>NOT ENOUGH INFO</strong>)。对于SUPPORTED和REFUTED的判断，需要给出证据句子。其中 16.82%的例子中，需要多个证据句子来进行判断，12.15%的情况下，证据句来源于多篇文档。</p><h3 id="Data-Statistics"><a href="#Data-Statistics" class="headerlink" title="Data Statistics"></a>Data Statistics</h3><div class="table-container"><table><thead><tr><th>split</th><th>SUPPORTED</th><th>REFUTED</th><th>NEI</th></tr></thead><tbody><tr><td>Train</td><td>80035</td><td>29775</td><td>35639</td></tr><tr><td>Dev</td><td>6666</td><td>6666</td><td>6666</td></tr><tr><td>Test</td><td>6666</td><td>6666</td><td>6666</td></tr></tbody></table></div><h3 id="Baseline-System"><a href="#Baseline-System" class="headerlink" title="Baseline System"></a>Baseline System</h3><p>baseline 系统是 pipelined 形式，由三部分组成: 1.document retrieval, 2.sentence-level evidence selection, 3.textual entailment.</p><p>其中文本蕴含识别(recognizing textual entailment)部分采用的是 谷歌的 Decomposable Attention 模型</p><h3 id="Score-Metrics"><a href="#Score-Metrics" class="headerlink" title="Score Metrics"></a>Score Metrics</h3><p>official scorer: <a href="https://github.com/sheffieldnlp/fever-scorer" target="_blank" rel="noopener">https://github.com/sheffieldnlp/fever-scorer</a></p><p>判断 claim 的类型(SUPPORTED/REFUTED/NOT ENOUGH INFO)是个三分类问题，对此使用 accuracy 来评价。<br>对于 SUPPORTED 和 REFUTED 的类别，还需要提供证据片段，对此使用 F1 来评价。</p><h2 id="FEVER-Shared-Task-Top-3-Systems-Solutions"><a href="#FEVER-Shared-Task-Top-3-Systems-Solutions" class="headerlink" title="FEVER Shared Task Top-3 Systems Solutions"></a>FEVER Shared Task Top-3 Systems Solutions</h2><blockquote><p>The Fact Extraction and VERification (FEVER) Shared Task</p></blockquote><h3 id="Top-1-UNC-NLP"><a href="#Top-1-UNC-NLP" class="headerlink" title="Top-1: UNC-NLP"></a>Top-1: UNC-NLP</h3><blockquote><p>Combining Fact Extraction and Verification with Neural Semantic Matching Networks<br>AAAI 2019</p></blockquote><h3 id="Top-2-UCL-Machine-Reading-Group"><a href="#Top-2-UCL-Machine-Reading-Group" class="headerlink" title="Top-2: UCL Machine Reading Group"></a>Top-2: UCL Machine Reading Group</h3><blockquote><p>UCL Machine Reading Group: Four Factor Framework For Fact Finding (HexaF)</p></blockquote><h3 id="Top-3-Athene-UKP-TU-Darmstadt"><a href="#Top-3-Athene-UKP-TU-Darmstadt" class="headerlink" title="Top-3: Athene UKP TU Darmstadt"></a>Top-3: Athene UKP TU Darmstadt</h3><blockquote><p>Multi-Sentence Textual Entailment for Claim Verification</p></blockquote><h3 id="Shared-Task-Overview"><a href="#Shared-Task-Overview" class="headerlink" title="Shared Task Overview"></a>Shared Task Overview</h3><h2 id="FEVER-Workshop-Notes"><a href="#FEVER-Workshop-Notes" class="headerlink" title="FEVER Workshop Notes"></a>FEVER Workshop Notes</h2><ul><li>FEVER workshop info: <a href="http://fever.ai/workshop.html" target="_blank" rel="noopener">details here</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> paper </tag>
            
            <tag> research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MRC 的评测与所需能力</title>
      <link href="/2019/01/14/mrc-measure-skills/"/>
      <url>/2019/01/14/mrc-measure-skills/</url>
      
        <content type="html"><![CDATA[<h2 id="Reading-Comprehension-Skills"><a href="#Reading-Comprehension-Skills" class="headerlink" title="Reading Comprehension Skills"></a>Reading Comprehension Skills</h2><blockquote><p>An Analysis of Prerequisite Skills for Reading Comprehension.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup><br>Prerequisite skills for reading comprehension: Multi-perspective analysis of mctest datasets and systems. AAAI,2017.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup><br>Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability. ACL,2017.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></p></blockquote><p>(从数据的角度)将阅读理解所需要的能力分为两类: <span id="inline-blue">认知能力(prerequisite skill)</span> 和 <span id="inline-blue">语言能力(readability)</span></p><h3 id="1-prerequisite-skills"><a href="#1-prerequisite-skills" class="headerlink" title="1.prerequisite skills"></a>1.prerequisite skills</h3><p>认知能力: measure different types of reasoning and knowledge required to answer the question</p><p>定义了 13 种认知能力<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>, 如下:</p><ol><li>object tracking 目标跟踪<ul><li>同时锁定和跟踪多个目标，如集合或个体，也被称为列举或枚举</li></ul></li><li>mathematical reasoning 数学推理<ul><li>能够完成统计或者量化操作</li></ul></li><li>coreference resolution 指代消解<ul><li>将指代词映射到相应的实体上</li></ul></li><li>logical reasoning 逻辑推理<ul><li>逻辑操作，例如对量词、否定、条件以及转移推理</li></ul></li><li>analogy 类比<ul><li>能够了解一些隐喻，如转喻和提喻</li></ul></li><li>causal relation 因果关系<ul><li>理解文本中的因果关系</li></ul></li><li>spatiotemporal relation 空间关系<ul><li>理解空间或者时间上的关系</li></ul></li><li>ellipsis 省略<ul><li>识别出文章中隐含或者忽略的信息，如参数、谓词、量词、时间等等</li></ul></li><li>bridging 间接引用<ul><li>能够根据词法或者句法的信息进行推理</li></ul></li><li>elaboration 阐述<ul><li>能够根据已有事实/常识进行推理</li></ul></li><li>meta-knowledge 元知识<ul><li>理解读者、作者或者文体信息（如：谁是这个故事的主人公）</li></ul></li><li>schematic clause relation 短语关系<ul><li>理解包含有并列、从句或者关系子句的复杂句子</li></ul></li><li>punctuation 标点符号<ul><li>理解文章中标点符号代表的意义</li></ul></li></ol><p>备注：</p><ul><li>认知能力与RC的一个联系是：当回答一个问题时，需要用到的认知能力越多，该问题越难回答</li><li>9和10的区别在于：9利用词项/句法信息还是10通用的常识信息</li><li>8到11是对Commonsense reasoning的细致分类</li><li>1-11是涉及到多句的，12-13涉及单句</li></ul><h3 id="2-readability"><a href="#2-readability" class="headerlink" title="2.readability"></a>2.readability</h3><p>语言能力: 如何将低层次的文本符号（字、词）组合为高层次的含义的能力<br>measures the text ease of processing and a wide range of linguistic features/human readability measurements are used.</p><ul><li>词义辨析：正确理解和区分词的意思。</li><li>句法识别：识别出文本的句法信息，使推理过程不受句法 变化的影响。</li><li>语义组合：根据句法信息将基本单元（字、词）的语义组 合成高等单元（句子，篇章）的语义。</li></ul><h2 id="Metrics-Evaluation"><a href="#Metrics-Evaluation" class="headerlink" title="Metrics/Evaluation"></a>Metrics/Evaluation</h2><p>如何评价一个example的难度？</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">https://aclweb.org/anthology/W16-6001</span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">http://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/14-Sugawara-14614.pdf</span><a href="#fnref:2" rev="footnote"> ↩</a></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">3.</span><span style="display: inline-block; vertical-align: top;">https://aclanthology.info/papers/P17-1075/p17-1075</span><a href="#fnref:3" rev="footnote"> ↩</a></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">4.</span><span style="display: inline-block; vertical-align: top;">王炳宁《A_Cognitive_Perspective_of_Machine_Comprehension_And_Recent_Advances》</span><a href="#fnref:4" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> MRC-Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> research </tag>
            
            <tag> metric </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器阅读理解数据集</title>
      <link href="/2019/01/12/mrc-dataset-info/"/>
      <url>/2019/01/12/mrc-dataset-info/</url>
      
        <content type="html"><![CDATA[<p>Contributed by Luxi Xing and Yuqiang Xie. IIE, CAS.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Based on the style of the ANSWER for each datasets, we split the datasets into several category and we will give details for each datasets.</p><blockquote><ol><li><a href="#Extractive">Extractive</a></li><li><a href="#Multi-Choice">Multi-Choice</a></li><li><a href="#Generative">Generative</a></li><li><a href="#Sequential">Sequential</a></li><li><a href="#Others">Others</a><br><del>Cloze-Style</del></li></ol></blockquote><h2 id="Extractive"><a href="#Extractive" class="headerlink" title="Extractive"></a>Extractive</h2><div class="table-container"><table><thead><tr><th>Dataset</th><th>Language</th><th>Domain</th><th>#Train<br>#Doc</th><th>#Dev</th><th>#Test</th><th>Year</th><th>Features</th></tr></thead><tbody><tr><td>SQuADv1</td><td>English</td><td>Wikipedia</td><td>87599<br>442</td><td>10570<br>48</td><td>9533<br>46</td><td>2016</td><td></td></tr><tr><td>SQuADv2</td><td>English</td><td>Wikipedia</td><td>130319<br>442</td><td>11873<br>35</td><td>8862<br>28</td><td>2018</td><td>have no answer</td></tr><tr><td>TriviaQA</td><td>English</td><td>Web<br>Wikipedia</td><td>528979<br>61888</td><td>68621<br>9951</td><td>65059<br>9509</td><td>2017</td><td>avg. length is 2895</td></tr><tr><td>HotPotQA</td><td>English</td><td>Wikipedia</td><td>90564</td><td>7405</td><td>7405</td><td>2017</td><td>multiple supporting doc. to answer</td></tr><tr><td>Natural Questions</td><td>English</td><td>Wikipedia</td><td>307k</td><td>8k</td><td>8k</td><td>2019</td><td>whole wikipedia article;<br>long answer</td></tr></tbody></table></div><h2 id="Multi-Choice"><a href="#Multi-Choice" class="headerlink" title="Multi-Choice"></a>Multi-Choice</h2><div class="table-container"><table><thead><tr><th>Dataset</th><th>Language</th><th>Domain</th><th>#Train<br>#Doc</th><th>#Dev</th><th>#Test</th><th>Year</th><th>Features</th></tr></thead><tbody><tr><td>RACE</td><td>English</td><td>Multi</td><td>87866<br>25137</td><td>4887<br>1389</td><td>4934<br>1407</td><td>2017</td><td>high<br>middle</td></tr><tr><td>MCScript</td><td>English</td><td>InScript</td><td>9731<br>1470</td><td>1411<br>219</td><td>2797<br>430</td><td>2018</td><td></td></tr><tr><td>ARC</td><td>English</td><td>Science</td><td></td><td></td><td>14M/7787</td><td>2018</td><td>hard</td></tr><tr><td>OpenBook</td><td>English</td><td></td><td>4957</td><td>500</td><td>500</td><td>2018</td><td>multi-hop;<br>commonsense;<br>science fact</td></tr><tr><td>MultiRC</td><td>English</td><td>7 domain</td><td>9872<br>871</td><td></td><td></td><td>2018</td><td>multi correct answers</td></tr><tr><td>QAngaroo<br>wikihop</td><td>English</td><td></td><td>43738</td><td>5129</td><td>2451</td><td>2017</td><td>multi evidence pieces;<br>multi options</td></tr><tr><td>DREAM</td><td>English</td><td></td><td></td><td></td><td></td><td>2019</td><td>dialogue-based multi-choice</td></tr></tbody></table></div><h2 id="Generative"><a href="#Generative" class="headerlink" title="Generative"></a>Generative</h2><p>Free-form answer generation</p><div class="table-container"><table><thead><tr><th>Dataset</th><th>Language</th><th>Domain</th><th>#Train</th><th>#Dev</th><th>#Test</th><th>Year</th><th>Features</th></tr></thead><tbody><tr><td>MSMARCO<br>v1</td><td>English</td><td>Web</td><td></td><td></td><td>100k</td><td>2016</td><td></td></tr><tr><td>MSMARCO<br>v2</td><td>English</td><td>Web</td><td></td><td></td><td>100k</td><td>2018</td><td></td></tr><tr><td>NarrativeQA</td><td>English</td><td>book/moive<br>(wikipedia)</td><td>32747</td><td>3461</td><td>10557</td><td>2018</td><td>based on summary</td></tr><tr><td>DuReader</td><td>Chinese</td><td>Web</td><td></td><td></td><td></td><td>2017</td></tr></tbody></table></div><ul><li>Metric for evaluate the Generative-style QA dataset is: ROUGE-L/BLEU</li></ul><h3 id="MS-MARCO"><a href="#MS-MARCO" class="headerlink" title="MS MARCO"></a>MS MARCO</h3><ul><li>More details about MSMARCO, please reference to <a href="https://github.com/IndexFziQ/MSMARCO-MRC-Analysis" target="_blank" rel="noopener">this REPO</a></li></ul><h3 id="Narrative-QA"><a href="#Narrative-QA" class="headerlink" title="Narrative QA"></a>Narrative QA</h3><ul><li>dataset consists of two settings:<ul><li>based on the summary： average 659 tokens</li><li>based on the full book/movie script: average 62528 tokens</li></ul></li></ul><h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h2><div class="table-container"><table><thead><tr><th>Dataset</th><th>Language</th><th>Domain</th><th>Document</th><th>Question</th><th>Size<br>(Train/Dev/Test)</th><th>Year</th><th>Features</th></tr></thead><tbody><tr><td>QuAC</td><td>English</td><td>Wikipedia</td><td></td><td></td><td></td><td>2018</td><td></td></tr><tr><td>CoQA</td><td>English</td><td>Wikipedia</td><td></td><td></td><td></td><td>2018</td><td></td></tr><tr><td>DREAM</td><td>English</td><td>Daily life</td><td>6444</td><td>10197</td><td></td><td>2019</td><td>dialogue-based multi-choice;<br>multi-turn multi-party</td></tr></tbody></table></div><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><h3 id="Chinese-Datasets"><a href="#Chinese-Datasets" class="headerlink" title="Chinese Datasets"></a>Chinese Datasets</h3><ul><li>People Daily &amp; Children’s Fairy Tale (PD&amp;CFT): <a href="https://github.com/ymcui/Chinese-RC-Dataset" target="_blank" rel="noopener">https://github.com/ymcui/Chinese-RC-Dataset</a><ul><li>cloze-style</li></ul></li><li>dureader: <a href="https://github.com/baidu/DuReader" target="_blank" rel="noopener">https://github.com/baidu/DuReader</a><ul><li>generative</li></ul></li><li>cmrc2018: <a href="https://github.com/ymcui/cmrc2018" target="_blank" rel="noopener">https://github.com/ymcui/cmrc2018</a><ul><li>extractive</li></ul></li></ul><h3 id="Multi-Documents-Datasets"><a href="#Multi-Documents-Datasets" class="headerlink" title="Multi-Documents Datasets"></a>Multi-Documents Datasets</h3><ul><li>TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.</li><li>SearchQA: A new Q&amp;A dataset augmented with context from a search engine.</li><li>HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.</li></ul><h3 id="Commonsense-Reasoning"><a href="#Commonsense-Reasoning" class="headerlink" title="Commonsense Reasoning"></a>Commonsense Reasoning</h3><ul><li>Winograd Schema Challenge. <a href="https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html" target="_blank" rel="noopener">https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html</a></li><li>COPA: Choice of Plausible Alternatives. <a href="https://www.cs.york.ac.uk/semeval-2012/task7/index.html" target="_blank" rel="noopener">https://www.cs.york.ac.uk/semeval-2012/task7/index.html</a></li><li>ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension</li><li>CommonsenseQA</li></ul><h3 id="FEVER-Fact-Extraction-and-VERification"><a href="#FEVER-Fact-Extraction-and-VERification" class="headerlink" title="FEVER: Fact Extraction and VERification"></a>FEVER: Fact Extraction and VERification</h3><ul><li>FEVER data: <a href="http://fever.ai/resources.html" target="_blank" rel="noopener">link</a></li><li>FEVER shared Task(with NAACL 2018) info: <a href="http://fever.ai/task.html" target="_blank" rel="noopener">details here</a></li><li>FEVER official baseline code: <a href="https://github.com/sheffieldnlp/fever-naacl-2018" target="_blank" rel="noopener">github repo</a></li><li>my note and analysis of FEVER: <a href="/2019/01/15/mrc-analysis-fever/" title="fever-note">fever-note</a></li></ul><h3 id="Google-Natural-Questions"><a href="#Google-Natural-Questions" class="headerlink" title="Google Natural Questions"></a>Google Natural Questions</h3><p>official details: <a href="https://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.html" target="_blank" rel="noopener">https://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.html</a><br>github: <a href="https://github.com/google-research-datasets/natural-questions" target="_blank" rel="noopener">https://github.com/google-research-datasets/natural-questions</a></p><p>自然问题数据集(NQ)是一个公开的自然发生问题(即由寻求信息的人提出的问题)</p><ul><li>用于训练和评估开放领域问答系统的新的、大规模语料库;</li><li>也是第一个复制人类查找问题答案的端到端流程的语料库;</li><li>专注于通过阅读整个页面来查找答案，而不是从一个短段落中提取答案;</li><li>来源于Wikipedia;</li><li>人工注释答案:<ul><li>要求注释者通过通读整个维基百科页面来找到答案，就好像这个问题是他们自己提出的一样。</li><li>注释者需要找到一个长答案和一个短答案，长答案涵盖推断问题所需的所有信息，短答案需要用一个或多个实体的名称简洁地回答问题</li></ul></li></ul><p><strong>自然语言理解挑战：</strong></p><ul><li>NQ的目的是使QA系统能够阅读和理解完整的维基百科文章，其中可能包含问题的答案，也可能不包含问题的答案。</li><li>系统首先需要确定这个问题的定义是否足够充分，是否可以回答<ul><li>许多问题本身基于错误的假设，或者过于模糊，无法简明扼要地回答。</li></ul></li><li>然后，系统需要确定维基百科页面中是否包含推断答案所需的所有信息。</li><li>作者认为，相比在知道长答案后在寻找短答案，长答案识别任务(找到推断答案所需的所有信息)需要更深层次的语言理解。</li></ul><p>human upper bound:</p><ul><li>long answer selection task: 87% F1</li><li>short answer selection task: 76% F1</li></ul><p>dataset statistics:</p><ul><li>train: 307k</li><li>dev: 8k</li><li>test: 8k</li></ul>]]></content>
      
      
      <categories>
          
          <category> MRC-Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> paper </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension</title>
      <link href="/2019/01/12/paper-emnlp2018-dcu/"/>
      <url>/2019/01/12/paper-emnlp2018-dcu/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension<br>EMNLP 2018<br>Yi Tay et al.<br>多粒度/尺度序列编码; 膨胀组合单元</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>Sequence encoder是MRC中的重要部件: helps to model compositionality of words, capturing rich and complex linguistic and syntactic structure in language</li><li>Sequence encoder的问题:<ul><li>文档较长，文档数多时计算开销大;</li><li>限制获取长距离上下文;</li><li>限制了多句和文档内部的推理;</li></ul></li><li>本文工作: 采取组合式编码方式<ul><li>主要思路是将多个尺度的信息组合在一起进行编码，利用多尺度 n 元语法信息来实现语义融合，得到更好的文档表达，用于后续的推理和Attention操作.</li><li>设计了一种 dilated compositions 机制来建模多个尺度之间的关系，相当于通过门控的方式决定要保留多少信息<ul><li>多尺度 包括：word-level、phrase-level、sentence-level、paragraph-level etc.</li><li>一种 divide-and-conquer 的序列编码方式</li></ul></li></ul></li><li>本文贡献:<ul><li>提出了一个 compositional encoder DCU (Dilated Compositional Units), DCU 既可以进行独立编码，也可以以RNN-style的方式进行更有表示能力的编码；</li><li>DCU 可以加速序列编码速度，并且保持相邻词之间的交互关系；</li><li>建模长句子时，模型可以获得前方更多的信息，是对所有上下文的全局概览；</li><li>相当于一个门控单元对不同粒度的关系进行建模，有利于捕获文档内部细粒度关系；</li></ul></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul><li>DCU用于MRC时的整体结构，左侧为DCU Encoder，右侧为DCU分别用于span prediction model和Multiple choice model的结构:<br><img src="/images/paper-emnlp2018-dcu/dcu-1-overall.png" alt="dcu-model"></li></ul><p>本节只介绍 DCU 的操作和 encoding 操作，不对整体的mrc模型进行介绍</p><h3 id="Dilated-Compositional-Mechanism"><a href="#Dilated-Compositional-Mechanism" class="headerlink" title="Dilated Compositional Mechanism"></a>Dilated Compositional Mechanism</h3><p>Notations:</p><ul><li>Input Sequence: $S=[w_1, w_2, …, w_l]$</li><li>Range list: $R={r_1, r_2, …, r_k}$<ul><li>$k$ 表示进行 $k$ 次 fold/unfold 操作</li></ul></li></ul><h4 id="1-Fold"><a href="#1-Fold" class="headerlink" title="1.Fold"></a>1.Fold</h4><p>对于每个 $r_j$:</p><ul><li>将$S$中的每 $r_j$ 个词进行串接(concat, neural bag-of-words 表示), 原输入长度缩减为 $l/r_j$</li><li>对于新得到的、包含 $l/r_j$ 个 tokens/blocks 的序列中的每个表示进行如下计算:<ul><li>$\bar{w}_t = \sigma_r(W_a(w_t)) + b_a$</li><li>$W_a \in \mathbb{R}^{d\times d}, b\in \mathbb{R}^d$</li></ul></li><li>Fold 的操作次数等于 range list 的大小</li><li>对于 range list 中不同的 $r$ 值, 参数 $W_a$ 和 $b_a$ 不共享</li></ul><h4 id="2-Unfold"><a href="#2-Unfold" class="headerlink" title="2.Unfold"></a>2.Unfold</h4><p>将transformed之后的序列展开为原长</p><ul><li>下图中为 $r_j=2$ 时的 Fold-Unfold 操作:<br><img src="/images/paper-emnlp2018-dcu/dcu-2-op.png" alt="dcu-fold-unfold"></li></ul><h4 id="3-Multi-Granular-Reasoning"><a href="#3-Multi-Granular-Reasoning" class="headerlink" title="3.Multi-Granular Reasoning"></a>3.Multi-Granular Reasoning</h4><p>多尺度推理</p><ul><li>将不同尺度的unfold之后的token表示进行串接，然后通过两层前馈神经网络得到一个门向量<ul><li>$g_t = F_2(F_1([w_1^t,w_2^t,…,w_t^k]))$</li><li>$F(\cdot) = ReLU(Wx+b)$</li></ul></li><li>$g_t$ 相当于一个从多尺度中学习的门控向量，尺度值最低的那些词会拥有相同的 $g_t$ 值</li></ul><h3 id="Encoding-Operation"><a href="#Encoding-Operation" class="headerlink" title="Encoding Operation"></a>Encoding Operation</h3><h4 id="1-Simple-Encoding"><a href="#1-Simple-Encoding" class="headerlink" title="1.Simple Encoding"></a>1.Simple Encoding</h4><ul><li>$z_t = tanh(W_p w_t) + b_p$</li><li>$y_t = \sigma(g_t) \ast w_t + (1-\sigma(g_t))z_t$</li></ul><h4 id="2-Recurrent-Encoding"><a href="#2-Recurrent-Encoding" class="headerlink" title="2.Recurrent Encoding"></a>2.Recurrent Encoding</h4><p>DCU 相当于循环神经网络中的 cell</p><ul><li>$c_t = g_t \odot c_{t-1} + (1-g_t)\odot z_t$</li><li>$o_t = W_o(w_t) + b_o$</li><li>$h_t = o_t \odot c_t$</li></ul><blockquote><p>问题：<br>此处有一点疑问是，为什么通过DCU得到的门控向量 $g_t$ 没有参与到后续的编码过程, 而只是作为了控制初始 $w_t$ 词向量的门控输入</p></blockquote><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><ul><li>Multi-Choice 模型:<ul><li>数据输入: include the standard EM (exact match) binary feature to each word. In this case, we use a three-way EM adaptation, i.e., EM(P, Q), EM(Q, A) and EM(P, A). The projected embeddings are then passed into a single layered highway network</li><li>输出(答案选择层): 将每个候选答案的答案向量转化为标量<ul><li>$a_j^f=softmax(W_2(\sigma_r(W_1([a_j])+b_1)+b_2))$</li></ul></li></ul></li><li>range valuse: ${1,2,4,10,25}$</li><li>最大序列长度 (RACE/SearchQA/NarrativeQA): $500/200/1100$</li></ul><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>只关注了一下RACE和NarrativeQA上的结果，在没有使用RNN-based编码器的情况下，取得了不错的效果，在论文完成时(2018.03)是RACE上的top-1</p><h4 id="RACE"><a href="#RACE" class="headerlink" title="RACE"></a>RACE</h4><p><img src="/images/paper-emnlp2018-dcu/dcu-results-race.png" alt="results-race"></p><h4 id="NarrativeQA"><a href="#NarrativeQA" class="headerlink" title="NarrativeQA"></a>NarrativeQA</h4><p><img src="/images/paper-emnlp2018-dcu/dcu-results-narrativeqa.png" alt="results-nqa"></p><h2 id="Summary-amp-Analysis"><a href="#Summary-amp-Analysis" class="headerlink" title="Summary &amp; Analysis"></a>Summary &amp; Analysis</h2><ul><li>对于长文本的编码问题是MRC中的重点问题之一【—&gt;表示问题】<ul><li>长文本和多篇文本</li></ul></li><li>本文提供了一种跨尺度的交互，或是融合跨尺度的信息的有效方式</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
            <tag> reasoning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WIP| On the Capabilities and Limitations of Reasoning for Natural Language Understanding</title>
      <link href="/2019/01/11/paper-1901-02522/"/>
      <url>/2019/01/11/paper-1901-02522/</url>
      
        <content type="html"><![CDATA[<p>[Under reading]</p><blockquote><p>On the Capabilities and Limitations of Reasoning for Natural Language Understanding<br>Khashabi et al.<br>作者来自 University of Pennsylvania, Indiana University 和 AllennAI<br>题目: 论自然语言理解推理的能力与局限</p></blockquote><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>一些NLU系统在克服查找 Style Reasoning 的语言可变性(linguistic variability)方面具有很强的实力, 但是他们的准确率会随着推理步数的增加而下降.<ul><li>key: style reasoning; linguistic variability; reasoning step;</li></ul></li><li>本文基于上述观察第一次提出了一个正式框架, 旨在解决<strong>使用语言表示隐藏概念空间时引入的</strong>:<ul><li><span id="inline-blue">1.模糊性(ambiguity) </span></li><li><span id="inline-blue">2.冗余性(redundancy) </span></li><li><span id="inline-blue">3.不完整性(incompleteness) </span></li><li><span id="inline-blue">4.不准确性(inaccuracy) </span></li></ul></li><li>模型使用了两个相互关联的(interrelated)空间:<ul><li><span id="inline-green">conceptual meaning space</span>: unambiguous and complete but hidden.</li><li><span id="inline-yellow">linguistic symbol space</span>: captures a noisy grounding of the meaning space in the symbols or words of a language.</li></ul></li><li>本文引用此框架来研究无向图中的连通性(connectivity)问题: 是构成更复杂的多步推理的基础核心推理问题<ul><li>证明了构建高质量算法来检测 latent meaning graph 中的连通性是可能的, 前提: 基于一个可观察的 noisy symbol graph, 并且这些噪声低于我们定量的噪声等级.</li><li>此外, 证明了一个不可能的结果: 如果一个query需要大量的推理步数, no reasoning system operating over the symbol graph is likely to recover any useful property of the meaning graph.</li><li>这一点同时强调了对于推理问题和系统，<strong>需要的是限制两个空间的距离</strong>，而不是投入更多的推理步数(hops)</li></ul></li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>Reasoning的定义: the process of combining facts(事实) and beliefs(信念), in order to make decisions.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> mrc </tag>
            
            <tag> nlu </tag>
            
            <tag> reasoning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Text Summarization - Main Problems</title>
      <link href="/2019/01/09/nlp-challenge-ts/"/>
      <url>/2019/01/09/nlp-challenge-ts/</url>
      
        <content type="html"><![CDATA[<p>文本摘要中的主要问题与挑战</p><h2 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h2><p>区分是挑战还是技术问题</p><h3 id="1-关注重点的词、句；"><a href="#1-关注重点的词、句；" class="headerlink" title="1.关注重点的词、句；"></a>1.关注重点的词、句；</h3><ul><li>Attention、pos、tf-idf</li><li>intra-temporal attention</li><li>encoder-less self-attention 【generating Wikipedia by Summarizing long Sequences】</li></ul><h3 id="2-不准确的复制-生成事实细节；"><a href="#2-不准确的复制-生成事实细节；" class="headerlink" title="2.不准确的复制\生成事实细节；"></a>2.不准确的复制\生成事实细节；</h3><ul><li>不准确的复制\生成事实细节；<ul><li>copy</li></ul></li></ul><h3 id="3-重复性短语、句子"><a href="#3-重复性短语、句子" class="headerlink" title="3.重复性短语、句子"></a>3.重复性短语、句子</h3><ul><li>重复性短语、句子；<ul><li>temporal Attention</li><li>coverage</li><li>intra-attention</li></ul></li></ul><h3 id="4-处理长文档"><a href="#4-处理长文档" class="headerlink" title="4.处理长文档"></a>4.处理长文档</h3><ul><li>处理长文档；<ul><li>Sentence-level Attention</li><li>selective gate</li><li>self-attention：缓解长距离依赖</li></ul></li></ul><h3 id="5-生成可读性好的摘要"><a href="#5-生成可读性好的摘要" class="headerlink" title="5.生成可读性好的摘要"></a>5.生成可读性好的摘要</h3><ul><li>生成可读性好的摘要；<ul><li>RL</li></ul></li></ul><h3 id="6-生成新的词（基于理解的基础上）；"><a href="#6-生成新的词（基于理解的基础上）；" class="headerlink" title="6.生成新的词（基于理解的基础上）；"></a>6.生成新的词（基于理解的基础上）；</h3><ul><li>生成新的词（基于理解的基础上）；</li></ul><h3 id="7-词汇问题"><a href="#7-词汇问题" class="headerlink" title="7.词汇问题"></a>7.词汇问题</h3><ul><li>罕见词（rare but important）、未登录词；<ul><li>add n-gram match term to loss【A neural Attention model for Sentence Summarization】</li><li>pointer  </li></ul></li><li>使用大规模词典；candidate sampling</li></ul><h2 id="关注要解决的问题："><a href="#关注要解决的问题：" class="headerlink" title="关注要解决的问题："></a>关注要解决的问题：</h2><ul><li>如何在生成过程中使decoder的注意力更集中，使Attention更聚焦，由于输入序列的长度 比较长？即便使用Attention模型，也不能很好的聚焦到对应的源端token（loss focus）；<ul><li>encoder的输出在用于Attention计算时包含噪声</li><li>使重点更突出；而不是过滤？区别？</li></ul></li><li>copy 机制的贡献程度？<ul><li>以及coverage  </li></ul></li><li>如何判定信息冗余与信息丢失</li></ul>]]></content>
      
      
      <categories>
          
          <category> Text Summarization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> research </tag>
            
            <tag> challenge </tag>
            
            <tag> ts </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Machine Translation - Main Problems</title>
      <link href="/2019/01/09/nlp-challenge-nmt/"/>
      <url>/2019/01/09/nlp-challenge-nmt/</url>
      
        <content type="html"><![CDATA[<p>神经机器翻译中的主要问题与挑战</p><h2 id="1-难以跨领域"><a href="#1-难以跨领域" class="headerlink" title="1.难以跨领域"></a>1.难以跨领域</h2><p>神经机器翻译（NMT）在处理领域之外的数据时的表现很糟：<br>当前的机器翻译系统会生成非常流畅的输出，这些输出与领域外数据的输入无关。因此像Google翻译这样的通用机器翻译系统在法律或金融等专业领域的表现尤其糟糕。与基于短语的系统等传统方法相比，NMT系统的效果更差。</p><h2 id="2-小数据集上表现不佳"><a href="#2-小数据集上表现不佳" class="headerlink" title="2.小数据集上表现不佳"></a>2.小数据集上表现不佳</h2><p>NMT在小数据集上表现不佳：一般而言，大多数机器学习都是这样，但这个问题在NMT上尤为突出。 NMT的优点在于，随着数据量的增加，它的表现要（比基于短语的机器翻译）更好，但在数据量很低的情况下，NMT的表现确实更差。事实上，正如作者所说，“在资源条件较差的情况下，NMT会产生与输入内容无关的流畅输出。”这可能是Motherboard的文章探讨的一些关于NMT表现奇怪的另一个原因。</p><h2 id="3-罕见词汇"><a href="#3-罕见词汇" class="headerlink" title="3.罕见词汇"></a>3.罕见词汇</h2><p>NMT在罕见词汇上的表现不佳：尽管比基于短语的翻译的表现更好，但NMT对于罕见或未见过的词语翻译的表现不佳。对于存在大量变形词的语言及大量命名实体的领域，这可能成为一个问题，因为变形词和命名实体一般非常罕见。</p><h2 id="4-长句翻译"><a href="#4-长句翻译" class="headerlink" title="4.长句翻译"></a>4.长句翻译</h2><p>长句的翻译问题：对长句编码及生成长句仍然是一个没有解决的问题。 机器翻译系统随句子长度的增加，其表现会越来越糟，NMT系统尤其如此。使用注意力有帮助，但问题远未“解决”。在许多领域，如法律领域，冗长复杂的句子是很常见的。</p><h2 id="5-注意力机制不等于简单对齐"><a href="#5-注意力机制不等于简单对齐" class="headerlink" title="5.注意力机制不等于简单对齐"></a>5.注意力机制不等于简单对齐</h2><p>注意力（Attention）机制不等于简单对齐：这是一个非常微妙但重要的问题。在传统的SMT系统（如基于短语的MT）中，对齐翻译为模型的检测提供了有用的调试信息。但是注意机制不能被视为传统意义上的对齐，即使论文经常将注意力机制作为“软对齐”引起注意。在NMT系统中，除了源语言中的动词之外，目标语言中的动词也可以作为主语和宾语成分。</p><h2 id="6-翻译质量"><a href="#6-翻译质量" class="headerlink" title="6.翻译质量"></a>6.翻译质量</h2><ul><li>难以控制翻译质量：每个单词都有多种翻译，典型的机器翻译系统在源句的翻译结构上表现很好。为了保持句子结构的大小合理，会使用集束搜索（beam search）。通过改变集束宽度，可以找到低概率但正确的平移。而对于NMT系统，调整集束的宽度似乎没有任何影响，甚至可能会有不良影响。</li><li>翻译的忠实度（adequacy）与流畅度（fluency）</li><li>信达雅</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>参考链接：<a href="http://deliprao.com/archives/301" target="_blank" rel="noopener">http://deliprao.com/archives/301</a></li><li>论文地址：<a href="http://www.aclweb.org/anthology/W/W17/W17-3204.pdf" target="_blank" rel="noopener">http://www.aclweb.org/anthology/W/W17/W17-3204.pdf</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NMT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> research </tag>
            
            <tag> challenge </tag>
            
            <tag> nmt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019年讨论组日程记录</title>
      <link href="/2019/01/09/schedule-2019/"/>
      <url>/2019/01/09/schedule-2019/</url>
      
        <content type="html"><![CDATA[<h1 id="2019年讨论组论文报告会"><a href="#2019年讨论组论文报告会" class="headerlink" title="2019年讨论组论文报告会"></a>2019年讨论组论文报告会</h1><a id="more"></a><style>    table th:nth-of-type(1){    width: 90px;    }    table th:nth-of-type(2){    width: 90px;    }    table th:nth-of-type(3){    width: 50%;    }    table th:nth-of-type(4){    width: 100px;    }</style><div class="table-container"><table><thead><tr><th style="text-align:left">Date</th><th style="text-align:left">Reporter</th><th style="text-align:left">Title/Papers</th><th style="text-align:left">Appx.</th></tr></thead><tbody><tr><td style="text-align:left">01.13</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Multi-Granular Sequence Encoding via Dilated Compositional Units for RC</td><td style="text-align:left"><a href="http://aclweb.org/anthology/D18-1238" target="_blank" rel="noopener">EMNLP2018</a><br><a href="/2019/01/12/paper-emnlp2018-dcu/" title="note link">note link</a></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">孙雅静</td><td style="text-align:left">The Design and Implementation of XiaoIce, an Empathetic Social Chatbot</td><td style="text-align:left"><a href="https://arxiv.org/abs/1812.08989" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Analysis on MS MARCOv1 Leaderboard Top 3:<br>1.S-Net<br>2.V-Net<br>3.MARS<br>Multi-task Learning</td><td style="text-align:left"><a href="https://github.com/IndexFziQ/MSMARCO-MRC-Analysis" target="_blank" rel="noopener">details-1</a><br><a href="https://github.com/IndexFziQ/Thinking-about-Multi-Task-Learning" target="_blank" rel="noopener">details-2</a></td></tr><tr><td style="text-align:left">01.18</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Self-Supervised Learning Introduction</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.18</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Meta-Learning Introduction</td><td style="text-align:left"><a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html" target="_blank" rel="noopener">blog</a></td></tr><tr><td style="text-align:left">01.18</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Meta-Learning a Dynamic Language Model</td><td style="text-align:left">ICLR workshop</td></tr><tr><td style="text-align:left">01.18</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Continual Leanring Introduction</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.24</td><td style="text-align:left">孙雅静</td><td style="text-align:left">1. A Dual-Attention Hierarchical RNN for Dialogue Act Classification<br>2.LifeLong Learning with Dynamically Expandable Networks</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.24</td><td style="text-align:left">谢玉强</td><td style="text-align:left">A Survey to Self-Supervised Learning</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.24</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Self-Supervised Learning<br>Unsupervised Learning by Cross-Channel Prediction</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.24</td><td style="text-align:left">彭伟</td><td style="text-align:left">Attention-over-Attention NN for RC</td><td style="text-align:left">ACL,2017</td></tr><tr><td style="text-align:left">02.20</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability</td><td style="text-align:left">ACL,2017</td></tr><tr><td style="text-align:left">02.20</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Multi-Style Generative Reading Comprehension</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">02.20</td><td style="text-align:left">彭伟</td><td style="text-align:left">Match-LSTM</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">02.20</td><td style="text-align:left">孙雅静</td><td style="text-align:left">个性化对话:<br>1.Persona-Chat对话数据集<br>2.Personalizing a Dialogue Systems with Transfer Reinforcement Learning</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">02.20</td><td style="text-align:left">于静</td><td style="text-align:left">Lifelong Learning Cross Media Search</td><td style="text-align:left">ACM-MM</td></tr><tr><td style="text-align:left">03.01</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">TG with Commonsense</td><td style="text-align:left"><a href="/2019/01/23/paper-tg-with-commonsense/" title="tg-with-cs-note">tg-with-cs-note</a></td></tr><tr><td style="text-align:left">03.01</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Incorporating Structured Commonsense Knowledge in Story Completion</td><td style="text-align:left">AAAI,2019</td></tr><tr><td style="text-align:left">03.01</td><td style="text-align:left">孙雅静</td><td style="text-align:left">What makes a good conversation? How controllable attributes affect human judgments</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.08</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Learning Personalized End-to-End Goal-Oriented Dialog</td><td style="text-align:left">AAAI,2019</td></tr><tr><td style="text-align:left">03.08</td><td style="text-align:left">于静</td><td style="text-align:left">1.Compositional Attention Networks for Machine Reasoning<br>2.Visual Dialog</td><td style="text-align:left">ICLR,2018<br>CVPR,2017</td></tr><tr><td style="text-align:left">03.08</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Commonsense for Generative Multi-Hop Question Answering Tasks</td><td style="text-align:left"><a href="http://xingluxi.github.io/2019/02/21/paper-emnlp2018-mhpgm/">details</a></td></tr><tr><td style="text-align:left">03.09</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">1.Cross-Lingual Language Model Pretraining<br>2.An Effective Approach to Unsupervised Machine Translation</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.09</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Self-supervised learning in video and multi-modal</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.09</td><td style="text-align:left">彭伟</td><td style="text-align:left">Dynamic Coattention Networks For Question Answering</td><td style="text-align:left">ICLR,2017</td></tr><tr><td style="text-align:left">03.15</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Improving Machine Reading Comprehension with General Reading Strategies</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.15</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Improving Question Answering with External Knowledge</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.15</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Learning to Select Knowledge for Response Generation in Dialog System</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.22</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering</td><td style="text-align:left">ACL, 2018</td></tr><tr><td style="text-align:left">03.22</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Tackling the Story Encding Biases in The Story Cloze Test</td><td style="text-align:left">ACL,2018</td></tr><tr><td style="text-align:left">03.22</td><td style="text-align:left">于静</td><td style="text-align:left">文本匹配模型介绍</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.29</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Multi-Choice Machine Reading Comprehension Task - PartI</td><td style="text-align:left"><a href="/2019/03/28/mrc-analysis-multichoice/" title="details">details</a></td></tr><tr><td style="text-align:left">03.29</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Sentence embedding Alignment for LifeLong Relation Extraction</td><td style="text-align:left">NAACL,2019</td></tr><tr><td style="text-align:left">04.12</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Does it care what you asked? Understanding Importance of Verbs in QA Model</td><td style="text-align:left">EMNLP,2018 workshop</td></tr><tr><td style="text-align:left">04.12</td><td style="text-align:left">孙雅静</td><td style="text-align:left">1.Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems<br>2.Global-to-Local Memory Pointer Networks for Task-Oriented Dialogue</td><td style="text-align:left">ACL,2018<br>ICLR,2019</td></tr><tr><td style="text-align:left">04.12</td><td style="text-align:left">李云鹏</td><td style="text-align:left">Relation Extraction Survey - 1</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.19</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Commonsense Reasoning for Natural Language Understanding</td><td style="text-align:left"><a href="/2019/04/18/mrc-cs-reasoning-for-nlu-survey/" title="detail">detail</a></td></tr><tr><td style="text-align:left">05.24</td><td style="text-align:left">彭伟</td><td style="text-align:left">1.A Deep Cascade Model for Multi-Document Reading Comprehension<br>2.Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.24</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">MASS: Masked Sequence to Sequence Pre-training for Language Generation</td><td style="text-align:left">ICML,2019</td></tr><tr><td style="text-align:left">05.31</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">HotpotQA at ACL 2019</td><td style="text-align:left"><a href="http://xingluxi.github.io/2019/05/30/mrc-paper-hotpotqa/">details</a></td></tr><tr><td style="text-align:left">05.31</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Challenges in Building Intelligent Open-domain Dialog Systems</td><td style="text-align:left">2019</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td></tr></tbody></table></div><h1 id="往期内容列表"><a href="#往期内容列表" class="headerlink" title="往期内容列表"></a>往期内容列表</h1><ul><li><a href="/2019/01/09/schedule-2017-md/" title="2017年讨论组内容列表">2017年讨论组内容列表</a></li><li><a href="/2019/01/09/schedule-2018-md/" title="2018年讨论组内容列表">2018年讨论组内容列表</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Group-Discussion </category>
          
      </categories>
      
      
        <tags>
            
            <tag> schedule </tag>
            
            <tag> group </tag>
            
            <tag> discussion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KIM</title>
      <link href="/2019/01/09/paper-kim/"/>
      <url>/2019/01/09/paper-kim/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Neural Natural Language Inference Models Enhanced with External Knowledge<br>ACL,2018.<br>Qian Chen, et al.<br>offical code link: <a href="https://github.com/lukecq1231/kim" target="_blank" rel="noopener">here</a>.</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>作者针对NLI任务提出了一个问题：</p><ul><li>是否可以从数据中学习NLI所需要的所有知识？</li><li>如果不能，如何使外部知识帮助神经网络模型？如何构建NLI模型利用外部知识？</li></ul><p>本文的工作是作者基于其ESIM模型之上完成的</p><h2 id="Model-Framework"><a href="#Model-Framework" class="headerlink" title="Model Framework"></a>Model Framework</h2><ul><li>模型整体结构分为四部分:<ul><li><img src="/images/paper-kim/kim.png" alt="kim"></li><li>1、representing</li><li>2、collecting local inference information</li><li>3、aggregating and composing local information</li><li>4、make global decision at sentence level</li><li>其中，将外部知识加入到了: co-attention、local inference collection、inference composition 模块中</li></ul></li></ul><h3 id="External-Knowledge"><a href="#External-Knowledge" class="headerlink" title="External Knowledge"></a>External Knowledge</h3><p>首先确定需要哪些知识来帮助NLI任务</p><ul><li>external, inference-related knowledge</li><li>intuitively Knowledge: <strong>synonymy(同义)</strong>、<strong>antonymy(反义)</strong>、<strong>hypernymy(上位)</strong>、<strong>hyponymy(下位)</strong><ul><li>上下位的关系可以帮助捕捉 entailment 信息</li><li>反义和co-hyponymy(共享相同上位词)的关系可以更好的建模 contradiction 关系</li></ul></li><li>这篇文章主要是引入了基础的词汇级的语义知识<ul><li>建模词$w_i$和$w_j$知识为 $r_{ij}$</li><li>重点是：如何构建 $r_ij$</li></ul></li></ul><h3 id="M1-Encoding-Premise-and-Hypothesis"><a href="#M1-Encoding-Premise-and-Hypothesis" class="headerlink" title="M1.Encoding Premise and Hypothesis"></a>M1.Encoding Premise and Hypothesis</h3><ul><li>premise: $a=(a_1,…,a_m)$</li><li>hypothesis: $b=(b_1,…,b_n)$</li><li>通过BiLSTM进行编码，得到context-dependent 隐藏状态：<ul><li>$a_i^s = Encoder(E(a),i)$</li><li>$b_j^s = Encoder(E(b),j)$</li></ul></li></ul><h3 id="M2-Knowledge-Enriched-Co-Attention"><a href="#M2-Knowledge-Enriched-Co-Attention" class="headerlink" title="M2.Knowledge-Enriched Co-Attention"></a>M2.Knowledge-Enriched Co-Attention</h3><ul><li>知识关系特征：$r_{ij} \in \mathbb{R}^{d_r}$</li><li>Co-attention:<ul><li>$e_{ij} = (a^s_i)^T b_j^s + F(r_{ij})$</li><li>$F(r_{ij}) = \lambda \mathbb{1}(r_{ij})$<ul><li>$\mathbb{1}(r_{ij})$ 判断 $r_{ij}$ 是否为0向量</li></ul></li><li>得到的 co-attention 矩阵 $e \in \mathbb{R}^{m\times n}$</li></ul></li><li>根据co-attention对premise和hypothesis的表示进行更新：<ul><li><script type="math/tex; mode=display">a_i^c=\sum_{j=1}^n \alpha _{ij} b_j^s</script><ul><li><script type="math/tex; mode=display">\alpha_{ij} = exp(e_{ij}) / \sum_{k=1}^n exp(e_{ik})</script></li></ul></li><li><script type="math/tex; mode=display">b_j^c=\sum_{i=1}^m \beta _{ij} a_i^s</script><ul><li><script type="math/tex; mode=display">\beta_{ij} = exp(e_{ij}) / \sum_{k=1}^m exp(e_{kj})</script></li></ul></li><li>$\alpha \in \mathbb{R}^{m\times n}$、$\beta \in \mathbb{R}^{m\times n}$</li></ul></li></ul><h3 id="M3-Local-inference-Collection-with-External-Knowledge"><a href="#M3-Local-inference-Collection-with-External-Knowledge" class="headerlink" title="M3.Local inference Collection with External Knowledge"></a>M3.Local inference Collection with External Knowledge</h3><ul><li>通过比较$a_i^s$、$a_i^c$ 和 他们的关系（从外部知识获得），可以获得词级的推理信息</li><li>Knowledge-enriched local inference:<ul><li><script type="math/tex; mode=display">a_i^m = G([a_i^s; a_i^c; a_i^s - a_i^c; a_i^s \circ a_i^c; \sum_{j=1}^n \alpha_{ij}r_{ij}])</script></li><li><script type="math/tex; mode=display">b_j^m = G([b_j^s; b_j^c; b_j^s - b_j^c; b_j^s \circ b_j^c; \sum_{i=1}^m \beta_{ij}r_{ji}])</script></li><li>最后一项的目的是：收集对齐词之间的关系特征，是从外部知识获得的word-level inference information</li><li>$G$ 是非线性映射，用于降维，relu + shortcut connection<ul><li><script type="math/tex; mode=display">a_i^m + \sum_{j=1}^n \alpha_{ij}r_{ij}</script></li><li><script type="math/tex; mode=display">b_j^m + \sum_{i=1}^m \beta_{ij}r_{ij}</script></li></ul></li></ul></li></ul><h3 id="M4-Knowledge-Enhanced-Inference-Composition"><a href="#M4-Knowledge-Enhanced-Inference-Composition" class="headerlink" title="M4.Knowledge-Enhanced Inference Composition"></a>M4.Knowledge-Enhanced Inference Composition</h3><ul><li>决定总体的推理关系：BiLSTM —&gt; mean;max;weighted Pooling，得到定长向量<ul><li>Composition = BiLSTM<ul><li>$a_i^v = Composition(a^m,i)$</li><li>$b_j^v = Composition(b^m,j)$</li></ul></li><li>Weighted Pooling<ul><li><script type="math/tex; mode=display">a^w = \sum_{i=1}^m \left( \frac{exp(H(\sum_{j=1}^n \alpha_{ij}r_{ij}))}{\sum_{i=1}^m exp(H(\sum_{j=1}^n\alpha_{ij}r_{ij}))} \right) a_i^v</script></li><li><script type="math/tex; mode=display">b^w = \sum_{j=1}^n \left( \frac{exp(H(\sum_{i=1}^m \beta_{ij}r_{ji}))}{\sum_{j=1}^n exp(H(\sum_{i=1}^m\beta_{ij}r_{ji}))} \right) b_j^v</script></li><li>$H$ 函数是 1层的前馈神经网络，激活函数是relu</li></ul></li></ul></li><li>最后得到定长向量之后，再过一层MLP，激活函数为tanh，和一层 softmax，得到分类结果</li></ul><h2 id="Experiment-Settings"><a href="#Experiment-Settings" class="headerlink" title="Experiment Settings"></a>Experiment Settings</h2><h3 id="Representation-of-External-Knowledge"><a href="#Representation-of-External-Knowledge" class="headerlink" title="Representation of External Knowledge"></a>Representation of External Knowledge</h3><h4 id="Lexical-Semantic-Relations"><a href="#Lexical-Semantic-Relations" class="headerlink" title="Lexical Semantic Relations"></a>Lexical Semantic Relations</h4><ul><li>relations of lexical pairs: 检索范围是wordnet<ul><li>synonymy: 如果词对是同义词，使值为1，否则为0</li><li>antonymy: 如果词对是反义词，使值为1，否则为0</li><li>hypernymy: 如果一个词是另一个词的上位词，取值 $1-\frac{n}{8}$，否则为0<ul><li>其中 $n$ 是两个词在层次上的边数</li><li>忽略边数超过8的</li></ul></li><li>hyponymy: inverse of the hypernymy feature</li><li>co-hyponymys: 如果两个词有相同的上位词且不是同义词，取值为1，否则为0</li><li>向量 $r$ 的维度为 $d_r = 5$</li><li>在 wordnet 中还有额外的一些（15种）关系，但是对结果的提升没有贡献</li></ul></li><li>relation embeddings<ul><li>pretrain based on wordnet</li><li>TransE</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> note </tag>
            
            <tag> nli </tag>
            
            <tag> wordnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Knowledgeable Reader Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge</title>
      <link href="/2019/01/09/paper-knreader/"/>
      <url>/2019/01/09/paper-knreader/</url>
      
        <content type="html"><![CDATA[<p>Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge</p><ul><li>ACL,2018. Todor Mihaylov, et al.</li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><h2 id="Model-Framework"><a href="#Model-Framework" class="headerlink" title="Model Framework"></a>Model Framework</h2><p><img src="/images/paper-knreader/knreader.png" alt="knreader"></p>]]></content>
      
      
      <categories>
          
          <category> Paper-Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> mrc </tag>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Knowledge-based NLU-Papers</title>
      <link href="/2019/01/09/kmrc-paper-info/"/>
      <url>/2019/01/09/kmrc-paper-info/</url>
      
        <content type="html"><![CDATA[<p>A list of recent papers on knowledge-based Natural Language Understand.</p><blockquote><p><em>Contributed by Luxi Xing and Yuqiang Xie, National Engineering Laboratory for Information Security Technologies, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China.</em> </p></blockquote><hr><style>    table th:nth-of-type(2){    width: 60%;    }</style><h2 id="NLI-with-Knowledge"><a href="#NLI-with-Knowledge" class="headerlink" title="NLI with Knowledge"></a>NLI with Knowledge</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Conf.</th><th style="text-align:left">Title</th><th style="text-align:left">Authors/Org.</th><th style="text-align:center">Note</th></tr></thead><tbody><tr><td style="text-align:center">ACL<br>2018</td><td style="text-align:left">Neural Natural Language Inference Models Enhanced with External Knowledge</td><td style="text-align:left">Qian Chen</td><td style="text-align:center"><a href="/2019/01/09/paper-kim/" title="KIM-note">KIM-note</a></td></tr><tr><td style="text-align:center">2018</td><td style="text-align:left">Improving Natural Language Inference Using External Knowledge in the Science Questions Domain</td><td style="text-align:left">Xiaoyan Wang</td></tr></tbody></table></div><h2 id="MRC-with-Knowledge"><a href="#MRC-with-Knowledge" class="headerlink" title="MRC with Knowledge"></a>MRC with Knowledge</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Conf.</th><th style="text-align:left">Title</th><th style="text-align:left">Authors/Org.</th><th style="text-align:center">Note</th></tr></thead><tbody><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="https://doi.org/10.18653/v1/P17-1132" target="_blank" rel="noopener">Leveraging knowledge bases in lstms for improving machine reading</a></td><td style="text-align:left">Yang, et al.<br>CMU</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/D17-1086" target="_blank" rel="noopener">World knowledge for reading comprehension: Rare entity prediction with hierarchical lstms using external descriptions</a></td><td style="text-align:left">Long, et al.<br>McGill University</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2018</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P18-1076" target="_blank" rel="noopener">Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge</a></td><td style="text-align:left">Mihaylov, et al.<br>Heidelberg University</td><td style="text-align:center"><a href="/2019/01/09/paper-knreader/" title="knreader-note">knreader-note</a></td></tr><tr><td style="text-align:center">EMNLP<br>2018</td><td style="text-align:left"><a href="https://www.aclweb.org/anthology/D18-1454" target="_blank" rel="noopener">Commonsense for Generative Multi-Hop Question Answering Tasks</a></td><td style="text-align:left">Lisa Bauer</td><td style="text-align:center"><a href="/2019/02/21/paper-emnlp2018-mhpgm/" title="mhpgm-note">mhpgm-note</a></td></tr></tbody></table></div><h2 id="Dialog-with-Knowledge"><a href="#Dialog-with-Knowledge" class="headerlink" title="Dialog with Knowledge"></a>Dialog with Knowledge</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Conf.</th><th style="text-align:left">Title</th><th style="text-align:left">Authors/Org.</th><th style="text-align:center">Note</th></tr></thead><tbody><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P17-1162" target="_blank" rel="noopener">Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings</a></td><td style="text-align:left">He, et al.<br>Stanford</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">IJCAI18</td><td style="text-align:left"><a href="https://www.ijcai.org/proceedings/2018/0643.pdf" target="_blank" rel="noopener">Commonsense Knowledge Aware Conversation Generation with Graph Attention</a></td><td style="text-align:left">Zhou, et al.<br>THU</td><td style="text-align:center"><a href="/2019/01/23/paper-tg-with-commonsense/" title="note">note</a></td></tr><tr><td style="text-align:center">AAAI<br>2018</td><td style="text-align:left"><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/02/A_Knowledge_Grounded_Neural_Conversation_Model.pdf" target="_blank" rel="noopener">A Knowledge-Grounded Neural Conversation Model</a></td><td style="text-align:left">Ghazvininejad, et al.<br>Microsoft Research</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AAAI<br>2018</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1709.04264.pdf" target="_blank" rel="noopener">Flexible End-to-End Dialogue System for Knowledge Grounded Conversation</a></td><td style="text-align:left">Zhu, et al.<br>HKUST</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1902.04911" target="_blank" rel="noopener">Learning to Select Knowledge for Response Generation in Dialog Systems</a></td><td style="text-align:left">Baidu</td></tr></tbody></table></div><h2 id="Text-Generation-with-Knowledge"><a href="#Text-Generation-with-Knowledge" class="headerlink" title="Text Generation with Knowledge"></a>Text Generation with Knowledge</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Conf.</th><th style="text-align:left">Title</th><th style="text-align:left">Authors/Org.</th><th style="text-align:center">Note</th></tr></thead><tbody><tr><td style="text-align:center">AAAI<br>2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1808.10113" target="_blank" rel="noopener">Story Ending Generation with Incremental Encoding and Commonsense Knowledge</a></td><td style="text-align:left">Jian Guan, et al.<br>THU</td><td style="text-align:center"><a href="/2019/01/23/paper-tg-with-commonsense/" title="note">note</a></td></tr></tbody></table></div><h2 id="Representation-with-Knowledge"><a href="#Representation-with-Knowledge" class="headerlink" title="Representation with Knowledge"></a>Representation with Knowledge</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Conf.</th><th style="text-align:left">Title</th><th style="text-align:left">Authors/Org.</th><th style="text-align:center">Note</th></tr></thead><tbody><tr><td style="text-align:center">ICLR<br>2017</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1608.00318v1.pdf" target="_blank" rel="noopener">A Neural Knowledge Language Model</a></td><td style="text-align:left">Ahn.et al.<br>Université de Montréal</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2017</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P17-1187" target="_blank" rel="noopener">Improved Word Representation Learning with Sememes</a></td><td style="text-align:left">Niu, et al.<br>THU</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2018</td><td style="text-align:left"><a href="http://aclweb.org/anthology/D18-1033" target="_blank" rel="noopener">Cross-lingual Lexical Sememe Prediction</a></td><td style="text-align:left">Qi, et al.<br>THU</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AAAI<br>2018</td><td style="text-align:left"><a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16321/16167" target="_blank" rel="noopener">Improving Neural Fine-Grained Entity Typing with Knowledge Attention</a></td><td style="text-align:left">Xin, et al.<br>THU</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ACL<br>2019</td><td style="text-align:left"><a href="https://www.aclweb.org/anthology/P19-1571" target="_blank" rel="noopener">Modeling Semantic Compositionality with Sememe Knowledge</a></td><td style="text-align:left">Qi, et al.<br>THU</td><td style="text-align:center"><a href="/2019/08/27/paper-acl2019-sc-with-sememe/" title="note">note</a></td></tr><tr><td style="text-align:center">2019</td><td style="text-align:left"><a href="https://arxiv.org/abs/1908.05646" target="_blank" rel="noopener">SenseBERT: Driving Some Sense into BERT</a></td><td style="text-align:left">AI21 Labs</td><td style="text-align:center"><a href="/2019/08/18/paper-2019-sense-bert/" title="note">note</a></td></tr><tr><td style="text-align:center"></td><td style="text-align:left"></td><td style="text-align:left"></td></tr></tbody></table></div><h4 id="Level"><a href="#Level" class="headerlink" title="Level:"></a>Level:</h4><ol><li>KMRC area.</li><li>Relevant research area.</li><li>Heuristic.</li><li>Review.</li></ol><h4 id="Field"><a href="#Field" class="headerlink" title="Field:"></a>Field:</h4><ul><li><strong>KMRC:</strong> <strong>K</strong>nowledge-based <strong>M</strong>achine <strong>R</strong>eading <strong>C</strong>omprehension;</li><li><strong>KDS:</strong> <strong>K</strong>nowledge-based <strong>D</strong>ialogue <strong>S</strong>ystem;</li><li><strong>KIR:</strong> <strong>K</strong>nowledge-based <strong>I</strong>nformation <strong>R</strong>etrieval;</li><li><strong>SC:</strong> <strong>S</strong>ememe <strong>C</strong>omputation;</li><li><strong>NKLM:</strong> <strong>N</strong>eural <strong>K</strong>nowledge <strong>L</strong>anguage <strong>M</strong>odel.</li></ul>]]></content>
      
      
      <categories>
          
          <category> MRC-Paper-Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> knowledge </tag>
            
            <tag> paper </tag>
            
            <tag> text-generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2018年讨论组日程表</title>
      <link href="/2019/01/09/schedule-2018-md/"/>
      <url>/2019/01/09/schedule-2018-md/</url>
      
        <content type="html"><![CDATA[<h1 id="2018年讨论组论文报告会"><a href="#2018年讨论组论文报告会" class="headerlink" title="2018年讨论组论文报告会"></a>2018年讨论组论文报告会</h1><a id="more"></a><style>    /* 第一列表格宽度 */    table th:nth-of-type(1){    width: 90px;    }    /* 第二列表格宽度 */    table th:nth-of-type(2){    width: 90px;    }    /* 第三列表格宽度 */    table th:nth-of-type(3){    width: 50%;    }    /* 第四列表格宽度 */    table th:nth-of-type(4){    width: 100px;    }</style><div class="table-container"><table><thead><tr><th style="text-align:left">Date</th><th style="text-align:left">Reporter</th><th style="text-align:left">Title/Papers</th><th style="text-align:left">Confere.</th></tr></thead><tbody><tr><td style="text-align:left">01.13</td><td style="text-align:left">邢璐茜</td><td style="text-align:left"><a href="https://papers.nips.cc/paper/6775-deliberation-networks-sequence-generation-beyond-one-pass-decoding.pdf" target="_blank" rel="noopener">Deliberation Networks: Sequence Generation Beyond One-Pass Decoding</a></td><td style="text-align:left">NIPS,2017</td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">谢玉强</td><td style="text-align:left">词法分析调研</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">孙雅静</td><td style="text-align:left">句法分析调研</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">彭  伟</td><td style="text-align:left">The Colorful World in the Neural Network</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.17</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">SIX Challenges In the Text Summarization</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.17</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">深度学习与文本分类</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.17</td><td style="text-align:left">孙雅静</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention is all you need</a></td><td style="text-align:left">NIPS,2017</td></tr><tr><td style="text-align:left">03.17</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Neural Word Segmentation Learning for Chinese</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">03.17</td><td style="text-align:left">彭  伟</td><td style="text-align:left">Neural Network + CRF</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">NLG survey(partly)</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">NLG survey(partly)</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">开题报告</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Neural Word Segmentation Learning for Chinese</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">孙雅静</td><td style="text-align:left">1. <a href="https://arxiv.org/pdf/1711.02281.pdf" target="_blank" rel="noopener">Non autoregressive neural machine translation</a> <br>2. <a href="https://arxiv.org/pdf/1802.06901.pdf" target="_blank" rel="noopener">Deterministic Non autoregressive neural sequence modeling by iterative refinement</a></td><td style="text-align:left">ICLR,2018 <br> arXiv,2018</td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">彭  伟</td><td style="text-align:left">毕设报告 + LSTM-CRF</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.12</td><td style="text-align:left">邢璐茜</td><td style="text-align:left"><a href="https://aclanthology.info/pdf/D/D17/D17-1122.pdf" target="_blank" rel="noopener">Inter-Weighted Alignment Network for Sentence Pair Modeling</a></td><td style="text-align:left">EMNLP,2017</td></tr><tr><td style="text-align:left">05.12</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">知识库问答系统-综述</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.12</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Low Resource NMT 开题预讲</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.12</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Recursive Neural Network Architecture</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.12</td><td style="text-align:left">谢玉强</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/P16-1122" target="_blank" rel="noopener">Inner Attention based Recurrent Neural Networks for Answer Selection</a></td><td style="text-align:left">ACL,2016</td></tr><tr><td style="text-align:left">06.09</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC 综述(Part I)</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">06.09</td><td style="text-align:left">魏相鹏</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/N18-1202" target="_blank" rel="noopener">Deep contextualized word representations</a></td><td style="text-align:left">NAACL,2018<br>best paper</td></tr><tr><td style="text-align:left">06.09</td><td style="text-align:left">雷扬帆</td><td style="text-align:left"><a href="http://aclweb.org/anthology/N18-1038" target="_blank" rel="noopener">Learning Visually Grounded Sentence Representations</a></td><td style="text-align:left">NAACL-HLT,2018</td></tr><tr><td style="text-align:left">06.09</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Generative Adversarial Network</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">06.09</td><td style="text-align:left">谢玉强</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1801.00102" target="_blank" rel="noopener">Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference</a></td><td style="text-align:left">EMNLP,2018</td></tr><tr><td style="text-align:left">06.30</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC 综述(Part II)</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">06.30</td><td style="text-align:left">魏相鹏</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1805.09461" target="_blank" rel="noopener">Deep Reinforcement Learning for Sequence to Sequence Models</a></td><td style="text-align:left">arXiv,2018</td></tr><tr><td style="text-align:left">06.30</td><td style="text-align:left">雷扬帆</td><td style="text-align:left"><a href="https://www.ijcai.org/proceedings/2017/579" target="_blank" rel="noopener">Bilateral Multi-Perspective Matching for Natural Language Sentences</a></td><td style="text-align:left">IJCAI,2017</td></tr><tr><td style="text-align:left">06.30</td><td style="text-align:left">孙雅静</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P18-1087" target="_blank" rel="noopener">Transformation networks for target-oriented sentiment classification</a></td><td style="text-align:left">ACL,2018</td></tr><tr><td style="text-align:left">06.30</td><td style="text-align:left">谢玉强</td><td style="text-align:left"><a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" target="_blank" rel="noopener">Factorization Machines</a></td><td style="text-align:left">ICDM,2010</td></tr><tr><td style="text-align:left">08.03</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">1. <a href="http://anthology.aclweb.org/attachments/P/P18/P18-2047.Notes.pdf" target="_blank" rel="noopener">A Simple and Effective Approach to Coverage-Aware NMT</a><br>2. <a href="http://www.aclweb.org/anthology/P18-2053" target="_blank" rel="noopener">Bag-of-Words as Traget for NMT</a></td><td style="text-align:left">ACL,2018<br>short paper</td></tr><tr><td style="text-align:left">08.03</td><td style="text-align:left">孙雅静</td><td style="text-align:left">对话系统综述</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">08.03</td><td style="text-align:left">谢玉强</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1801.03603" target="_blank" rel="noopener">Syntax-aware Entity Embedding for Neural Relation Extraction</a></td><td style="text-align:left">AAAI,2018</td></tr><tr><td style="text-align:left">08.03</td><td style="text-align:left">彭  伟</td><td style="text-align:left">语言表示</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">08.12</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC 综述(Part III)</td><td style="text-align:left">R-Net<br>FastQA</td></tr><tr><td style="text-align:left">08.12</td><td style="text-align:left">雷扬帆</td><td style="text-align:left"><a href="https://www.ijcai.org/proceedings/2018/0613.pdf" target="_blank" rel="noopener">Multiway Attention Networks for Modeling Sentence Pairs</a></td><td style="text-align:left">IJCAI,2018</td></tr><tr><td style="text-align:left">08.12</td><td style="text-align:left">孙雅静</td><td style="text-align:left">对话系统综述-safe response</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">08.12</td><td style="text-align:left">谢玉强</td><td style="text-align:left">MRC 模型介绍</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">08.12</td><td style="text-align:left">彭伟</td><td style="text-align:left"><a href="http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors" target="_blank" rel="noopener">Learned in Translation: Contextualized Word Vectors</a></td><td style="text-align:left">NIPS,2017</td></tr><tr><td style="text-align:left">08.20</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC: J-Net Exploring Question understanding &amp; Adaptation in NN-based QA</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">08.20</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">VAE与NMT<br>1. <a href="https://arxiv.org/abs/1605.07869" target="_blank" rel="noopener">Variational Neural Machine Translation</a></td><td style="text-align:left">EMNLP,2016</td></tr><tr><td style="text-align:left">08.20</td><td style="text-align:left">孙雅静</td><td style="text-align:left">对话系统: Safe response<br>1. <a href="http://aclweb.org/anthology/D17-1065" target="_blank" rel="noopener">Neural Response Generation via GAN with an Approxiamte Embedding Layer</a><br>2. <a href="http://www.aclweb.org/anthology/P18-1139" target="_blank" rel="noopener">Generating Informative Responses with Controlled Sentence Function</a></td><td style="text-align:left">ACL,2018</td></tr><tr><td style="text-align:left">09.04</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC: Adversarial Evaluation &amp; Unanswerable Question for SQuAD</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.04</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">鲁棒性神经机器翻译</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.04</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">文本匹配</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.04</td><td style="text-align:left">孙雅静</td><td style="text-align:left">对话: 对话一致性 <br>1. <a href="https://arxiv.org/pdf/1603.06155.pdf" target="_blank" rel="noopener">A Persona-Based Neural Conversation Model</a><br>2. <a href="http://aclweb.org/anthology/P17-1162" target="_blank" rel="noopener">Learning Symmetric collaborateve Dialogue with Dynamic Knowledge Graph Embedding</a><br>3.<a href="https://arxiv.org/pdf/1808.07042.pdf" target="_blank" rel="noopener">CoQA: A Conversational Question Answering Challenge</a></td><td style="text-align:left">arXiv,2016<br> ACL,2017 <br>arXiv,2018</td></tr><tr><td style="text-align:left">09.14</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC: Co-Match for Multi-Choice Reading Comprehension</td><td style="text-align:left">ACL,2018</td></tr><tr><td style="text-align:left">09.14</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">文档级神经机器翻译<br>1. <a href="https://arxiv.org/abs/1809.01576" target="_blank" rel="noopener">Document-Level Neural Machine Translation with Hierachical Attention Networks</a></td><td style="text-align:left">EMNLP,2018</td></tr><tr><td style="text-align:left">09.14</td><td style="text-align:left">谢玉强</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1608.07905.pdf" target="_blank" rel="noopener">Match-LSTM</a></td><td style="text-align:left">ICLR,2017</td></tr><tr><td style="text-align:left">09.14</td><td style="text-align:left">孙雅静</td><td style="text-align:left">文本改写<br>1. <a href="http://aclweb.org/anthology/Q18-1031" target="_blank" rel="noopener">Generating Sentences by Editing Prototypes</a></td><td style="text-align:left">TACL,2018</td></tr><tr><td style="text-align:left">09.22</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">CopyNet<br>1. <a href="https://papers.nips.cc/paper/5866-pointer-networks.pdf" target="_blank" rel="noopener">Pointer Network</a><br>2. <a href="http://aclweb.org/anthology/P16-1154" target="_blank" rel="noopener">Incorporating Copying mechanism in Sequence-to-Sequence</a><br>3. <a href="https://arxiv.org/abs/1704.04368" target="_blank" rel="noopener">Get to The Point: Summarization with Pointer-Generator Networks</a></td><td style="text-align:left">NIPS,2015 <br>ACL,2016<br>arXiv,2017</td></tr><tr><td style="text-align:left">09.22</td><td style="text-align:left">魏相鹏</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P18-2048" target="_blank" rel="noopener">Dynamic Sentence Sampling for Efficient Training of NMT</a></td><td style="text-align:left">ACL,2018</td></tr><tr><td style="text-align:left">09.22</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Modeling Sentence Tutorial</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.22</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Two Methods for Training Deeper Networks<br>1. <a href="https://arxiv.org/pdf/1505.00387.pdf" target="_blank" rel="noopener">Highway Network</a><br>2. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Residual Network</a></td><td style="text-align:left">ICML,2015<br>CVPR,2016</td></tr><tr><td style="text-align:left">09.22</td><td style="text-align:left">孙雅静</td><td style="text-align:left">GAN回顾</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.22</td><td style="text-align:left">彭伟</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></td><td style="text-align:left">ICLR,2015</td></tr><tr><td style="text-align:left">10.20</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">The Pre-Training Language Model<br>1. <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></td><td style="text-align:left">arXiv,2018</td></tr><tr><td style="text-align:left">10.20</td><td style="text-align:left">魏相鹏</td><td style="text-align:left"><a href="https://openreview.net/pdf?id=ryza73R9tQ" target="_blank" rel="noopener">Machine Translation with Weakly Paired Bilingual Documents</a></td><td style="text-align:left">ICLR,2019.<br>OpenReview</td></tr><tr><td style="text-align:left">10.20</td><td style="text-align:left">谢玉强</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1805.11360.pdf" target="_blank" rel="noopener">Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information</a></td><td style="text-align:left">AAAI,2019</td></tr><tr><td style="text-align:left">10.20</td><td style="text-align:left">孙雅静</td><td style="text-align:left"><a href="https://aclanthology.info/papers/P17-1171/p17-1171" target="_blank" rel="noopener">Reading Wikipedia to Answer Open-domain Questions</a></td><td style="text-align:left">ACL,2017</td></tr><tr><td style="text-align:left">10.20</td><td style="text-align:left">彭  伟</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention is all you need</a></td><td style="text-align:left">NIPS,2017</td></tr><tr><td style="text-align:left">11.17</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC: Multi-Step Reasoning &amp; Open-Domain QA <br>1. <a href="http://aclweb.org/anthology/P18-1157" target="_blank" rel="noopener">Stochastic Answer Networks for Machine Reading Comprehension</a><br>2. <a href="http://aclweb.org/anthology/P18-1161" target="_blank" rel="noopener">Denoising Distantly Supervised Open-Domain Question Answering</a></td><td style="text-align:left">ACL,2018<br>ACL,2018</td></tr><tr><td style="text-align:left">11.17</td><td style="text-align:left">孙雅静</td><td style="text-align:left">1. <a href="http://www.aclweb.org/anthology/P17-1046" target="_blank" rel="noopener">Sequential Matching Network：A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots</a><br>2. <a href="https://openreview.net/forum?id=ByftGnR9KX" target="_blank" rel="noopener">FlowQA: Grasping Flow in History for Conversational Machine Comprehension</a></td><td style="text-align:left">ACL,2017<br>ICLR,2019.OR</td></tr><tr><td style="text-align:left">11.17</td><td style="text-align:left">彭  伟</td><td style="text-align:left">文本分类<br>1. <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552" target="_blank" rel="noopener">Recurrent convolutional neural networks for text classification</a> <br>2. <a href="http://www.aclweb.org/anthology/N16-1174" target="_blank" rel="noopener">Hierarchical Attention Networks for Document Classification</a></td><td style="text-align:left">AAAI,2015<br>NAACL,2016</td></tr><tr><td style="text-align:left">12.09</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">MRC: external knowledge<br><a href="http://aclweb.org/anthology/P18-1076" target="_blank" rel="noopener">Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge</a></td><td style="text-align:left">ACL,2018</td></tr><tr><td style="text-align:left">12.09</td><td style="text-align:left">魏相鹏</td><td style="text-align:left"><a href="http://aclweb.org/anthology/D18-1045" target="_blank" rel="noopener">Understanding Back-Translation at Scale</a></td><td style="text-align:left">EMNLP,2018</td></tr><tr><td style="text-align:left">12.09</td><td style="text-align:left">彭  伟</td><td style="text-align:left"><a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">End-to-End Memory Networks</a></td><td style="text-align:left">NIPS,2015</td></tr></tbody></table></div><h1 id="相关内容列表"><a href="#相关内容列表" class="headerlink" title="相关内容列表"></a>相关内容列表</h1><ul><li><a href="/2019/01/09/schedule-2017-md/" title="2017年讨论组内容列表">2017年讨论组内容列表</a></li><li><a href="/2019/01/09/schedule-2019/" title="2019年讨论组内容列表">2019年讨论组内容列表</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Group-Discussion </category>
          
      </categories>
      
      
        <tags>
            
            <tag> schedule </tag>
            
            <tag> group </tag>
            
            <tag> discussion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2017年讨论组日程表</title>
      <link href="/2019/01/09/schedule-2017-md/"/>
      <url>/2019/01/09/schedule-2017-md/</url>
      
        <content type="html"><![CDATA[<h1 id="2017年讨论组论文报告会"><a href="#2017年讨论组论文报告会" class="headerlink" title="2017年讨论组论文报告会"></a>2017年讨论组论文报告会</h1><a id="more"></a><style>    /* 第一列表格宽度 */    table th:nth-of-type(1){    width: 90px;    }    /* 第二列表格宽度 */    table th:nth-of-type(2){    width: 90px;    }    /* 第三列表格宽度 */    table th:nth-of-type(3){    width: 50%;    }    /* 第四列表格宽度 */    table th:nth-of-type(4){    width: 100px;    }</style><div class="table-container"><table><thead><tr><th style="text-align:left">日期</th><th style="text-align:left">报告人</th><th style="text-align:left">报告主题</th><th style="text-align:left">简介</th></tr></thead><tbody><tr><td style="text-align:left">03.25</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Generative Adversarial Nets</td><td style="text-align:left"><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">03.25</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Neural Architectures for Named Entity Recognition</td><td style="text-align:left"><a href="https://arxiv.org/pdf/1603.01360.pdf" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">03.25</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">使用字符级解码器的机器翻译</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">CSGAN for Machine Translation</td><td style="text-align:left"><a href="https://arxiv.org/abs/1703.04887" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Hybrid Word-Character Models for NMT</td><td style="text-align:left"><a href="https://arxiv.org/abs/1604.00788" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">04.15</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Character-based Neural Machine Translation</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.06</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">GAN4NLP</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">05.06</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Neural Machine Translation with Reconstruction</td><td style="text-align:left"><a href="https://arxiv.org/abs/1611.01874" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">05.06</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Minimum Risk Training &amp; Modeling Coverage</td><td style="text-align:left"><a href="https://arxiv.org/abs/1512.02433" target="_blank" rel="noopener">link1</a><br><a href="https://arxiv.org/abs/1601.04811" target="_blank" rel="noopener">link2</a><br></td></tr><tr><td style="text-align:left">05.27</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/P16-1154" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">05.27</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Convolutional Sequence to Sequence Learning</td><td style="text-align:left"><a href="https://arxiv.org/abs/1705.03122" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">05.27</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">多语神经机器翻译</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">06.17</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Context Gates for Neural Machine Translation</td><td style="text-align:left"><a href="https://arxiv.org/abs/1608.06043" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">06.17</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Massive Exploration of Neural Machine Translation Architectures</td><td style="text-align:left"><a href="https://arxiv.org/abs/1703.03906" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">06.17</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">String-based Translation</td><td style="text-align:left"><a href="https://www.isi.edu/natural-language/mt/emnlp16-nmt-grammar.pdf" target="_blank" rel="noopener">link1</a><br><a href="https://arxiv.org/abs/1704.04743" target="_blank" rel="noopener">link2</a><br><a href="https://arxiv.org/abs/1705.01020" target="_blank" rel="noopener">link3</a><br></td></tr><tr><td style="text-align:left">07.22</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Knowledge-Based Semantic Embedding for Machine Translation</td><td style="text-align:left"><a href="http://aclweb.org/anthology/P16-1212" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">07.22</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">What do Neural Machine Translation Models Learn about Morphology?<br>Visualizing and understanding neural machine translation<br></td><td style="text-align:left"><a href="https://arxiv.org/abs/1704.03471" target="_blank" rel="noopener">link1</a><br><a href="http://nlp.csai.tsinghua.edu.cn/~ly/papers/acl2017_dyz.pdf" target="_blank" rel="noopener">link2</a><br></td></tr><tr><td style="text-align:left">07.22</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Dual Supervised Learning</td><td style="text-align:left"><a href="https://arxiv.org/abs/1707.00415" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">08.09</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Pointer Network &amp; It’s application in ACL 16\17</td><td style="text-align:left"><a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">08.16</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Interactive Attention for Neural Machine Translation <br> Neural Machine Translation with Supervised Attention</td><td style="text-align:left"><a href="https://arxiv.org/abs/1610.05011" target="_blank" rel="noopener">link1</a><br><a href="https://arxiv.org/abs/1609.04186" target="_blank" rel="noopener">link2</a></td></tr><tr><td style="text-align:left">09.23</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Plan, Attend, Generate: Char-NMT with Planning</td><td style="text-align:left"><a href="https://arxiv.org/abs/1706.05087" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">09.23</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">AI Challenger</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.23</td><td style="text-align:left">谢玉强</td><td style="text-align:left">RNN</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">09.23</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Neural Machine Translation with Word Predictions（EMNLP,2017）</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/D17-1013" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">10.14</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">CWMT2017 Review</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">10.14</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Experiments</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">10.14</td><td style="text-align:left">谢玉强</td><td style="text-align:left">A Character-Aware Encoder for Neural Machine Translation（COLING,2016）</td><td style="text-align:left"><a href="http://www.aclweb.org/old_anthology/C/C16/C16-1288.pdf" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">10.14</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Sequence-to-Dependency Neural Machine Translation（EMNLP,2017）</td><td style="text-align:left"><a href="http://www.aclweb.org/anthology/P17-1065" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.04</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems (EMNLP,2015)</td><td style="text-align:left"><a href="https://arxiv.org/abs/1508.01745" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.04</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Topic Aware Neural Response Generation (AAAI,2017)</td><td style="text-align:left"><a href="https://arxiv.org/abs/1606.08340" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.04</td><td style="text-align:left">雷扬帆</td><td style="text-align:left">Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</td><td style="text-align:left"><a href="https://arxiv.org/abs/1704.04675" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.04</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Agreement on Target-Bidirectional LSTMs for Sequence-to-Sequence Learning (AAAI,2016) <br> Agreement on Target-Bidirectional Neural Machine Translation（NAACL-HLT,2016）</td><td style="text-align:left"><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12028" target="_blank" rel="noopener">link1</a> <br> <a href="http://www.aclweb.org/anthology/N16-1046" target="_blank" rel="noopener">link2</a></td></tr><tr><td style="text-align:left">11.04</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Gated-Attention Readers for Text Comprehension</td><td style="text-align:left"><a href="https://arxiv.org/abs/1606.01549" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.25</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Unsupervised Machine Translation Using Monolingual Corpora Only</td><td style="text-align:left"><a href="https://arxiv.org/abs/1711.00043" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.25</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Unsupervised Neural Machine Translation</td><td style="text-align:left"><a href="https://arxiv.org/abs/1710.11041" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.25</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Memory Augmented Neural Machine Translation</td><td style="text-align:left">EMNLP,2017<br><a href="https://arxiv.org/abs/1708.02005" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">11.25</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation</td><td style="text-align:left">ACL,2017 <br> <a href="http://www.aclweb.org/anthology/P17-1140" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">12.16</td><td style="text-align:left">魏相鹏</td><td style="text-align:left">Decoding with Value Networks for Neural Machine Translation</td><td style="text-align:left">NIPS,2017 <br> <a href="http://papers.nips.cc/paper/6622-decoding-with-value-networks-for-neural-machine-translation" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">12.16</td><td style="text-align:left">孙雅静</td><td style="text-align:left">Learning to Remember Translation History with Continuous Cache</td><td style="text-align:left">TACL,2018 <br> <a href="https://arxiv.org/abs/1711.09367" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">12.16</td><td style="text-align:left">谢玉强</td><td style="text-align:left">Adversarial Multi-task Learning for Text Classification</td><td style="text-align:left">ACL,2017 <br> <a href="https://arxiv.org/abs/1704.05742" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">邢璐茜</td><td style="text-align:left">Deliberation Networks: Sequence Generation Beyond One-Pass Decoding</td><td style="text-align:left">NIPS,2017 <br> <a href="https://papers.nips.cc/paper/6775-deliberation-networks-sequence-generation-beyond-one-pass-decoding.pdf" target="_blank" rel="noopener">link</a></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">谢玉强</td><td style="text-align:left">词法分析调研</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">孙雅静</td><td style="text-align:left">句法分析调研</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">01.13</td><td style="text-align:left">彭  伟</td><td style="text-align:left">The Colorful World in the Neural Network</td></tr></tbody></table></div><h1 id="相关内容列表"><a href="#相关内容列表" class="headerlink" title="相关内容列表"></a>相关内容列表</h1><ul><li><a href="/2019/01/09/schedule-2018-md/" title="2018年讨论组内容列表">2018年讨论组内容列表</a></li><li><a href="/2019/01/09/schedule-2019/" title="2019年讨论组内容列表">2019年讨论组内容列表</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Group-Discussion </category>
          
      </categories>
      
      
        <tags>
            
            <tag> schedule </tag>
            
            <tag> group </tag>
            
            <tag> discussion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/01/07/hello-world/"/>
      <url>/2019/01/07/hello-world/</url>
      
        <content type="html"><![CDATA[<p>This is a post to record some solutions in blog with hexo<br><a id="more"></a></p><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><h2 id="美化方案"><a href="#美化方案" class="headerlink" title="美化方案"></a>美化方案</h2><p>参考：<a href="https://jerry011235.github.io/2015/05/06/Hexo%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BC%98%E5%8C%96%EF%BC%88%E4%BA%8C%EF%BC%89/" target="_blank" rel="noopener">https://jerry011235.github.io/2015/05/06/Hexo%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BC%98%E5%8C%96%EF%BC%88%E4%BA%8C%EF%BC%89/</a></p><h3 id="用css控制Markdown表格列宽"><a href="#用css控制Markdown表格列宽" class="headerlink" title="用css控制Markdown表格列宽"></a>用css控制Markdown表格列宽</h3><p>参考: <a href="http://blog.echoxu.cn/2018-05-15-Hexo中用CSS控制Markdown各列表格宽度.html" target="_blank" rel="noopener">http://blog.echoxu.cn/2018-05-15-Hexo中用CSS控制Markdown各列表格宽度.html</a></p><h2 id="编辑"><a href="#编辑" class="headerlink" title="编辑"></a>编辑</h2><h3 id="post样式"><a href="#post样式" class="headerlink" title="post样式"></a>post样式</h3><p>参考: <a href="http://dinghongkai.com/2017/12/19/Blog-development-6-Customized-Style-of-Writing/" target="_blank" rel="noopener">添加文章书写样</a></p><h3 id="添加站内文章链接"><a href="#添加站内文章链接" class="headerlink" title="添加站内文章链接"></a>添加站内文章链接</h3><ul><li><code>\{\% post_link post_name_in_source_posts link_show_title \%}</code></li><li><code>[title](/year/month/day/name.md#section)</code></li></ul><h3 id="字体颜色"><a href="#字体颜色" class="headerlink" title="字体颜色"></a>字体颜色</h3><ul><li>可以直接在Markdown 文档编辑中使用html语法<ul><li><code>&lt;font size=4 &gt; 这里输入文字，自定义大小 &lt;/font&gt;</code></li><li><code>&lt;font color=&quot;#FF0000&quot;&gt; 这里输入文字，自定义颜色的字体 &lt;/font&gt;</code></li></ul></li></ul><h3 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h3><ul><li>图片大小控制：<ul><li><code>&lt;div style=&quot;width: 200px; margin: auto&quot;&gt;![image-caption](image-link)&lt;/div&gt;</code></li></ul></li></ul><h3 id="在Hexo中渲染MathJax数学公式"><a href="#在Hexo中渲染MathJax数学公式" class="headerlink" title="在Hexo中渲染MathJax数学公式"></a>在Hexo中渲染MathJax数学公式</h3><ul><li><a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">https://www.jianshu.com/p/7ab21c7f0674</a></li></ul><h2 id="bugs"><a href="#bugs" class="headerlink" title="bugs"></a>bugs</h2><ul><li><code>fatal: multiple stage entries for merged file &#39;lib/pace&#39;</code><ul><li><code>cd .deploy_git</code></li><li><code>rm .git/index</code></li><li><code>git add -A</code></li><li><code>git commit -m &quot;&quot;</code></li></ul></li></ul><h2 id="迁移"><a href="#迁移" class="headerlink" title="迁移"></a>迁移</h2><ul><li>重装hexo、配置依赖</li><li>Git clone next主题</li><li>复制 站点配置 文件和 主题文件夹</li><li>复制 source 文件夹</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
